{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176c5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "from matplotlib import patches,  lines\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05548b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "VQA_PATH='/Data_Storage/Rui_Data_Space/multimodal/VQA'\n",
    "GQA_PATH='/Data_Storage/Rui_Data_Space/multimodal/GQA'\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def load_pkl(path):\n",
    "    data=pkl.load(open(path,'rb'))\n",
    "    return data\n",
    "\n",
    "def load_json(path):\n",
    "    data=json.load(open(path,'r'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b70b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e7af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "stemmer = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c92b73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vqa_prepro'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mvqa_prepro\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodify_program\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mclean_modules\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_program\u001b[39m(program):\n\u001b[1;32m      4\u001b[0m     program\u001b[38;5;241m=\u001b[39mclean_modules\u001b[38;5;241m.\u001b[39meliminate_obj_id(program)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vqa_prepro'"
     ]
    }
   ],
   "source": [
    "import vqa_prepro.modify_program as clean_modules\n",
    "\n",
    "def update_program(program):\n",
    "    program=clean_modules.eliminate_obj_id(program)\n",
    "    program=clean_modules.modify_choose(program)\n",
    "    program=clean_modules.modify_diff_same(program)\n",
    "    program=clean_modules.modify_verify(program)\n",
    "    program=clean_modules.modify_relate(program)\n",
    "    program=clean_modules.modify_filter(program)\n",
    "    return program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65efae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_filter(prog):\n",
    "    new_prog=[]\n",
    "    flag=False\n",
    "    select_idx={i:0 for i in range(len(prog))}\n",
    "    ops=[step['operation'] for step in prog]\n",
    "    if 'attentionand' not in ops:\n",
    "        return prog\n",
    "    for k,p in enumerate(prog):\n",
    "        module=p['operation']\n",
    "        if  p['operation']=='select' and  prog[k+1]['operation']=='attentionand':\n",
    "            flag=True\n",
    "            for i in range(k,len(prog)):\n",
    "                select_idx[i]+=1\n",
    "            #print (select_idx)\n",
    "            dep=prog[k+1]['dependencies'][0]\n",
    "            dep-=select_idx[dep]\n",
    "            new_prog.append(\n",
    "                {'operation':'filter',\n",
    "                 'argument':p['argument'],\n",
    "                 'dependencies':[dep]}\n",
    "            )\n",
    "        elif p['operation']=='attentionand' and flag:\n",
    "            flag=False\n",
    "            continue\n",
    "        else:\n",
    "            if p['operation'] in ['and','or']:\n",
    "                dep=[]\n",
    "                exist_0=prog[p['dependencies'][0]]['dependencies'][0]\n",
    "                exist_1=prog[p['dependencies'][1]]['dependencies'][0]\n",
    "                dep.append(p['dependencies'][0]-select_idx[exist_0])\n",
    "                dep.append(p['dependencies'][1]-select_idx[exist_1])\n",
    "            else:\n",
    "                dep=[dep-select_idx[k] for dep in p['dependencies']]\n",
    "            new_prog.append(\n",
    "                {'operation':p['operation'],\n",
    "                 'argument':p['argument'],\n",
    "                 'dependencies':dep}\n",
    "            )\n",
    "    return new_prog\n",
    "\n",
    "def mine_clean_compare(prog):\n",
    "    if prog[-1]['operation']=='compare':\n",
    "        if len(prog)==4:\n",
    "            new_prog=[]\n",
    "            new_prog.append(prog[0])\n",
    "            new_prog.append(prog[1])\n",
    "            final=prog[-1]\n",
    "            new_prog.append({\n",
    "                'argument':final['argument'],\n",
    "                'dependencies':[0,1],\n",
    "                'operation':final['operation']\n",
    "            })\n",
    "            return new_prog\n",
    "    return prog\n",
    "\n",
    "def mine_choose(prog):\n",
    "    if prog[-1]['operation']!='choose' or prog[-2]['operation']!='exist':\n",
    "        return prog\n",
    "    if 'to the left of' not in prog[-1]['argument'] and\\\n",
    "                'behind' not in prog[-1]['argument'] and\\\n",
    "                'above' not in prog[-1]['argument']:\n",
    "        new_prog=[]\n",
    "        choose_from=prog[-1]['argument']\n",
    "        for k,p in enumerate(prog):\n",
    "            if p['operation']=='select' and p['argument'][0] in choose_from:\n",
    "                break\n",
    "            else:\n",
    "                new_prog.append(p)\n",
    "        if len(new_prog)==0:\n",
    "            new_prog.append(\n",
    "                {'operation':'select',\n",
    "                 'argument':choose_from[0],\n",
    "                 'dependencies':[]}\n",
    "            )\n",
    "            new_prog.append(\n",
    "                {'operation':'select',\n",
    "                 'argument':choose_from[1],\n",
    "                 'dependencies':[]}\n",
    "            )\n",
    "        new_prog.append(\n",
    "            {'operation':'choose',\n",
    "            'argument':choose_from,\n",
    "            'dependencies':[len(new_prog)-1]}\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        ops=[step['operation'] for step in prog]\n",
    "        #print (ops)\n",
    "        select_idx=ops.index('exist')\n",
    "        #print (select_idx)\n",
    "        deps=prog[select_idx-1]['dependencies']\n",
    "        dep_0=deps[0]\n",
    "        dep_1=deps[1]\n",
    "        choose_from=prog[-1]['argument']\n",
    "        new_prog=[]\n",
    "        new_prog.extend([prog[i] for i in range(dep_0)])\n",
    "        new_prog.extend([prog[i] for i in range(dep_0+1,dep_1+1)])\n",
    "        new_prog.append(\n",
    "            {'operation':'choose',\n",
    "             'argument':choose_from,\n",
    "             'dependencies':[dep_0-1,len(new_prog)-1]}\n",
    "        )\n",
    "        #print (new_prog)\n",
    "    if new_prog[0]['operation']=='select' and new_prog[1]['operation']=='select':\n",
    "        if len(new_prog)==3 and len(new_prog[-1]['dependencies'])==1:\n",
    "            new_prog[-1]['dependencies'].insert(0,0)\n",
    "    return new_prog\n",
    "\n",
    "def mine_not(prog):\n",
    "    ops=[step['operation'] for step in prog]\n",
    "    if 'attentionnot' not in ops:\n",
    "        return prog\n",
    "    select_idx={i:0 for i in range(len(prog))}\n",
    "    new_prog=[]\n",
    "    flag=True\n",
    "    \n",
    "    invalid=[]\n",
    "    for k,step in enumerate(prog):\n",
    "        if k in invalid:\n",
    "            continue\n",
    "        if step['operation']=='select' and prog[k+1]['operation']=='attentionnot':\n",
    "            flag=False\n",
    "            invalid.append(k+1)\n",
    "            invalid.append(k+2)\n",
    "            for i in range(k,len(prog)):\n",
    "                select_idx[i]+=2\n",
    "            new_prog.append(\n",
    "                {\n",
    "                    'operation':'filter',\n",
    "                    'dependencies':[k-1-select_idx[k-1]],\n",
    "                    'argument': ['not '+step['argument'][0]]\n",
    "                })\n",
    "        else:\n",
    "            if flag:\n",
    "                new_prog.append(step)\n",
    "            else:\n",
    "                dep=step['dependencies']\n",
    "                new_dep=[p-select_idx[p] for p in dep]\n",
    "                new_prog.append({\n",
    "                    'operation':step['operation'],\n",
    "                    'dependencies':new_dep,\n",
    "                    'argument':step['argument']\n",
    "                })\n",
    "    return new_prog\n",
    "\n",
    "def mine_clean_all(prog):\n",
    "    choose_prog=mine_choose(prog)\n",
    "    filter_prog=mine_filter(choose_prog)\n",
    "    not_prog=mine_not(filter_prog)\n",
    "    comp_prog=mine_clean_compare(not_prog)\n",
    "    for step in comp_prog:\n",
    "        if type(step['argument']) is not list:\n",
    "            step['argument']=[step['argument']]\n",
    "    return comp_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e54a57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132062\n",
      "10696\n"
     ]
    }
   ],
   "source": [
    "gqa_val_q=json.load(\n",
    "    open(os.path.join(GQA_PATH,'original','val_balanced_questions.json')\n",
    "         ,'r')\n",
    ")\n",
    "print (len(gqa_val_q))\n",
    "names=list(gqa_val_q.keys())\n",
    "\n",
    "val_graphs=json.load(\n",
    "    open(os.path.join(GQA_PATH,'val_sceneGraphs.json')\n",
    "         ,'r')\n",
    ")\n",
    "print (len(val_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b449e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage import io as skimage_io\n",
    "from skimage import transform as skimage_transform\n",
    "\n",
    "def get_tokens(text_queries):\n",
    "    tokenized_queries = np.array([\n",
    "        module.tokenize(q, config.dataset_configs.max_query_length)\n",
    "        for q in text_queries\n",
    "    ])\n",
    "    # Pad tokenized queries to avoid recompilation if number of queries changes:\n",
    "    tokenized_queries = np.pad(\n",
    "        tokenized_queries,\n",
    "        pad_width=((0, 100 - len(text_queries)), (0, 0)),\n",
    "        constant_values=0)\n",
    "    return tokenized_queries\n",
    "\n",
    "def get_gqa_feat(img_id):\n",
    "    # Load example image:\n",
    "    filename = os.path.join(GQA_PATH,'images',img_id+'.jpg')\n",
    "    image_uint8 = skimage_io.imread(filename)\n",
    "    image = image_uint8.astype(np.float32) / 255.0\n",
    "\n",
    "    # Pad to square with gray pixels on bottom and right:\n",
    "    h, w, _ = image.shape\n",
    "    size = max(h, w)\n",
    "    image_padded = np.pad(\n",
    "        image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)\n",
    "\n",
    "    # Resize to model input size:\n",
    "    input_image = skimage.transform.resize(\n",
    "        image_padded,\n",
    "        (840, 840),\n",
    "        anti_aliasing=True)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f53ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ashkamath_mdetr_main\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "CUDA_DEVICE=15\n",
    "torch.cuda.set_device(CUDA_DEVICE)\n",
    "device = torch.device(\"cuda:\"+str(CUDA_DEVICE))\n",
    "#the default ipykernel links to the first conda environment\n",
    "\n",
    "model, postprocessor = torch.hub.load('ashkamath/mdetr:main', 'mdetr_efficientnetB5', pretrained=True, return_postprocessor=True)\n",
    "model = model.to(device)\n",
    "model.eval();\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf19652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference(im, caption, plot=True):\n",
    "    # mean-std normalize the input image (batch-size: 1)\n",
    "    img = transform(im).unsqueeze(0).cuda()\n",
    "\n",
    "    # propagate through the model\n",
    "    memory_cache = model(img, [caption], encode_and_save=True)\n",
    "    outputs = model(img, [caption], encode_and_save=False, memory_cache=memory_cache)\n",
    "    #print (outputs['pred_logits'])\n",
    "\n",
    "    # keep only predictions with 0.7+ confidence\n",
    "    probas = 1 - outputs['pred_logits'].softmax(-1)[0, :, -1].cpu()\n",
    "    keep = (probas > 0.7).cpu()\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'].cpu()[0, keep], im.size)\n",
    "\n",
    "    # Extract the text spans predicted by each box\n",
    "    positive_tokens = (outputs[\"pred_logits\"].cpu()[0, keep].softmax(-1) > 0.1).nonzero().tolist()\n",
    "    predicted_spans = defaultdict(str)\n",
    "    for tok in positive_tokens:\n",
    "        item, pos = tok\n",
    "        if pos < 255:\n",
    "            span = memory_cache[\"tokenized\"].token_to_chars(0, pos)\n",
    "            predicted_spans [item] += \" \" + caption[span.start:span.end]\n",
    "\n",
    "    labels = [predicted_spans [k] for k in sorted(list(predicted_spans .keys()))]\n",
    "    if plot:\n",
    "        plot_results(im, probas[keep], bboxes_scaled, labels)\n",
    "    return probas[keep], bboxes_scaled.tolist(), labels\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c] * 255,\n",
    "                                  image[:, :, c])\n",
    "    return image\n",
    "\n",
    "def plot_results(pil_img, scores, boxes, labels, masks=None):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    np_image = np.array(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    if masks is None:\n",
    "        masks = [None for _ in range(len(scores))]\n",
    "    assert len(scores) == len(boxes) == len(labels) == len(masks)\n",
    "    for s, (xmin, ymin, xmax, ymax), l, mask, c in zip(scores, boxes.tolist(), labels, masks, colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f'{l}: {s:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "        if mask is None:\n",
    "            continue\n",
    "        np_image = apply_mask(np_image, mask, c)\n",
    "\n",
    "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            # Subtract the padding and flip (y, x) to (x, y)\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=c)\n",
    "            ax.add_patch(p)\n",
    "\n",
    "\n",
    "    plt.imshow(np_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_res(results, ax, color='green'):\n",
    "    #for tt in results.values():\n",
    "    if True:\n",
    "        bboxes = results['boxes']\n",
    "        labels = results['labels']\n",
    "        scores = results['scores']\n",
    "        #keep = scores >= 0.0\n",
    "        #bboxes = bboxes[keep].tolist()\n",
    "        #labels = labels[keep].tolist()\n",
    "        #scores = scores[keep].tolist()\n",
    "    #print(torchvision.ops.box_iou(tt['boxes'].cpu().detach(), torch.as_tensor([[xmin, ymin, xmax, ymax]])))\n",
    "    \n",
    "    colors = ['purple', 'yellow', 'red', 'green', 'orange', 'pink']\n",
    "    \n",
    "    for i, (b, ll, ss) in enumerate(zip(bboxes, labels, scores)):\n",
    "        ax.add_patch(plt.Rectangle((b[0], b[1]), b[2] - b[0], b[3] - b[1], fill=False, color=colors[i], linewidth=3))\n",
    "        cls_name = ll if isinstance(ll,str) else CLASSES[ll]\n",
    "        text = f'{cls_name}: {ss:.2f}'\n",
    "        print(text)\n",
    "        ax.text(b[0], b[1], text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89421fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_bbox_only(bbox,scores,input_image,gt_bbox=None,text=None,threshold=0.2,vis_pred=True):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    if vis_pred:\n",
    "        for i,box in enumerate(bbox):\n",
    "            score=scores[i]\n",
    "            if score<threshold:\n",
    "                continue\n",
    "            cx, cy, w, h = box\n",
    "            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
    "            ax.text(\n",
    "                cx - w / 2,\n",
    "                cy + h / 2 + 0.015,\n",
    "                f'{score:1.2f}',\n",
    "                ha='left',\n",
    "                va='top',\n",
    "                color='red',\n",
    "                bbox={\n",
    "                    'facecolor': 'white',\n",
    "                    'edgecolor': 'red',\n",
    "                    'boxstyle': 'square,pad=.3'\n",
    "                })\n",
    "    if text is not None:\n",
    "        ax.set_title(text,  fontsize=12)\n",
    "    if gt_bbox is not None:\n",
    "        for i,box in enumerate(gt_bbox):\n",
    "            cx, cy, w, h = box\n",
    "            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77ac8bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "def size_aware_pred(bb):\n",
    "    if bb[2]>0.3 or bb[3]>0.3:\n",
    "        return 'large'\n",
    "    else:\n",
    "        return 'small'\n",
    "def pos_aware_pred(valid_bbox,candidates):\n",
    "    if 'bottom' in candidates:\n",
    "        if valid_bbox[1]>0.5:\n",
    "            return 'bottom'\n",
    "        else:\n",
    "            return 'top'\n",
    "    elif 'right' in candidates:\n",
    "        if valid_bbox[0]<0.5:\n",
    "            return 'left'\n",
    "        else:\n",
    "            return 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55c8b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coord(bbox,img_id):\n",
    "    width=val_graphs[img_id]['width']\n",
    "    height=val_graphs[img_id]['height']\n",
    "    size = max(height, width)\n",
    "    x=bbox[0]\n",
    "    y=bbox[1]\n",
    "    w=bbox[2]-bbox[0]\n",
    "    h=bbox[3]-bbox[1]\n",
    "    #print (bbox)\n",
    "    return [(x+w/2)/size,(y+h/2)/size,w/size,h/size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b801886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjectives(text):\n",
    "    sent=text.split(',')[0]\n",
    "    words=sent.split(' ')\n",
    "    if 'less' in words:\n",
    "        words=words[-2:]\n",
    "    else:\n",
    "        words=words[-1:]\n",
    "    words=' '.join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97aae31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mdert_result(scores,bboxs,labels,obj,img_id):\n",
    "    sf_s=[]\n",
    "    sf_bbox=[]\n",
    "    labels=[''.join(label.split(' ')) for label in labels]\n",
    "    flag=False\n",
    "    if obj in labels:\n",
    "        flag=True\n",
    "    #print (obj,labels)\n",
    "    for i,label in enumerate(labels):\n",
    "        if flag:\n",
    "            if obj==label:\n",
    "                sf_s.append(scores[i])\n",
    "                sf_bbox.append(bboxs[i])\n",
    "        else:\n",
    "            if obj in label:\n",
    "                sf_s.append(scores[i])\n",
    "                sf_bbox.append(bboxs[i])\n",
    "    #print (len(sf_s))\n",
    "    if len(sf_bbox)==0:\n",
    "        for i,label in enumerate(labels):\n",
    "            sf_s.append(scores[i])\n",
    "            sf_bbox.append(bboxs[i])\n",
    "    max_s=max(sf_s)\n",
    "    max_id=sf_s.index(max_s)\n",
    "    return_bbox=sf_bbox[max_id]\n",
    "    coord=transform_coord(return_bbox,img_id)\n",
    "    return max_s.item(),coord\n",
    "#only consider one bbox now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe5fb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase_ground(img_id,cap,obj):\n",
    "    cap=cap+' '\n",
    "    im=Image.open(os.path.join(GQA_PATH,'images',img_id+'.jpg'))\n",
    "    try:\n",
    "        scores,bboxs,labels=plot_inference(im, cap, plot=False)\n",
    "    except:\n",
    "        print ('Invalid caption generation for',img_id,obj,cap)\n",
    "        scores=[torch.Tensor([0.99])]\n",
    "        bboxs=[[0.5,0.5,0.99,0.99]]\n",
    "        labels=[obj]\n",
    "    if len(bboxs)==0:\n",
    "        max_s=0.99\n",
    "        valid_bbox=[0.5,0.5,0.99,0.99]\n",
    "    else:\n",
    "        # print (scores,bboxs)\n",
    "        max_s,valid_bbox=preprocess_mdert_result(scores,bboxs,labels,obj,img_id)\n",
    "    return max_s,valid_bbox\n",
    "\n",
    "def bbox_generator(layout,idx,img_id,ques_id):\n",
    "    if layout[idx]['operation']=='select':\n",
    "        #print (idx)\n",
    "        try:\n",
    "            scenic_result=load_pkl(os.path.join(GQA_PATH,\n",
    "                                                'meta_one-all-scenic',\n",
    "                                                ques_id+'.pkl'))\n",
    "            obj=layout[idx]['argument'][0]\n",
    "            scores=[s['score'] for s in scenic_result[obj]]\n",
    "            bboxs=[s['bbox'] for s in scenic_result[obj]]\n",
    "        except:\n",
    "            bboxs=[]\n",
    "            obj=layout[idx]['argument'][0]\n",
    "        if len(bboxs)>0:\n",
    "            max_s=max(scores)\n",
    "            max_idx=scores.index(max_s)\n",
    "            bbox=bboxs[max_idx]\n",
    "            cap=obj\n",
    "        else:\n",
    "            cap=obj\n",
    "            split_obj=''.join(obj.split(' '))\n",
    "            #print (cap,split_obj)\n",
    "            max_s,bbox=generate_phrase_ground(img_id,cap,split_obj)\n",
    "            #print (max_s,bbox)\n",
    "    else:\n",
    "        cap,obj=cap_generator(layout,idx,img_id)\n",
    "        max_s,bbox=generate_phrase_ground(img_id,cap,obj)\n",
    "    return max_s,bbox,cap        \n",
    "            \n",
    "def cap_generator(layout,idx,img_id):\n",
    "    words=[]\n",
    "    dep=layout[idx]['dependencies']\n",
    "    word=layout[idx]['argument'][0]\n",
    "    words.append(word)\n",
    "    obj=''.join(word.split(' '))\n",
    "    while len(dep)>0:\n",
    "        cur_step=layout[dep[0]]\n",
    "        if cur_step['operation']=='relocate':\n",
    "            relo_symbol=cur_step['argument'][1]\n",
    "            cur_phrase=' '.join(words)\n",
    "            words=[]\n",
    "            words.append(cur_phrase)\n",
    "            #print(cur_step)\n",
    "            words.append(cur_step['argument'][0])\n",
    "            dep=cur_step['dependencies']\n",
    "            cur_step=layout[dep[0]]\n",
    "            dep=cur_step['dependencies']\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            if relo_symbol=='o':\n",
    "                words=list(reversed(words))\n",
    "        else:\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            dep=cur_step['dependencies']\n",
    "    cap=' '.join(words)\n",
    "    return cap,obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7731d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_bbox_generator_exist(layout,idx,img_id):\n",
    "    cap=cap_generator_exist(layout,idx,img_id)\n",
    "    max_s,bbox=generate_phrase_ground(img_id,cap,cap)\n",
    "    return bbox,cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc20992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped(bb,input_image,showing=False):\n",
    "    width=bb[2]*840\n",
    "    height=bb[3]*840\n",
    "    xy=(bb[0]*840-width/2,bb[1]*840-height/2)\n",
    "    h0=int(max(0,xy[1]))\n",
    "    w0=int(max(0,xy[0]))\n",
    "    h1=int(max(0,840-(height+xy[1])))\n",
    "    w1=int(max(0,840-(width+xy[0])))\n",
    "    cropped=skimage.util.crop(input_image,((h0,h1),(w0,w1),(0,0)), copy=False)\n",
    "    trans_crop=Image.fromarray(np.uint8(cropped*255.0))\n",
    "    if showing:\n",
    "        display.display(trans_crop)\n",
    "    return trans_crop\n",
    "\n",
    "def clip_aware_pred(img_feat,valid_bbox,candidates,showing=False):\n",
    "    #tokens=clip.tokenize(candidates)\n",
    "    trans_crop=get_cropped(valid_bbox,img_feat)\n",
    "    if showing:\n",
    "        display.display(trans_crop)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "        text_features = clip_model.encode_text(clip.tokenize(candidates).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    #print (indices[0][0].item())\n",
    "    ans=candidates[indices[0][0].item()]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "534f7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_holder=['left','right','bottom','top','front']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0da2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d29b75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(ques):\n",
    "    nouns=[]\n",
    "    tokens_tag = pos_tag(ques.replace('?','').split(' '))\n",
    "    for w,pos in tokens_tag:\n",
    "        if pos in ['NN','NNS'] and w not in position_holder:\n",
    "            nouns.append(lemmatizer.lemmatize(w))\n",
    "    return nouns\n",
    "\n",
    "def verify_noun(nouns,obj):\n",
    "    if obj in nouns or \\\n",
    "    lemmatizer.lemmatize(obj) in nouns or\\\n",
    "    stemmer.singular_noun(obj) in nouns:\n",
    "        return True\n",
    "    elif len(obj.split(' '))>1:\n",
    "        words=obj.split(' ')\n",
    "        for w in words:\n",
    "            #print (w)\n",
    "            if lemmatizer.lemmatize(w) in nouns or\\\n",
    "            stemmer.singular_noun(w) in nouns:\n",
    "                return True\n",
    "    elif obj in ['he','she','they']:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0c26abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relo(layout):\n",
    "    ops=[step['operation'] for step in layout]\n",
    "    length=len(layout)-2\n",
    "    relo_id=length\n",
    "    counter=0\n",
    "    rela_symbol=[]\n",
    "    for i in range(len(layout)-2):\n",
    "        cur_op=layout[length-i]['operation']\n",
    "        if cur_op=='relocate':\n",
    "            relo_id=length-i\n",
    "            rela_symbol=layout[length-i]['argument']\n",
    "            break\n",
    "        elif cur_op=='filter':\n",
    "            counter+=1#find some attributes finally, if true\n",
    "    return rela_symbol,relo_id, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fc4a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_generator_exist(layout,idx,img_id):\n",
    "    words=[]\n",
    "    dep=layout[idx]['dependencies']\n",
    "    while len(dep)>0:\n",
    "        cur_step=layout[dep[0]]\n",
    "        if cur_step['operation']=='relocate':\n",
    "            relo_symbol=cur_step['argument'][1]\n",
    "            cur_phrase=' '.join(words)\n",
    "            words=[]\n",
    "            words.append(cur_phrase)\n",
    "            #print(cur_step)\n",
    "            words.append(cur_step['argument'][0])\n",
    "            dep=cur_step['dependencies']\n",
    "            new_sent=[]\n",
    "            while len(dep)>0:\n",
    "                cur_step=layout[dep[0]]\n",
    "                dep=cur_step['dependencies']\n",
    "                arg=cur_step['argument'][0]\n",
    "                new_sent.append(arg)\n",
    "            words.append(' '.join(new_sent))\n",
    "            if relo_symbol=='o':\n",
    "                words=list(reversed(words))\n",
    "        else:\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            dep=cur_step['dependencies']\n",
    "    cap=' '.join(words)\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61d6469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_involved_ans(layout,img_id,ques_id,thresh=0.2):\n",
    "    nouns=get_nouns(gqa_val_q[ques_id]['question'][:-1])#remove the punctuation\n",
    "    ops=[step['operation'] for step in layout]\n",
    "    #must have select operation\n",
    "    for m,step in enumerate(layout[:-1]):\n",
    "        op=step['operation']\n",
    "        if op not in ['filter','select']:\n",
    "            continue\n",
    "        arg=step['argument'][0]\n",
    "        if arg in hold_list:\n",
    "            continue\n",
    "        if op=='filter':\n",
    "            flag=verify_noun(nouns,arg)\n",
    "            if flag==False:#not a noun\n",
    "                continue\n",
    "        if op=='select':\n",
    "            #infos=select_info[arg]\n",
    "            s,_,_=bbox_generator(layout,m,img_id,ques_id)\n",
    "            if s<thresh:\n",
    "                return 'no'\n",
    "        else:\n",
    "            try:\n",
    "                filter_info=load_pkl(os.path.join(GQA_PATH,\n",
    "                                              'meta_filter-related',\n",
    "                                              ques_id+'.pkl'))\n",
    "                infos=filter_info[arg]\n",
    "                s=[info['score'] for info in infos if info['score']>=thresh]\n",
    "            except:\n",
    "                print('No filter result',arg,ques_id)\n",
    "                continue\n",
    "            \n",
    "            if len(s)==0:\n",
    "                return 'no'\n",
    "    rela_symbol,rela_id,counter=get_relo(new_prog)    \n",
    "    if counter==1:\n",
    "        pred=verify_relation(layout,img_id,ques_id,rela_symbol,rela_id)\n",
    "    else:\n",
    "        pred=verify_attr_last_step(layout,img_id,ques_id)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51c2d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "heur_rela=['to the left of','to the right of',\n",
    "           'above','under','on top of','below',\n",
    "           'beneath','underneath']\n",
    "#if color on the last argument, use phrase grounding and binary color function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d42b2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heur_rules(bb_a,bb_b,rela):\n",
    "    #whether there is bb_a rela bb_b\n",
    "    if rela=='to the left of':\n",
    "        if bb_a[0]<bb_b[0]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif rela=='to the right of':\n",
    "        if bb_a[0]>bb_b[0]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif rela in ['above','on top of']:\n",
    "        if bb_a[1]>bb_b[1]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif rela in [ 'under','below','beneath','underneath']:\n",
    "        if bb_a[1]<bb_b[1]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e276faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_relation(layout,img_id,ques_id,rela_symbol,rela_id,showing=False):\n",
    "    relation=rela_symbol[0]\n",
    "    sub_obj=rela_symbol[1]\n",
    "    obj=layout[-2]['argument'][0]\n",
    "    cap=cap_generator_exist(layout,len(layout)-1,img_id)\n",
    "    cap=cap.replace(obj,'').replace(relation,'')\n",
    "    if relation in heur_rela:\n",
    "        try:\n",
    "            scenic_info=load_pkl(os.path.join(GQA_PATH,\n",
    "                                              'meta_filter-related',\n",
    "                                              ques_id+'.pkl'))\n",
    "        #print (scenic_info.keys())\n",
    "            info=scenic_info[obj]\n",
    "            scores=[i['score'] for i in info]\n",
    "            bboxs=[i['bbox'] for i in info]\n",
    "            if len(scores)==0:\n",
    "                return 'no'\n",
    "            bb_1=bboxs[scores.index(max(scores))]\n",
    "        except:\n",
    "            s,b,_=bbox_generator(layout,len(layout)-2,img_id,ques_id)\n",
    "            bb_1=b\n",
    "        \n",
    "        if rela_id==1:\n",
    "            _,bb_2,_=bbox_generator(layout,0,img_id,ques_id)\n",
    "        else:\n",
    "            obj=layout[rela_id-1]['argument'][0]\n",
    "            _,bb_2=generate_phrase_ground(img_id,cap,obj)\n",
    "        if sub_obj=='o':\n",
    "            bb_a=bb_2\n",
    "            bb_b=bb_1\n",
    "        else:\n",
    "            bb_a=bb_1\n",
    "            bb_b=bb_2\n",
    "        pred=heur_rules(bb_a,bb_b,relation)\n",
    "        if showing:\n",
    "            img_feat=get_gqa_feat(img_id)\n",
    "            #print (bb_a,bb_b)\n",
    "            vis_bbox_only([bb_a],[0.5],img_feat,[bb_b],img_id)\n",
    "    else:\n",
    "        if sub_obj=='s':\n",
    "            pos=' '.join([obj,'is',relation,cap])\n",
    "            neg=' '.join([obj,'is not',relation,cap])\n",
    "        else:\n",
    "            pos=' '.join([cap,'is',relation,obj])\n",
    "            neg=' '.join([cap,'is not',relation,obj])\n",
    "        #print (pos,'\\n',neg,'\\n',gqa_val_q[ques_id]['question'])\n",
    "        pred=binary_matching(img_id,[pos,neg])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b52834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_matching(img_id,sents):\n",
    "    tokens=clip.tokenize(sents)\n",
    "    im=Image.open(os.path.join(GQA_PATH,'images',img_id+'.jpg'))\n",
    "    with torch.no_grad():\n",
    "        text_features=clip_model.encode_text(tokens.to(device))\n",
    "        image_features = clip_model.encode_image(preprocess(im).unsqueeze(0).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    indices=torch.max(similarity,1)[1]\n",
    "    #print (similarity,indices)\n",
    "    similarity=similarity.squeeze()\n",
    "    scores=[str(similarity[0].item()),str(similarity[1].item())]\n",
    "    if indices.squeeze().item()==0:\n",
    "        pred='yes'\n",
    "    else:\n",
    "        pred='no'\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9c56c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_attr_last_step(layout,img_id,ques_id,showing=False):\n",
    "    cap=cap_generator_exist(layout,len(layout)-1,img_id)\n",
    "    attr=layout[-2]['argument'][0]\n",
    "    phrase=cap.replace(attr,'')\n",
    "    obj=layout[-3]['argument'][0]\n",
    "    _,bbox=generate_phrase_ground(img_id,phrase,obj)\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    pred=filter_binary_color(bbox,input_image,attr,showing)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3f71eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_binary_color(bb,input_image,color,showing=False):\n",
    "    width=bb[2]*840\n",
    "    height=bb[3]*840\n",
    "    xy=(bb[0]*840-width/2,bb[1]*840-height/2)\n",
    "    h0=int(max(0,xy[1]))\n",
    "    w0=int(max(0,xy[0]))\n",
    "    h1=int(max(0,840-(height+xy[1])))\n",
    "    w1=int(max(0,840-(width+xy[0])))\n",
    "    cropped=skimage.util.crop(input_image,((h0,h1),(w0,w1),(0,0)), copy=False)\n",
    "    trans_crop=Image.fromarray(np.uint8(cropped*255.0))\n",
    "    flag=True #color is a positive statement\n",
    "    if 'not' in color:\n",
    "        flag=False#reverse the pred\n",
    "        words=color.split(' ')[1:]\n",
    "        color=' '.join(words)\n",
    "    pos_sent=color\n",
    "    neg_sent='Not '+color\n",
    "    tokens=clip.tokenize([neg_sent,pos_sent])\n",
    "    with torch.no_grad():\n",
    "        text_features=clip_model.encode_text(tokens.to(device))\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    indices=torch.max(similarity,1)[1]\n",
    "    #print (similarity,indices)\n",
    "    similarity=similarity.squeeze()\n",
    "    scores=[str(similarity[0].item()),str(similarity[1].item())]\n",
    "    if showing:\n",
    "        vis_no_score([bb],input_image,'not '+color+'-'+'-'.join(scores))\n",
    "    if indices.squeeze().item()==1:\n",
    "        pred='yes'\n",
    "    else:\n",
    "        pred='no'\n",
    "    if flag:\n",
    "        return pred\n",
    "    else:\n",
    "        if pred=='yes':\n",
    "            return 'no'\n",
    "        else:\n",
    "            return 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2903ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_basic_ans(layout,img_id,ques_id,thresh=0.2):\n",
    "    arg=layout[0]['argument'][0]\n",
    "    scenic_result=load_pkl(os.path.join(GQA_PATH,'meta_one-all-scenic',ques_id+'.pkl'))\n",
    "    info=scenic_result[arg]\n",
    "    scores=[]\n",
    "    scores=[i['score'] for i in info if i['score']>=thresh]\n",
    "    if len(scores)==0:\n",
    "        return 'no'\n",
    "    if max(scores)>=thresh:\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47e79b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_position(bb,pos,input_image,showing=False):\n",
    "    if showing:\n",
    "        vis_no_score([bb],input_image,pos)\n",
    "    if pos=='left':\n",
    "        if bb[0]<0.4:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif pos=='right':\n",
    "        if bb[0]>0.6:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif pos=='top':\n",
    "        if bb[1]<0.4:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif pos=='bottom':\n",
    "        if bb[1]>0.6:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80de85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_no_score(bbox,input_image,text=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    for i,box in enumerate(bbox):\n",
    "        cx, cy, w, h = box\n",
    "        ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
    "    if text is not None:\n",
    "        ax.set_title(text,  fontsize=12)\n",
    "            \n",
    "def verify_attr_ans(layout,img_id,ques_id,showing=False):\n",
    "    attr=layout[-2]['argument'][0]#the last filter module\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    if len(layout)==3:\n",
    "        _,bbox,_=bbox_generator(layout,0,img_id,ques_id)\n",
    "    else:\n",
    "        bbox,cap=phrase_bbox_generator_exist(layout,len(layout)-2,img_id)\n",
    "    if attr in ['left','right','top','bottom']:\n",
    "        pred=filter_position(bbox,attr,input_image,showing)\n",
    "    else:\n",
    "        pred=filter_binary_color(bbox,input_image,attr,showing)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5d17f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scene_ans(layout,img_id,ques_id,showing=False):\n",
    "    attr=layout[-2]['argument'][0]#the last filter module\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    bbox=[0.5,0.5,0.99,0.99]#the whole image\n",
    "    pred=filter_binary_color(bbox,input_image,attr,showing)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "747298b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def existence_for_all(layout,img_id,ques_id):\n",
    "    ops=[step['operation'] for step in layout]\n",
    "    if 'filter' not in ops:\n",
    "        pred=generate_basic_ans(layout,img_id,ques_id)\n",
    "    elif 'relocate' not in ops:\n",
    "        if layout[0]['argument'][0]=='scene':\n",
    "            pred=generate_scene_ans(layout,img_id,ques_id)\n",
    "        else:\n",
    "            pred=verify_attr_ans(layout,img_id,ques_id)\n",
    "    else:\n",
    "        pred=relocate_involved_ans(layout,img_id,ques_id)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2395c738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68167\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "colors=defaultdict(int)\n",
    "materials=defaultdict(int)\n",
    "shapes=defaultdict(int)\n",
    "for k,name in enumerate(names):\n",
    "    row=gqa_val_q[name]\n",
    "    layout=row['semantic']\n",
    "    clean_layout=update_program(layout)\n",
    "    new_prog=mine_clean_all(clean_layout)\n",
    "    if new_prog[-1]['operation']!='query':\n",
    "        continue\n",
    "    if len(new_prog[-1]['argument'])==0:\n",
    "        continue\n",
    "    query_t=new_prog[-1]['argument'][0]\n",
    "    if query_t=='color':\n",
    "        colors[row['answer']]+=1\n",
    "    elif query_t=='material':\n",
    "        materials[row['answer']]+=1\n",
    "    elif query_t=='shape':\n",
    "        shapes[row['answer']]+=1\n",
    "    vis+=1\n",
    "print (vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a45f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_list=[]\n",
    "hold_list.extend([c for c in colors])\n",
    "hold_list.extend([c for c in materials])\n",
    "hold_list.extend([c for c in shapes])\n",
    "hold_list=set(hold_list)\n",
    "#print (hold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dade6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_prog(layout):\n",
    "    first_prog=[]\n",
    "    second_prog=[]\n",
    "    flag=True#whether meet the first exist module\n",
    "    flag_idx=0\n",
    "    inserts=0\n",
    "    for i ,step in enumerate(layout[:-1]):\n",
    "        if flag:\n",
    "            first_prog.append(step)\n",
    "            if step['operation']=='exist':\n",
    "                flag=False\n",
    "                flag_idx=i+1\n",
    "            continue\n",
    "        dep=step['dependencies']\n",
    "        second_prog.append(step)\n",
    "        while len(dep)>0 and dep[0]<flag_idx-1:\n",
    "            idx=dep[0]\n",
    "            insertion=layout[idx]\n",
    "            second_prog.insert(0,insertion)\n",
    "            inserts+=1\n",
    "            dep=insertion['dependencies']\n",
    "    for step in second_prog:\n",
    "        dep=step['dependencies']\n",
    "        if len(dep)>0:\n",
    "            idx=dep[0]\n",
    "            if idx>=flag_idx:\n",
    "                dep[0]-=flag_idx\n",
    "                dep[0]+=inserts\n",
    "    return first_prog,second_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "114948bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prog(layout):\n",
    "    for i,step in enumerate(layout):\n",
    "        dep=[str(d) for d in step['dependencies']]\n",
    "        dep=','.join(dep)\n",
    "        dep='  '+dep\n",
    "        print('\\t','--'.join([str(i),step['operation'],step['argument'][0],dep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c80c3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_logical(layout,img_id,ques_id):\n",
    "    first_prog,second_prog=separate_prog(layout)\n",
    "    first_ans=existence_for_all(first_prog,img_id,ques_id)\n",
    "    second_ans=existence_for_all(second_prog,img_id,ques_id)\n",
    "    logic=layout[-1]['operation']\n",
    "    if logic=='and':\n",
    "        if first_ans=='yes' and second_ans=='yes':\n",
    "            return 'yes',first_ans,second_ans\n",
    "        else:\n",
    "            return 'no',first_ans,second_ans\n",
    "    elif logic=='or':\n",
    "        if first_ans=='no' and second_ans=='no':\n",
    "            return 'no',first_ans,second_ans\n",
    "        else:\n",
    "            return 'yes',first_ans,second_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3314cf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already finished 500\n",
      "\tAccuracy 69.0\n",
      "\t 0\n",
      "Invalid caption generation for 2394519 pillow  pillow to the right of pillow \n",
      "Invalid caption generation for 2394519 pillow  pillow to the right of pillow \n",
      "Already finished 1000\n",
      "\tAccuracy 69.2\n",
      "\t 0\n",
      "Already finished 1500\n",
      "\tAccuracy 68.06666666666666\n",
      "\t 0\n",
      "Invalid caption generation for 2321443 pillow  pillow to the right of pillow \n",
      "Invalid caption generation for 2321443 pillow  pillow to the right of pillow \n",
      "Already finished 2000\n",
      "\tAccuracy 67.5\n",
      "\t 0\n",
      "Already finished 2500\n",
      "\tAccuracy 67.8\n",
      "\t 0\n",
      "Already finished 3000\n",
      "\tAccuracy 68.4\n",
      "\t 0\n",
      "Already finished 3500\n",
      "\tAccuracy 68.4\n",
      "\t 1\n",
      "Already finished 4000\n",
      "\tAccuracy 68.8\n",
      "\t 1\n",
      "Already finished 4500\n",
      "\tAccuracy 68.84444444444445\n",
      "\t 2\n",
      "Already finished 5000\n",
      "\tAccuracy 69.4\n",
      "\t 2\n",
      "Already finished 5500\n",
      "\tAccuracy 69.03636363636363\n",
      "\t 2\n",
      "Already finished 6000\n",
      "\tAccuracy 69.11666666666666\n",
      "\t 2\n",
      "Already finished 6500\n",
      "\tAccuracy 69.3076923076923\n",
      "\t 3\n",
      "Already finished 7000\n",
      "\tAccuracy 69.18571428571428\n",
      "\t 3\n",
      "Already finished 7500\n",
      "\tAccuracy 69.26666666666667\n",
      "\t 4\n",
      "Already finished 8000\n",
      "\tAccuracy 69.0625\n",
      "\t 6\n",
      "Already finished 8500\n",
      "\tAccuracy 69.18823529411765\n",
      "\t 6\n",
      "Already finished 9000\n",
      "\tAccuracy 69.35555555555555\n",
      "\t 8\n",
      "Already finished 9500\n",
      "\tAccuracy 69.42105263157895\n",
      "\t 8\n",
      "Already finished 10000\n",
      "\tAccuracy 69.58\n",
      "\t 8\n",
      "Already finished 10500\n",
      "\tAccuracy 69.48571428571428\n",
      "\t 8\n",
      "Already finished 11000\n",
      "\tAccuracy 69.5909090909091\n",
      "\t 9\n",
      "Already finished 11500\n",
      "\tAccuracy 69.5304347826087\n",
      "\t 11\n",
      "Already finished 12000\n",
      "\tAccuracy 69.65833333333333\n",
      "\t 11\n",
      "Already finished 12500\n",
      "\tAccuracy 69.632\n",
      "\t 11\n",
      "Already finished 13000\n",
      "\tAccuracy 69.68461538461538\n",
      "\t 11\n",
      "Already finished 13500\n",
      "\tAccuracy 69.65925925925926\n",
      "\t 11\n",
      "Already finished 14000\n",
      "\tAccuracy 69.62142857142857\n",
      "\t 11\n",
      "Already finished 14500\n",
      "\tAccuracy 69.60689655172413\n",
      "\t 12\n",
      "Already finished 15000\n",
      "\tAccuracy 69.65333333333334\n",
      "\t 12\n",
      "Already finished 15500\n",
      "\tAccuracy 69.49677419354839\n",
      "\t 13\n",
      "Invalid caption generation for 2324202 pillow  pillow to the right of pillow \n",
      "Invalid caption generation for 2324202 pillow  pillow to the right of pillow \n",
      "Already finished 16000\n",
      "\tAccuracy 69.5625\n",
      "\t 13\n",
      "16096\n",
      "13\n",
      "69.56386679920477\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "hold_dict={'hposition': 6502, 'name': 47463, 'color': 7790, \n",
    "           'material': 1387, 'size': 860, 'place': 2166, 'length': 203}\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "invalid=[]\n",
    "acc=0.0\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    if row['types']['structural']!='logical':\n",
    "        continue\n",
    "    layout=row['semantic']\n",
    "    clean_layout=update_program(layout)\n",
    "    new_prog=mine_clean_all(clean_layout)\n",
    "    texts=[' '.join([str(vis),row['question'],row['imageId'],name])]\n",
    "    for i, step in enumerate(new_prog[:-1]):\n",
    "        dep=[str(d) for d in step['dependencies']]\n",
    "        dep=','.join(dep)\n",
    "        dep='  '+dep\n",
    "        texts.append('--'.join([str(i),step['operation'],step['argument'][0],dep]))\n",
    "    texts.append(new_prog[-1]['operation'])\n",
    "    texts='\\n'.join(texts)\n",
    "    \"\"\"fs_prog,sd_prog=separate_prog(new_prog)\n",
    "    print_prog(fs_prog)\n",
    "    print ('\\n')\n",
    "    print_prog(sd_prog)\n",
    "    print ('\\n')\"\"\"\n",
    "    #pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    try:\n",
    "        pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('Already finished',vis)\n",
    "        print ('\\tAccuracy',acc*100.0/vis)\n",
    "        print ('\\t',len(invalid))\n",
    "        \n",
    "print (vis)\n",
    "print (len(invalid))\n",
    "print (acc*100.0/vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f83352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
