{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45409e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "from matplotlib import patches,  lines\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283e81d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "VQA_PATH='/Data_Storage/Rui_Data_Space/multimodal/VQA'\n",
    "GQA_PATH='/Data_Storage/Rui_Data_Space/multimodal/GQA'\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def load_pkl(path):\n",
    "    data=pkl.load(open(path,'rb'))\n",
    "    return data\n",
    "\n",
    "def load_json(path):\n",
    "    data=json.load(open(path,'r'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b39971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09afc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "stemmer = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5640a60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132062\n",
      "10696\n"
     ]
    }
   ],
   "source": [
    "gqa_val_q=json.load(\n",
    "    open(os.path.join(GQA_PATH,'original','val_balanced_questions.json')\n",
    "         ,'r')\n",
    ")\n",
    "print (len(gqa_val_q))\n",
    "names=list(gqa_val_q.keys())\n",
    "\n",
    "val_graphs=json.load(\n",
    "    open(os.path.join(GQA_PATH,'val_sceneGraphs.json')\n",
    "         ,'r')\n",
    ")\n",
    "print (len(val_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b9afb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage import io as skimage_io\n",
    "from skimage import transform as skimage_transform\n",
    "\n",
    "def get_tokens(text_queries):\n",
    "    tokenized_queries = np.array([\n",
    "        module.tokenize(q, config.dataset_configs.max_query_length)\n",
    "        for q in text_queries\n",
    "    ])\n",
    "    # Pad tokenized queries to avoid recompilation if number of queries changes:\n",
    "    tokenized_queries = np.pad(\n",
    "        tokenized_queries,\n",
    "        pad_width=((0, 100 - len(text_queries)), (0, 0)),\n",
    "        constant_values=0)\n",
    "    return tokenized_queries\n",
    "\n",
    "def get_gqa_feat(img_id):\n",
    "    # Load example image:\n",
    "    filename = os.path.join(GQA_PATH,'images',img_id+'.jpg')\n",
    "    image_uint8 = skimage_io.imread(filename)\n",
    "    image = image_uint8.astype(np.float32) / 255.0\n",
    "\n",
    "    # Pad to square with gray pixels on bottom and right:\n",
    "    h, w, _ = image.shape\n",
    "    size = max(h, w)\n",
    "    image_padded = np.pad(\n",
    "        image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)\n",
    "\n",
    "    # Resize to model input size:\n",
    "    input_image = skimage.transform.resize(\n",
    "        image_padded,\n",
    "        (840, 840),\n",
    "        anti_aliasing=True)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a6a2c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ashkamath_mdetr_main\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "CUDA_DEVICE=15\n",
    "torch.cuda.set_device(CUDA_DEVICE)\n",
    "device = torch.device(\"cuda:\"+str(CUDA_DEVICE))\n",
    "#the default ipykernel links to the first conda environment\n",
    "\n",
    "model, postprocessor = torch.hub.load('ashkamath/mdetr:main', 'mdetr_efficientnetB5', pretrained=True, return_postprocessor=True)\n",
    "model = model.to(device)\n",
    "model.eval();\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb50a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference(im, caption, plot=True):\n",
    "    # mean-std normalize the input image (batch-size: 1)\n",
    "    img = transform(im).unsqueeze(0).cuda()\n",
    "\n",
    "    # propagate through the model\n",
    "    memory_cache = model(img, [caption], encode_and_save=True)\n",
    "    outputs = model(img, [caption], encode_and_save=False, memory_cache=memory_cache)\n",
    "    #print (outputs['pred_logits'])\n",
    "\n",
    "    # keep only predictions with 0.7+ confidence\n",
    "    probas = 1 - outputs['pred_logits'].softmax(-1)[0, :, -1].cpu()\n",
    "    keep = (probas > 0.7).cpu()\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'].cpu()[0, keep], im.size)\n",
    "\n",
    "    # Extract the text spans predicted by each box\n",
    "    positive_tokens = (outputs[\"pred_logits\"].cpu()[0, keep].softmax(-1) > 0.1).nonzero().tolist()\n",
    "    predicted_spans = defaultdict(str)\n",
    "    for tok in positive_tokens:\n",
    "        item, pos = tok\n",
    "        if pos < 255:\n",
    "            span = memory_cache[\"tokenized\"].token_to_chars(0, pos)\n",
    "            predicted_spans [item] += \" \" + caption[span.start:span.end]\n",
    "\n",
    "    labels = [predicted_spans [k] for k in sorted(list(predicted_spans .keys()))]\n",
    "    if plot:\n",
    "        plot_results(im, probas[keep], bboxes_scaled, labels)\n",
    "    return probas[keep], bboxes_scaled.tolist(), labels\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c] * 255,\n",
    "                                  image[:, :, c])\n",
    "    return image\n",
    "\n",
    "def plot_results(pil_img, scores, boxes, labels, masks=None):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    np_image = np.array(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    if masks is None:\n",
    "        masks = [None for _ in range(len(scores))]\n",
    "    assert len(scores) == len(boxes) == len(labels) == len(masks)\n",
    "    for s, (xmin, ymin, xmax, ymax), l, mask, c in zip(scores, boxes.tolist(), labels, masks, colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f'{l}: {s:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "        if mask is None:\n",
    "            continue\n",
    "        np_image = apply_mask(np_image, mask, c)\n",
    "\n",
    "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            # Subtract the padding and flip (y, x) to (x, y)\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=c)\n",
    "            ax.add_patch(p)\n",
    "\n",
    "\n",
    "    plt.imshow(np_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_res(results, ax, color='green'):\n",
    "    #for tt in results.values():\n",
    "    if True:\n",
    "        bboxes = results['boxes']\n",
    "        labels = results['labels']\n",
    "        scores = results['scores']\n",
    "        #keep = scores >= 0.0\n",
    "        #bboxes = bboxes[keep].tolist()\n",
    "        #labels = labels[keep].tolist()\n",
    "        #scores = scores[keep].tolist()\n",
    "    #print(torchvision.ops.box_iou(tt['boxes'].cpu().detach(), torch.as_tensor([[xmin, ymin, xmax, ymax]])))\n",
    "    \n",
    "    colors = ['purple', 'yellow', 'red', 'green', 'orange', 'pink']\n",
    "    \n",
    "    for i, (b, ll, ss) in enumerate(zip(bboxes, labels, scores)):\n",
    "        ax.add_patch(plt.Rectangle((b[0], b[1]), b[2] - b[0], b[3] - b[1], fill=False, color=colors[i], linewidth=3))\n",
    "        cls_name = ll if isinstance(ll,str) else CLASSES[ll]\n",
    "        text = f'{cls_name}: {ss:.2f}'\n",
    "        print(text)\n",
    "        ax.text(b[0], b[1], text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b91db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_bbox_only(bbox,scores,input_image,gt_bbox=None,text=None,threshold=0.2,vis_pred=True):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    if vis_pred:\n",
    "        for i,box in enumerate(bbox):\n",
    "            score=scores[i]\n",
    "            if score<threshold:\n",
    "                continue\n",
    "            cx, cy, w, h = box\n",
    "            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
    "            ax.text(\n",
    "                cx - w / 2,\n",
    "                cy + h / 2 + 0.015,\n",
    "                f'{score:1.2f}',\n",
    "                ha='left',\n",
    "                va='top',\n",
    "                color='red',\n",
    "                bbox={\n",
    "                    'facecolor': 'white',\n",
    "                    'edgecolor': 'red',\n",
    "                    'boxstyle': 'square,pad=.3'\n",
    "                })\n",
    "    if text is not None:\n",
    "        ax.set_title(text,  fontsize=12)\n",
    "    if gt_bbox is not None:\n",
    "        for i,box in enumerate(gt_bbox):\n",
    "            cx, cy, w, h = box\n",
    "            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "476ac4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "def size_aware_pred(bb):\n",
    "    if bb[2]>0.3 or bb[3]>0.3:\n",
    "        return 'large'\n",
    "    else:\n",
    "        return 'small'\n",
    "def pos_aware_pred(valid_bbox,candidates):\n",
    "    if 'bottom' in candidates:\n",
    "        if valid_bbox[1]>0.5:\n",
    "            return 'bottom'\n",
    "        else:\n",
    "            return 'top'\n",
    "    elif 'right' in candidates:\n",
    "        if valid_bbox[0]<0.5:\n",
    "            return 'left'\n",
    "        else:\n",
    "            return 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e9592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coord(bbox,img_id):\n",
    "    width=val_graphs[img_id]['width']\n",
    "    height=val_graphs[img_id]['height']\n",
    "    size = max(height, width)\n",
    "    x=bbox[0]\n",
    "    y=bbox[1]\n",
    "    w=bbox[2]-bbox[0]\n",
    "    h=bbox[3]-bbox[1]\n",
    "    #print (bbox)\n",
    "    return [(x+w/2)/size,(y+h/2)/size,w/size,h/size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa8dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjectives(text):\n",
    "    sent=text.split(',')[0]\n",
    "    words=sent.split(' ')\n",
    "    if 'less' in words:\n",
    "        words=words[-2:]\n",
    "    else:\n",
    "        words=words[-1:]\n",
    "    words=' '.join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2ec65ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mdert_result(scores,bboxs,labels,obj,img_id):\n",
    "    sf_s=[]\n",
    "    sf_bbox=[]\n",
    "    labels=[''.join(label.split(' ')) for label in labels]\n",
    "    flag=False\n",
    "    if obj in labels:\n",
    "        flag=True\n",
    "    #print (obj,labels)\n",
    "    for i,label in enumerate(labels):\n",
    "        if flag:\n",
    "            if obj==label:\n",
    "                sf_s.append(scores[i])\n",
    "                sf_bbox.append(bboxs[i])\n",
    "        else:\n",
    "            if obj in label:\n",
    "                sf_s.append(scores[i])\n",
    "                sf_bbox.append(bboxs[i])\n",
    "    #print (len(sf_s))\n",
    "    if len(sf_bbox)==0:\n",
    "        for i,label in enumerate(labels):\n",
    "            sf_s.append(scores[i])\n",
    "            sf_bbox.append(bboxs[i])\n",
    "    max_s=max(sf_s)\n",
    "    max_id=sf_s.index(max_s)\n",
    "    return_bbox=sf_bbox[max_id]\n",
    "    coord=transform_coord(return_bbox,img_id)\n",
    "    return max_s.item(),coord\n",
    "#only consider one bbox now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92edebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase_ground(img_id,cap,obj):\n",
    "    im=Image.open(os.path.join(GQA_PATH,'images',img_id+'.jpg'))\n",
    "    try:\n",
    "        scores,bboxs,labels=plot_inference(im, cap, plot=False)\n",
    "    except:\n",
    "        print ('Invalid caption generation for',img_id,obj,cap)\n",
    "        scores=[torch.Tensor([0.99])]\n",
    "        bboxs=[[0.5,0.5,0.99,0.99]]\n",
    "        labels=[obj]\n",
    "    if len(bboxs)==0:\n",
    "        max_s=[0.99]\n",
    "        valid_bbox=[0.5,0.5,0.99,0.99]\n",
    "    else:\n",
    "        # print (scores,bboxs)\n",
    "        max_s,valid_bbox=preprocess_mdert_result(scores,bboxs,labels,obj,img_id)\n",
    "    return max_s,valid_bbox\n",
    "\n",
    "def bbox_generator(layout,idx,img_id,ques_id):\n",
    "    if layout[idx]['operation']=='select':\n",
    "        #print (idx)\n",
    "        try:\n",
    "            scenic_result=load_pkl(os.path.join(GQA_PATH,\n",
    "                                                'features/one-all-scenic',\n",
    "                                                ques_id+'.pkl'))\n",
    "            obj=layout[idx]['argument'][0]\n",
    "            scores=[s['score'] for s in scenic_result[obj]]\n",
    "            bboxs=[s['bbox'] for s in scenic_result[obj]]\n",
    "        except:\n",
    "            bboxs=[]\n",
    "            obj=layout[idx]['argument'][0]\n",
    "        if len(bboxs)>0:\n",
    "            max_s=max(scores)\n",
    "            max_idx=scores.index(max_s)\n",
    "            bbox=bboxs[max_idx]\n",
    "            cap=obj\n",
    "        else:\n",
    "            cap=obj\n",
    "            split_obj=''.join(obj.split(' '))\n",
    "            #print (cap,split_obj)\n",
    "            max_s,bbox=generate_phrase_ground(img_id,cap,split_obj)\n",
    "            #print (max_s,bbox)\n",
    "    else:\n",
    "        cap,obj=cap_generator(layout,idx,img_id)\n",
    "        max_s,bbox=generate_phrase_ground(img_id,cap,obj)\n",
    "    return max_s,bbox,cap        \n",
    "            \n",
    "def cap_generator(layout,idx,img_id):\n",
    "    words=[]\n",
    "    dep=layout[idx]['dependencies']\n",
    "    word=layout[idx]['argument'][0]\n",
    "    words.append(word)\n",
    "    obj=''.join(word.split(' '))\n",
    "    while len(dep)>0:\n",
    "        cur_step=layout[dep[0]]\n",
    "        if cur_step['operation']=='relocate':\n",
    "            relo_symbol=cur_step['argument'][1]\n",
    "            cur_phrase=' '.join(words)\n",
    "            words=[]\n",
    "            words.append(cur_phrase)\n",
    "            #print(cur_step)\n",
    "            words.append(cur_step['argument'][0])\n",
    "            dep=cur_step['dependencies']\n",
    "            cur_step=layout[dep[0]]\n",
    "            dep=cur_step['dependencies']\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            if relo_symbol=='o':\n",
    "                words=list(reversed(words))\n",
    "        else:\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            dep=cur_step['dependencies']\n",
    "    cap=' '.join(words)\n",
    "    return cap,obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63cd40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped(bb,input_image,showing=False):\n",
    "    width=bb[2]*840\n",
    "    height=bb[3]*840\n",
    "    xy=(bb[0]*840-width/2,bb[1]*840-height/2)\n",
    "    h0=int(max(0,xy[1]))\n",
    "    w0=int(max(0,xy[0]))\n",
    "    h1=int(max(0,840-(height+xy[1])))\n",
    "    w1=int(max(0,840-(width+xy[0])))\n",
    "    cropped=skimage.util.crop(input_image,((h0,h1),(w0,w1),(0,0)), copy=False)\n",
    "    trans_crop=Image.fromarray(np.uint8(cropped*255.0))\n",
    "    if showing:\n",
    "        display.display(trans_crop)\n",
    "    return trans_crop\n",
    "\n",
    "def clip_aware_pred(img_feat,valid_bbox,candidates,showing=False):\n",
    "    #tokens=clip.tokenize(candidates)\n",
    "    trans_crop=get_cropped(valid_bbox,img_feat)\n",
    "    if showing:\n",
    "        display.display(trans_crop)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "        text_features = clip_model.encode_text(clip.tokenize(candidates).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    #print (indices[0][0].item())\n",
    "    ans=candidates[indices[0][0].item()]\n",
    "    return ans\n",
    "\n",
    "def clip_aware_pred_compare(img_feat,adj,candidates):\n",
    "    #tokens=clip.tokenize(candidates)\n",
    "    trans_crop=get_cropped([0.5,0.5,0.99,0.99],img_feat)\n",
    "    tokens=[adj+c for c in candidates]\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "        text_features = clip_model.encode_text(clip.tokenize(tokens).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    #print (indices[0][0].item())\n",
    "    ans=candidates[indices[0][0].item()]\n",
    "    return ans\n",
    "\n",
    "def generate_choose_one_ans(img_feat,valid_bbox,candidates):\n",
    "    if 'bottom' in candidates or 'right' in candidates:\n",
    "        pred=pos_aware_pred(valid_bbox,candidates)\n",
    "    elif 'small' in candidates or 'giant' in candidates:\n",
    "        pred=size_aware_pred(valid_bbox)\n",
    "    else:\n",
    "        #for debug only\n",
    "        #pred=clip_aware_pred(img_feat,valid_bbox,candidates,True)\n",
    "        pred=clip_aware_pred(img_feat,valid_bbox,candidates)\n",
    "    return pred\n",
    "\n",
    "def generate_choose_two_ans(img_feat,scores,bboxs,valid_bbox,candidates,ques_id,thresh=0.2):\n",
    "    flag=False#whether the detection is confident enough\n",
    "    if 'to the left of' in candidates:\n",
    "        if bboxs[0]<valid_bbox[0]:\n",
    "            return 'left'\n",
    "        else:\n",
    "            return 'right'\n",
    "    elif 'below' in candidates:\n",
    "        #this one is confusing!\n",
    "        if bboxs[1]>valid_bbox[1]:\n",
    "            return 'below'\n",
    "        else:\n",
    "            return 'above'\n",
    "    elif 'behind' in candidates:\n",
    "        #do not know how to decide\n",
    "        return 'behind'\n",
    "    else:\n",
    "        ques=gqa_val_q[ques_id]['question']\n",
    "        adj=get_adjectives(ques)\n",
    "        pred=clip_aware_pred_compare(img_feat,adj,candidates)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10d25b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_answers=['color','shape','material']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00ad08f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list=['white', 'black', 'blue', 'brown', 'gray', 'green',\n",
    "            'red', 'yellow', 'orange', 'pink', 'silver', 'dark', 'tan',\n",
    "            'purple', 'gold', 'blond', 'beige', 'light brown', \n",
    "            'light blue', 'dark brown', 'maroon', 'cream colored', 'dark blue', 'khaki']\n",
    "material_list=['wood', 'metal', 'plastic', 'glass', 'concrete', 'brick', 'leather', 'porcelain']\n",
    "shape_list=['triangular', 'square', 'round', 'rectangular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d85a085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 512])\n"
     ]
    }
   ],
   "source": [
    "COLOR_TEMP='[COLOR]'\n",
    "color_text=[COLOR_TEMP.replace('[COLOR]',c) for c in color_list]\n",
    "with torch.no_grad():\n",
    "    COLOR_TEXT_FEAT= clip_model.encode_text(clip.tokenize(color_text).to(device))\n",
    "COLOR_TEXT_FEAT/=COLOR_TEXT_FEAT.norm(dim=-1, keepdim=True)\n",
    "print (COLOR_TEXT_FEAT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03217a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_sim_dict={c:[] for c in color_list}\n",
    "for idx, color in enumerate(color_list):\n",
    "    print(color)\n",
    "    feat_1=COLOR_TEXT_FEAT[idx]\n",
    "    mask=[0 for i in range(24)]\n",
    "    mask[idx]=1\n",
    "    mask=torch.Tensor(mask).to(device).bool()\n",
    "    feat_2=COLOR_TEXT_FEAT[0]\n",
    "    sim=(100 * COLOR_TEXT_FEAT @ feat_1.T).float().masked_fill(mask,-1e30).softmax(dim=-1)\n",
    "    values, indices = sim.topk(5)\n",
    "    for i in range(5):\n",
    "        score=values[i].item()\n",
    "        if score<0.25:\n",
    "            break\n",
    "        cur_other_idx=indices[i].item()\n",
    "        color_sim_dict[color].append(color_list[cur_other_idx])\n",
    "    print ('\\tSimilar colors:',color_sim_dict[color])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff59f035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "SHAPE_TEMP='[SHAPE]'\n",
    "shape_text=[SHAPE_TEMP.replace('[SHAPE]',c) for c in shape_list]\n",
    "with torch.no_grad():\n",
    "    SHAPE_TEXT_FEAT= clip_model.encode_text(clip.tokenize(shape_text).to(device))\n",
    "SHAPE_TEXT_FEAT/=SHAPE_TEXT_FEAT.norm(dim=-1, keepdim=True)\n",
    "print (SHAPE_TEXT_FEAT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71f6f803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "MATERIAL_TEMP='[MATERIAL]'\n",
    "material_text=[MATERIAL_TEMP.replace('[MATERIAL]',c) for c in material_list]\n",
    "with torch.no_grad():\n",
    "    MATERIAL_TEXT_FEAT= clip_model.encode_text(clip.tokenize(material_text).to(device))\n",
    "MATERIAL_TEXT_FEAT/=MATERIAL_TEXT_FEAT.norm(dim=-1, keepdim=True)\n",
    "print (MATERIAL_TEXT_FEAT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "759cdf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(trans_crop):\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ COLOR_TEXT_FEAT.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    color=color_list[indices[0][0].item()]\n",
    "    return color,values[0][0].item()\n",
    "\n",
    "def get_material(trans_crop):\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ MATERIAL_TEXT_FEAT.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    color=material_list[indices[0][0].item()]\n",
    "    return color,values[0][0].item()\n",
    "\n",
    "def get_shape(trans_crop):\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ SHAPE_TEXT_FEAT.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    color=shape_list[indices[0][0].item()]\n",
    "    return color,values[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5243488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attribute(layout,idx,img_id,ques_id,img_feat):\n",
    "    _,bbox,_=bbox_generator(layout,idx,img_id,ques_id)\n",
    "    crop_img=get_cropped(bbox,img_feat)#showing set to be true\n",
    "    color,_=get_color(crop_img)\n",
    "    material,_=get_material(crop_img)\n",
    "    shape,_=get_shape(crop_img)\n",
    "    results={\n",
    "        'color':color,\n",
    "        'material':material,\n",
    "        'shape':shape\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f4335086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_common(layout,img_id,ques_id,showing=False):\n",
    "    img_feat=get_gqa_feat(img_id)\n",
    "    attr_0=get_attribute(layout,0,img_id,ques_id,img_feat)\n",
    "    attr_1=get_attribute(layout,1,img_id,ques_id,img_feat)\n",
    "    \n",
    "    if showing:\n",
    "        print ('\\t',attr_0['color'],attr_1['color'])\n",
    "        print ('\\t',attr_0['material'],attr_1['material'])\n",
    "        print ('\\t',attr_0['shape'],attr_1['shape'])\n",
    "        \n",
    "    if attr_0['color'] in color_sim_dict[attr_1['color']] or attr_1['color'] in color_sim_dict[attr_0['color']]:\n",
    "        pred='color'\n",
    "    elif attr_0['color']==attr_1['color']:\n",
    "        pred='color'\n",
    "    elif attr_0['material']==attr_1['material']:\n",
    "        pred='material'\n",
    "    else:\n",
    "        pred='shape'\n",
    "        #print (attr_0['shape'],attr_1['shape'])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8affa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMON_TEMP='[OBJ_0] and [OBJ_1] are of the same [ATTR]'\n",
    "COMMON_TEMP='[OBJ_0] and [OBJ_1] have the same [ATTR]'\n",
    "#COMMON_TEMP='[OBJ_0] is the same [ATTR] as [OBJ_1]'\n",
    "def direct_prompt_clip_common(layout,img_id):\n",
    "    obj_0=layout[0]['argument'][0]\n",
    "    obj_1=layout[1]['argument'][0]\n",
    "    sent=COMMON_TEMP.replace('[OBJ_0]',obj_0).replace('[OBJ_1]',obj_1)\n",
    "    prompts=[sent.replace('[ATTR]',c) for c in common_answers]\n",
    "    #print(prompts,img_id)\n",
    "    img_path=os.path.join(GQA_PATH,'images',img_id+'.jpg')\n",
    "    im=Image.open(img_path)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(im).unsqueeze(0).to(device))\n",
    "        text_features = clip_model.encode_text(clip.tokenize(prompts).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    ans=common_answers[indices[0][0].item()]\n",
    "    return ans,similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "861d87d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 37.2463768115942\n",
      "690\n"
     ]
    }
   ],
   "source": [
    "#some examples from \n",
    "#compare and common\n",
    "\n",
    "#explicit prediction do not work well\n",
    "\n",
    "#direct prompting CLIP model: whole image and the prompts\n",
    "#random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "        \n",
    "    row=gqa_val_q[name]\n",
    "    layout=row['semantic']\n",
    "    clean_layout=update_program(layout)\n",
    "    new_prog=mine_clean_all(clean_layout)\n",
    "    if new_prog[-1]['operation']!='common':\n",
    "        continue\n",
    "    #print (vis,row['imageId'],row['question'],row['answer'],name)\n",
    "    pred,score=direct_prompt_clip_common(new_prog,row['imageId'])\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    #print ('\\tprediction',pred,'\\n\\t',score)\n",
    "    #for i,step in enumerate(new_prog):\n",
    "    #    print (step['operation'],step['argument'],step['dependencies'])\n",
    "    vis+=1\n",
    "print ('accuracy',acc*100.0/vis)\n",
    "print (vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba04dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMMON_TEMP='[OBJ_0] and [OBJ_1] are of the same [ATTR]'\n",
    "COMMON_TEMP='[OBJ_0] and [OBJ_1] have [ATTR]'\n",
    "ans_cand=['same','different']\n",
    "#COMMON_TEMP='[OBJ_0] is the same [ATTR] as [OBJ_1]'\n",
    "def direct_prompt_clip_compare(layout,img_id):\n",
    "    if len(layout)==2:\n",
    "        sent=layout[0]['argument'][0]+' have [ATTR]'\n",
    "        attr=['same '+layout[1]['argument'][1],'different '+layout[1]['argument'][1]]\n",
    "        gt=layout[1]['argument'][0]\n",
    "    else:\n",
    "        obj_0=layout[0]['argument'][0]\n",
    "        obj_1=layout[1]['argument'][0]\n",
    "        attr=['same '+layout[2]['argument'][1],'different '+layout[2]['argument'][1]]\n",
    "        sent=COMMON_TEMP.replace('[OBJ_0]',obj_0).replace('[OBJ_1]',obj_1)\n",
    "        gt=layout[2]['argument'][0]\n",
    "    prompts=[sent.replace('[ATTR]',c) for c in attr]\n",
    "    #print(prompts,img_id)\n",
    "    img_path=os.path.join(GQA_PATH,'images',img_id+'.jpg')\n",
    "    im=Image.open(img_path)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(im).unsqueeze(0).to(device))\n",
    "        text_features = clip_model.encode_text(clip.tokenize(prompts).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    ans=ans_cand[indices[0][0].item()]\n",
    "    if ans==gt:\n",
    "        return 'yes',values[0][0].item()\n",
    "    else:\n",
    "        return 'no',values[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb305f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 51.83585313174946\n",
      "3241\n"
     ]
    }
   ],
   "source": [
    "#mask rest regions\n",
    "#some examples from \n",
    "#compare and common\n",
    "\n",
    "#explicit prediction do not work well\n",
    "#direct prompt does not work well\n",
    "\n",
    "#random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>3:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    layout=row['semantic']\n",
    "    clean_layout=update_program(layout)\n",
    "    new_prog=mine_clean_all(clean_layout)\n",
    "    if new_prog[-1]['operation']!='compare':\n",
    "        continue\n",
    "    \n",
    "    \"\"\"print (vis,row['imageId'],row['question'],row['answer'],name)\n",
    "    print ('\\tprediction',pred)\n",
    "    for i,step in enumerate(new_prog):\n",
    "        print (step['operation'],step['argument'],step['dependencies'])\"\"\"\n",
    "    pred,score=direct_prompt_clip_compare(new_prog,row['imageId'])\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    vis+=1\n",
    "print ('accuracy',acc*100.0/vis)\n",
    "print (vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9b97f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
