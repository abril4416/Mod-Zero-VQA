{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b497100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "from matplotlib import patches,  lines\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c0acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "VQA_PATH='/Data_Storage/Rui_Data_Space/multimodal/VQA'\n",
    "GQA_PATH='/Data_Storage/Rui_Data_Space/multimodal/GQA'\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def load_pkl(path):\n",
    "    data=pkl.load(open(path,'rb'))\n",
    "    return data\n",
    "\n",
    "def load_json(path):\n",
    "    data=json.load(open(path,'r'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ed96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71826039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vqa_prepro.modify_program as clean_modules\n",
    "\n",
    "def update_program(program):\n",
    "    program=clean_modules.eliminate_obj_id(program)\n",
    "    program=clean_modules.modify_choose(program)\n",
    "    program=clean_modules.modify_diff_same(program)\n",
    "    program=clean_modules.modify_verify(program)\n",
    "    program=clean_modules.modify_relate(program)\n",
    "    program=clean_modules.modify_filter(program)\n",
    "    return program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887c2744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_filter(prog):\n",
    "    new_prog=[]\n",
    "    flag=False\n",
    "    select_idx={i:0 for i in range(len(prog))}\n",
    "    ops=[step['operation'] for step in prog]\n",
    "    if 'attentionand' not in ops:\n",
    "        return prog\n",
    "    for k,p in enumerate(prog):\n",
    "        module=p['operation']\n",
    "        if  p['operation']=='select' and  prog[k+1]['operation']=='attentionand':\n",
    "            flag=True\n",
    "            for i in range(k,len(prog)):\n",
    "                select_idx[i]+=1\n",
    "            #print (select_idx)\n",
    "            dep=prog[k+1]['dependencies'][0]\n",
    "            dep-=select_idx[dep]\n",
    "            new_prog.append(\n",
    "                {'operation':'filter',\n",
    "                 'argument':p['argument'],\n",
    "                 'dependencies':[dep]}\n",
    "            )\n",
    "        elif p['operation']=='attentionand' and flag:\n",
    "            flag=False\n",
    "            continue\n",
    "        else:\n",
    "            if p['operation'] in ['and','or']:\n",
    "                dep=[]\n",
    "                exist_0=prog[p['dependencies'][0]]['dependencies'][0]\n",
    "                exist_1=prog[p['dependencies'][1]]['dependencies'][0]\n",
    "                dep.append(p['dependencies'][0]-select_idx[exist_0])\n",
    "                dep.append(p['dependencies'][1]-select_idx[exist_1])\n",
    "            else:\n",
    "                dep=[dep-select_idx[k] for dep in p['dependencies']]\n",
    "            new_prog.append(\n",
    "                {'operation':p['operation'],\n",
    "                 'argument':p['argument'],\n",
    "                 'dependencies':dep}\n",
    "            )\n",
    "    return new_prog\n",
    "\n",
    "def mine_clean_compare(prog):\n",
    "    if prog[-1]['operation']=='compare':\n",
    "        if len(prog)==4:\n",
    "            new_prog=[]\n",
    "            new_prog.append(prog[0])\n",
    "            new_prog.append(prog[1])\n",
    "            final=prog[-1]\n",
    "            new_prog.append({\n",
    "                'argument':final['argument'],\n",
    "                'dependencies':[0,1],\n",
    "                'operation':final['operation']\n",
    "            })\n",
    "            return new_prog\n",
    "    return prog\n",
    "\n",
    "def mine_choose(prog):\n",
    "    if prog[-1]['operation']!='choose' or prog[-2]['operation']!='exist':\n",
    "        return prog\n",
    "    if 'to the left of' not in prog[-1]['argument'] and\\\n",
    "                'behind' not in prog[-1]['argument'] and\\\n",
    "                'above' not in prog[-1]['argument']:\n",
    "        new_prog=[]\n",
    "        choose_from=prog[-1]['argument']\n",
    "        for k,p in enumerate(prog):\n",
    "            if p['operation']=='select' and p['argument'][0] in choose_from:\n",
    "                break\n",
    "            else:\n",
    "                new_prog.append(p)\n",
    "        if len(new_prog)==0:\n",
    "            new_prog.append(\n",
    "                {'operation':'select',\n",
    "                 'argument':choose_from[0],\n",
    "                 'dependencies':[]}\n",
    "            )\n",
    "            new_prog.append(\n",
    "                {'operation':'select',\n",
    "                 'argument':choose_from[1],\n",
    "                 'dependencies':[]}\n",
    "            )\n",
    "        new_prog.append(\n",
    "            {'operation':'choose',\n",
    "            'argument':choose_from,\n",
    "            'dependencies':[len(new_prog)-1]}\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        ops=[step['operation'] for step in prog]\n",
    "        #print (ops)\n",
    "        select_idx=ops.index('exist')\n",
    "        #print (select_idx)\n",
    "        deps=prog[select_idx-1]['dependencies']\n",
    "        dep_0=deps[0]\n",
    "        dep_1=deps[1]\n",
    "        choose_from=prog[-1]['argument']\n",
    "        new_prog=[]\n",
    "        new_prog.extend([prog[i] for i in range(dep_0)])\n",
    "        new_prog.extend([prog[i] for i in range(dep_0+1,dep_1+1)])\n",
    "        new_prog.append(\n",
    "            {'operation':'choose',\n",
    "             'argument':choose_from,\n",
    "             'dependencies':[dep_0-1,len(new_prog)-1]}\n",
    "        )\n",
    "        #print (new_prog)\n",
    "    if new_prog[0]['operation']=='select' and new_prog[1]['operation']=='select':\n",
    "        if len(new_prog)==3 and len(new_prog[-1]['dependencies'])==1:\n",
    "            new_prog[-1]['dependencies'].insert(0,0)\n",
    "    return new_prog\n",
    "\n",
    "def mine_not(prog):\n",
    "    ops=[step['operation'] for step in prog]\n",
    "    if 'attentionnot' not in ops:\n",
    "        return prog\n",
    "    select_idx={i:0 for i in range(len(prog))}\n",
    "    new_prog=[]\n",
    "    flag=True\n",
    "    \n",
    "    invalid=[]\n",
    "    for k,step in enumerate(prog):\n",
    "        if k in invalid:\n",
    "            continue\n",
    "        if step['operation']=='select' and prog[k+1]['operation']=='attentionnot':\n",
    "            flag=False\n",
    "            invalid.append(k+1)\n",
    "            invalid.append(k+2)\n",
    "            for i in range(k,len(prog)):\n",
    "                select_idx[i]+=2\n",
    "            new_prog.append(\n",
    "                {\n",
    "                    'operation':'filter',\n",
    "                    'dependencies':[k-1-select_idx[k-1]],\n",
    "                    'argument': ['not '+step['argument'][0]]\n",
    "                })\n",
    "        else:\n",
    "            if flag:\n",
    "                new_prog.append(step)\n",
    "            else:\n",
    "                dep=step['dependencies']\n",
    "                new_dep=[p-select_idx[p] for p in dep]\n",
    "                new_prog.append({\n",
    "                    'operation':step['operation'],\n",
    "                    'dependencies':new_dep,\n",
    "                    'argument':step['argument']\n",
    "                })\n",
    "    return new_prog\n",
    "\n",
    "def mine_clean_all(prog):\n",
    "    choose_prog=mine_choose(prog)\n",
    "    filter_prog=mine_filter(choose_prog)\n",
    "    not_prog=mine_not(filter_prog)\n",
    "    comp_prog=mine_clean_compare(not_prog)\n",
    "    for step in comp_prog:\n",
    "        if type(step['argument']) is not list:\n",
    "            step['argument']=[step['argument']]\n",
    "    return comp_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6923ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132062\n",
      "10696\n"
     ]
    }
   ],
   "source": [
    "gqa_val_q=json.load(\n",
    "    open(os.path.join(GQA_PATH,'original','val_balanced_questions.json')\n",
    "         ,'r')\n",
    ")\n",
    "print (len(gqa_val_q))\n",
    "names=list(gqa_val_q.keys())\n",
    "\n",
    "val_graphs=json.load(\n",
    "    open(os.path.join(GQA_PATH,'val_sceneGraphs.json')\n",
    "         ,'r')\n",
    ")\n",
    "print (len(val_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce7b4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage import io as skimage_io\n",
    "from skimage import transform as skimage_transform\n",
    "\n",
    "def get_tokens(text_queries):\n",
    "    tokenized_queries = np.array([\n",
    "        module.tokenize(q, config.dataset_configs.max_query_length)\n",
    "        for q in text_queries\n",
    "    ])\n",
    "    # Pad tokenized queries to avoid recompilation if number of queries changes:\n",
    "    tokenized_queries = np.pad(\n",
    "        tokenized_queries,\n",
    "        pad_width=((0, 100 - len(text_queries)), (0, 0)),\n",
    "        constant_values=0)\n",
    "    return tokenized_queries\n",
    "\n",
    "def get_gqa_feat(img_id):\n",
    "    # Load example image:\n",
    "    filename = os.path.join(GQA_PATH,'images',img_id+'.jpg')\n",
    "    image_uint8 = skimage_io.imread(filename)\n",
    "    image = image_uint8.astype(np.float32) / 255.0\n",
    "\n",
    "    # Pad to square with gray pixels on bottom and right:\n",
    "    h, w, _ = image.shape\n",
    "    size = max(h, w)\n",
    "    image_padded = np.pad(\n",
    "        image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)\n",
    "\n",
    "    # Resize to model input size:\n",
    "    input_image = skimage.transform.resize(\n",
    "        image_padded,\n",
    "        (840, 840),\n",
    "        anti_aliasing=True)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1182c752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ashkamath_mdetr_main\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "CUDA_DEVICE=15\n",
    "torch.cuda.set_device(CUDA_DEVICE)\n",
    "device = torch.device(\"cuda:\"+str(CUDA_DEVICE))\n",
    "#the default ipykernel links to the first conda environment\n",
    "\n",
    "model, postprocessor = torch.hub.load('ashkamath/mdetr:main', 'mdetr_efficientnetB5', pretrained=True, return_postprocessor=True)\n",
    "model = model.to(device)\n",
    "model.eval();\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1715d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "stemmer = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473c4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference(im, caption, plot=True):\n",
    "    # mean-std normalize the input image (batch-size: 1)\n",
    "    img = transform(im).unsqueeze(0).cuda()\n",
    "\n",
    "    # propagate through the model\n",
    "    memory_cache = model(img, [caption], encode_and_save=True)\n",
    "    outputs = model(img, [caption], encode_and_save=False, memory_cache=memory_cache)\n",
    "    #print (outputs['pred_logits'])\n",
    "\n",
    "    # keep only predictions with 0.7+ confidence\n",
    "    probas = 1 - outputs['pred_logits'].softmax(-1)[0, :, -1].cpu()\n",
    "    keep = (probas > 0.7).cpu()\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'].cpu()[0, keep], im.size)\n",
    "\n",
    "    # Extract the text spans predicted by each box\n",
    "    positive_tokens = (outputs[\"pred_logits\"].cpu()[0, keep].softmax(-1) > 0.1).nonzero().tolist()\n",
    "    predicted_spans = defaultdict(str)\n",
    "    for tok in positive_tokens:\n",
    "        item, pos = tok\n",
    "        if pos < 255:\n",
    "            span = memory_cache[\"tokenized\"].token_to_chars(0, pos)\n",
    "            predicted_spans [item] += \" \" + caption[span.start:span.end]\n",
    "\n",
    "    labels = [predicted_spans [k] for k in sorted(list(predicted_spans .keys()))]\n",
    "    if plot:\n",
    "        plot_results(im, probas[keep], bboxes_scaled, labels)\n",
    "    return probas[keep], bboxes_scaled.tolist(), labels\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c] * 255,\n",
    "                                  image[:, :, c])\n",
    "    return image\n",
    "\n",
    "def plot_results(pil_img, scores, boxes, labels, masks=None):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    np_image = np.array(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    if masks is None:\n",
    "        masks = [None for _ in range(len(scores))]\n",
    "    assert len(scores) == len(boxes) == len(labels) == len(masks)\n",
    "    for s, (xmin, ymin, xmax, ymax), l, mask, c in zip(scores, boxes.tolist(), labels, masks, colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f'{l}: {s:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "        if mask is None:\n",
    "            continue\n",
    "        np_image = apply_mask(np_image, mask, c)\n",
    "\n",
    "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            # Subtract the padding and flip (y, x) to (x, y)\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=c)\n",
    "            ax.add_patch(p)\n",
    "\n",
    "\n",
    "    plt.imshow(np_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_res(results, ax, color='green'):\n",
    "    #for tt in results.values():\n",
    "    if True:\n",
    "        bboxes = results['boxes']\n",
    "        labels = results['labels']\n",
    "        scores = results['scores']\n",
    "        #keep = scores >= 0.0\n",
    "        #bboxes = bboxes[keep].tolist()\n",
    "        #labels = labels[keep].tolist()\n",
    "        #scores = scores[keep].tolist()\n",
    "    #print(torchvision.ops.box_iou(tt['boxes'].cpu().detach(), torch.as_tensor([[xmin, ymin, xmax, ymax]])))\n",
    "    \n",
    "    colors = ['purple', 'yellow', 'red', 'green', 'orange', 'pink']\n",
    "    \n",
    "    for i, (b, ll, ss) in enumerate(zip(bboxes, labels, scores)):\n",
    "        ax.add_patch(plt.Rectangle((b[0], b[1]), b[2] - b[0], b[3] - b[1], fill=False, color=colors[i], linewidth=3))\n",
    "        cls_name = ll if isinstance(ll,str) else CLASSES[ll]\n",
    "        text = f'{cls_name}: {ss:.2f}'\n",
    "        print(text)\n",
    "        ax.text(b[0], b[1], text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9678ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_bbox_only(bbox,scores,input_image,gt_bbox=None,text=None,threshold=0.2,vis_pred=True):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    if vis_pred:\n",
    "        for i,box in enumerate(bbox):\n",
    "            score=scores[i]\n",
    "            if score<threshold:\n",
    "                continue\n",
    "            cx, cy, w, h = box\n",
    "            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
    "            ax.text(\n",
    "                cx - w / 2,\n",
    "                cy + h / 2 + 0.015,\n",
    "                f'{score:1.2f}',\n",
    "                ha='left',\n",
    "                va='top',\n",
    "                color='red',\n",
    "                bbox={\n",
    "                    'facecolor': 'white',\n",
    "                    'edgecolor': 'red',\n",
    "                    'boxstyle': 'square,pad=.3'\n",
    "                })\n",
    "    if text is not None:\n",
    "        ax.set_title(text,  fontsize=12)\n",
    "    if gt_bbox is not None:\n",
    "        for i,box in enumerate(gt_bbox):\n",
    "            cx, cy, w, h = box\n",
    "            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9822c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "def size_aware_pred(bb):\n",
    "    if bb[2]>0.3 or bb[3]>0.3:\n",
    "        return 'large'\n",
    "    else:\n",
    "        return 'small'\n",
    "def pos_aware_pred(valid_bbox,candidates):\n",
    "    if 'bottom' in candidates:\n",
    "        if valid_bbox[1]>0.5:\n",
    "            return 'bottom'\n",
    "        else:\n",
    "            return 'top'\n",
    "    elif 'right' in candidates:\n",
    "        if valid_bbox[0]<0.5:\n",
    "            return 'left'\n",
    "        else:\n",
    "            return 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4554bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coord(bbox,img_id):\n",
    "    width=val_graphs[img_id]['width']\n",
    "    height=val_graphs[img_id]['height']\n",
    "    size = max(height, width)\n",
    "    x=bbox[0]\n",
    "    y=bbox[1]\n",
    "    w=bbox[2]-bbox[0]\n",
    "    h=bbox[3]-bbox[1]\n",
    "    #print (bbox)\n",
    "    return [(x+w/2)/size,(y+h/2)/size,w/size,h/size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d4773f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mdert_result(scores,bboxs,labels,obj,img_id):\n",
    "    sf_s=[]\n",
    "    sf_bbox=[]\n",
    "    labels=[''.join(label.split(' ')) for label in labels]\n",
    "    flag=False\n",
    "    if obj in labels:\n",
    "        flag=True\n",
    "    #print (obj,labels)\n",
    "    for i,label in enumerate(labels):\n",
    "        if flag:\n",
    "            if obj==label:\n",
    "                sf_s.append(scores[i])\n",
    "                sf_bbox.append(bboxs[i])\n",
    "        else:\n",
    "            if obj in label:\n",
    "                sf_s.append(scores[i])\n",
    "                sf_bbox.append(bboxs[i])\n",
    "    #print (len(sf_s))\n",
    "    if len(sf_bbox)==0:\n",
    "        for i,label in enumerate(labels):\n",
    "            sf_s.append(scores[i])\n",
    "            sf_bbox.append(bboxs[i])\n",
    "    max_s=max(sf_s)\n",
    "    max_id=sf_s.index(max_s)\n",
    "    return_bbox=sf_bbox[max_id]\n",
    "    coord=transform_coord(return_bbox,img_id)\n",
    "    return max_s.item(),coord\n",
    "#only consider one bbox now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2348b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase_ground(img_id,cap,obj):\n",
    "    cap=cap+' '\n",
    "    im=Image.open(os.path.join(GQA_PATH,'images',img_id+'.jpg'))\n",
    "    try:\n",
    "        scores,bboxs,labels=plot_inference(im, cap, plot=False)\n",
    "    except:\n",
    "        print ('Invalid caption generation for',img_id,obj,cap)\n",
    "        scores=[torch.Tensor([0.99])]\n",
    "        bboxs=[[0.5,0.5,0.99,0.99]]\n",
    "        labels=[obj]\n",
    "    if len(bboxs)==0:\n",
    "        max_s=0.99\n",
    "        valid_bbox=[0.5,0.5,0.99,0.99]\n",
    "    else:\n",
    "        # print (scores,bboxs)\n",
    "        max_s,valid_bbox=preprocess_mdert_result(scores,bboxs,labels,obj,img_id)\n",
    "    return max_s,valid_bbox\n",
    "\n",
    "def bbox_generator(layout,idx,img_id,ques_id):\n",
    "    if layout[idx]['operation']=='select':\n",
    "        #print (idx)\n",
    "        try:\n",
    "            scenic_result=load_pkl(os.path.join(GQA_PATH,\n",
    "                                                'meta_one-all-scenic',\n",
    "                                                ques_id+'.pkl'))\n",
    "            obj=layout[idx]['argument'][0]\n",
    "            scores=[s['score'] for s in scenic_result[obj]]\n",
    "            bboxs=[s['bbox'] for s in scenic_result[obj]]\n",
    "        except:\n",
    "            bboxs=[]\n",
    "            obj=layout[idx]['argument'][0]\n",
    "        if len(bboxs)>0:\n",
    "            max_s=max(scores)\n",
    "            max_idx=scores.index(max_s)\n",
    "            bbox=bboxs[max_idx]\n",
    "            cap=obj\n",
    "        else:\n",
    "            cap=obj\n",
    "            split_obj=''.join(obj.split(' '))\n",
    "            #print (cap,split_obj)\n",
    "            max_s,bbox=generate_phrase_ground(img_id,cap,split_obj)\n",
    "            #print (max_s,bbox)\n",
    "    else:\n",
    "        cap,obj=cap_generator(layout,idx,img_id)\n",
    "        max_s,bbox=generate_phrase_ground(img_id,cap,obj)\n",
    "    return max_s,bbox,cap        \n",
    "            \n",
    "def cap_generator(layout,idx,img_id):\n",
    "    words=[]\n",
    "    dep=layout[idx]['dependencies']\n",
    "    word=layout[idx]['argument'][0]\n",
    "    words.append(word)\n",
    "    obj=''.join(word.split(' '))\n",
    "    while len(dep)>0:\n",
    "        cur_step=layout[dep[0]]\n",
    "        if cur_step['operation']=='relocate':\n",
    "            relo_symbol=cur_step['argument'][1]\n",
    "            cur_phrase=' '.join(words)\n",
    "            words=[]\n",
    "            words.append(cur_phrase)\n",
    "            #print(cur_step)\n",
    "            words.append(cur_step['argument'][0])\n",
    "            dep=cur_step['dependencies']\n",
    "            cur_step=layout[dep[0]]\n",
    "            dep=cur_step['dependencies']\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            if relo_symbol=='o':\n",
    "                words=list(reversed(words))\n",
    "        else:\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            dep=cur_step['dependencies']\n",
    "    cap=' '.join(words)\n",
    "    return cap,obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1232315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_generator_query(layout,idx,img_id):\n",
    "    words=['what']\n",
    "    dep=layout[idx]['dependencies']\n",
    "    while len(dep)>0:\n",
    "        cur_step=layout[dep[0]]\n",
    "        if cur_step['operation']=='relocate':\n",
    "            relo_symbol=cur_step['argument'][1]\n",
    "            cur_phrase=' '.join(words)\n",
    "            words=[]\n",
    "            words.append(cur_phrase)\n",
    "            #print(cur_step)\n",
    "            words.append(cur_step['argument'][0])\n",
    "            dep=cur_step['dependencies']\n",
    "            new_sent=[]\n",
    "            while len(dep)>0:\n",
    "                cur_step=layout[dep[0]]\n",
    "                dep=cur_step['dependencies']\n",
    "                arg=cur_step['argument'][0]\n",
    "                new_sent.append(arg)\n",
    "            words.append(' '.join(new_sent))\n",
    "            if relo_symbol=='o':\n",
    "                words=list(reversed(words))\n",
    "        else:\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            dep=cur_step['dependencies']\n",
    "    cap=' '.join(words)\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e66588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped(bb,input_image,showing=False):\n",
    "    width=bb[2]*840\n",
    "    height=bb[3]*840\n",
    "    xy=(bb[0]*840-width/2,bb[1]*840-height/2)\n",
    "    h0=int(max(0,xy[1]))\n",
    "    w0=int(max(0,xy[0]))\n",
    "    h1=int(max(0,840-(height+xy[1])))\n",
    "    w1=int(max(0,840-(width+xy[0])))\n",
    "    cropped=skimage.util.crop(input_image,((h0,h1),(w0,w1),(0,0)), copy=False)\n",
    "    trans_crop=Image.fromarray(np.uint8(cropped*255.0))\n",
    "    if showing:\n",
    "        display.display(trans_crop)\n",
    "    return trans_crop\n",
    "\n",
    "def clip_aware_pred(img_feat,valid_bbox,candidates,showing=False):\n",
    "    #tokens=clip.tokenize(candidates)\n",
    "    trans_crop=get_cropped(valid_bbox,img_feat)\n",
    "    if showing:\n",
    "        display.display(trans_crop)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "        text_features = clip_model.encode_text(clip.tokenize(candidates).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    #print (indices[0][0].item())\n",
    "    ans=candidates[indices[0][0].item()]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "725fecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img_with_text(img_path,texts):\n",
    "    f = Image.open(img_path)\n",
    "    img = f.convert('RGB')\n",
    "    img = np.asarray(img, dtype=np.float32)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax=fig.add_subplot(1,1,1)\n",
    "    ax.set_title(texts,  fontsize=12)\n",
    "    ax.imshow(img.astype(np.uint8))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd10d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_holder=['left','right','bottom','top','front']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52797c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3c15c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(ques):\n",
    "    nouns=[]\n",
    "    tokens_tag = pos_tag(ques.replace('?','').split(' '))\n",
    "    for w,pos in tokens_tag:\n",
    "        if pos in ['NN','NNS'] and w not in position_holder:\n",
    "            nouns.append(lemmatizer.lemmatize(w))\n",
    "    return nouns\n",
    "\n",
    "def verify_noun(nouns,obj):\n",
    "    if obj in nouns or \\\n",
    "    lemmatizer.lemmatize(obj) in nouns or\\\n",
    "    stemmer.singular_noun(obj) in nouns:\n",
    "        return True\n",
    "    elif len(obj.split(' '))>1:\n",
    "        words=obj.split(' ')\n",
    "        for w in words:\n",
    "            #print (w)\n",
    "            if lemmatizer.lemmatize(w) in nouns or\\\n",
    "            stemmer.singular_noun(w) in nouns:\n",
    "                return True\n",
    "    elif obj in ['he','she','they']:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0927a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heur_rela=['to the left of','to the right of',\n",
    "           'above','under','on top of','below',\n",
    "           'beneath','underneath']\n",
    "#if color on the last argument, use phrase grounding and binary color function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51fda09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_list=['store', 'city', 'street', 'cloudy', 'beach', 'cloudless', \n",
    "             'sunny', 'pavement', 'outdoors', 'restroom', 'ocean',\n",
    "             'sidewalk', 'skate park', 'indoors', 'field', 'garden',\n",
    "             'sea', 'park', 'kitchen', 'road', 'living room', 'garage', \n",
    "             'yard', 'foggy', 'clear', 'town', 'airport', 'zoo', 'restaurant',\n",
    "             'porch', 'highway', 'lawn', 'shore', 'desert', 'pen', 'forest', \n",
    "             'stadium', 'meadow', 'lake', 'bathroom', 'bedroom', 'swimming pool',\n",
    "             'walkway', 'display', 'mall', 'museum', 'path', 'partly cloudy', \n",
    "             'parking lot', 'supermarket', 'overcast', 'station', 'tunnel', \n",
    "             'runway', 'pasture', 'train station', 'church', 'courtyard', \n",
    "             'office', 'pub', 'backyard', 'library', 'marina', 'hallway', \n",
    "             'river', 'shop', 'plain', 'cafe', 'market', 'place', 'gas station', \n",
    "             'harbor', 'terminal', 'stage', 'hotel', 'roadway', 'cafeteria', \n",
    "             'stormy', 'rainy', 'hotel room', 'railroad', 'bakery', 'pond',\n",
    "             'kiosk', 'dining room', 'farm', 'patio', 'hangar', 'auditorium', \n",
    "             'classroom', 'salon', 'school', 'attic', 'village', 'swamp']\n",
    "\n",
    "color_list=['white', 'black', 'blue', 'brown', 'gray', 'green', 'red',\n",
    "            'yellow', 'orange', 'pink', 'silver', 'dark', 'tan', 'purple', \n",
    "            'gold', 'blond', 'beige', 'light brown', 'light blue', 'dark brown',\n",
    "            'maroon', 'cream colored', 'dark blue', 'khaki']\n",
    "\n",
    "material_list=['wood', 'metal', 'plastic', 'glass', 'concrete', 'brick',\n",
    "               'leather', 'porcelain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efda6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rela_query(layout,img_id,ques_id,showing=False,texts=None):\n",
    "    cap=cap_generator_query(layout,len(layout)-1,img_id)\n",
    "    cap=cap.replace('what','item')\n",
    "    score,bbox=generate_phrase_ground(img_id,cap,'item')\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    pred,_=ans_open_query_candi(bbox,input_image,ques_id,showing)\n",
    "    if showing:\n",
    "        texts+='\\nCaption:'+cap\n",
    "        texts+='\\nPrediction:'+pred\n",
    "        vis_bbox_only([bbox],[score],input_image,text=texts)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf8376af",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ans_cat=load_pkl('gqa_t5_pred/ans_per_cat_fix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bbcf724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\n",
      "\t ['surfer', 'spectator', 'officer', 'batter', 'catcher', 'girl', 'biker', 'man', 'boy', 'policeman', 'skater', 'skateboarder', 'player', 'passenger', 'guy', 'woman', 'mother', 'lady', 'jockey', 'toddler', 'person', 'snowboarder', 'baseball player', 'driver', 'skier', 'cyclist', 'baby', 'couple', 'umpire', 'crowd', 'gentleman', 'child']\n",
      "animal\n",
      "\t ['cat', 'bear', 'chicken', 'bird', 'goat', 'butterfly', 'donkey', 'sheep', 'bull', 'dog', 'giraffe', 'polar bear', 'elephant', 'frog', 'calf', 'puppy', 'dragon', 'kitten', 'cow', 'crab', 'duck', 'horse', 'zebra', 'seagull', 'lamb']\n",
      "furniture\n",
      "\t ['bookshelf', 'dresser', 'desk', 'computer desk', 'armchair', 'cupboard', 'bed', 'coffee table', 'nightstand', 'table', 'chair', 'couch', 'bar stool', 'shelf', 'dining table', 'tv stand', 'cabinet', 'bookcase', 'end table', 'ottoman', 'office chair', 'sofa', 'side table', 'closet', 'drawer']\n",
      "device\n",
      "\t ['computer mouse', 'keyboard', 'earphone', 'printer', 'hair dryer', 'cell phone', 'laptop', 'remote control', 'microphone', 'game controller', 'phone', 'wii controller', 'screen', 'camera', 'headphone', 'television', 'controller', 'speaker', 'monitor', 'radio', 'computer monitor', 'computer']\n",
      "vehicle\n",
      "\t ['car', 'van', 'taxi', 'locomotive', 'school bu', 'wagon', 'tractor', 'trailer', 'fire truck', 'truck', 'minivan', 'jeep', 'train car', 'carriage', 'bus', 'bu', 'train', 'suv']\n",
      "place\n",
      "\t ['sand', 'sidewalk', 'shore', 'sky', 'road', 'pavement', 'runway', 'ground', 'parking lot', 'gras', 'mountain', 'street', 'dirt', 'hillside', 'kitchen', 'beach', 'water', 'park', 'living room', 'ocean', 'forest', 'floor', 'hill', 'car', 'field', 'snow', 'balcony', 'zoo', 'bedroom', 'path', 'yard']\n",
      "vegetable\n",
      "\t ['olive', 'broccoli', 'onion', 'artichoke', 'tomato', 'avocado', 'lettuce', 'carrot', 'corn', 'spinach', 'basil', 'pepper', 'parsley', 'bean', 'celery', 'cauliflower', 'pickle', 'potato', 'cucumber', 'cabbage', 'lemon']\n",
      "food\n",
      "\t ['cookie', 'macaroni', 'sausage', 'sandwich', 'egg', 'salad', 'cereal', 'noodle', 'coleslaw', 'dessert', 'mushroom', 'shrimp', 'nut', 'candy', 'chocolate', 'soup', 'pasta', 'meat', 'tofu', 'rice', 'hotdog bun', 'cheese']\n",
      "appliance\n",
      "\t ['oven', 'heater', 'cooker', 'mixer', 'microwave oven', 'refrigerator', 'stove', 'blender', 'air conditioner', 'coffee maker', 'dishwasher', 'dryer', 'toaster', 'radiator', 'microwave']\n",
      "fruit\n",
      "\t ['banana', 'watermelon', 'apple', 'cherry', 'berry', 'strawberry', 'grape', 'blueberry', 'pineapple', 'orange']\n",
      "baked good\n",
      "\t ['donut', 'pastry', 'pie', 'bagel', 'bun', 'bread']\n",
      "fast food\n",
      "\t ['hot dog', 'pizza', 'fry']\n",
      "drink\n",
      "\t ['beer', 'coffee', 'wine', 'soda', 'tea', 'juice', 'milk']\n",
      "cooking utensil\n",
      "\t ['coffee pot', 'spatula', 'tong', 'cutting board', 'kettle', 'pan', 'tea kettle', 'skillet']\n",
      "meat\n",
      "\t ['beef', 'pepperoni', 'bacon', 'chicken', 'ham']\n",
      "watercraft\n",
      "\t ['boat', 'ship', 'canoe']\n",
      "bag\n",
      "\t ['shopping bag', 'handbag', 'trash bag', 'backpack', 'purse']\n",
      "toy\n",
      "\t ['rubber duck', 'teddy bear', 'stuffed animal', 'doll', 'stuffed bear']\n",
      "sign\n",
      "\t ['stop sign', 'street sign']\n",
      "aircraft\n",
      "\t ['jet', 'airplane']\n",
      "dessert\n",
      "\t ['cupcake', 'cake', 'ice cream']\n",
      "bird\n",
      "\t ['eagle', 'pigeon', 'parrot']\n"
     ]
    }
   ],
   "source": [
    "for name in valid_ans_cat:\n",
    "    print (name)\n",
    "    print ('\\t',valid_ans_cat[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c3f87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_pred=load_pkl('gqa_t5_pred/t5_generated_temp.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d85fc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_open_opt_candi(bbox,input_image,ques_id,obj,showing=False):\n",
    "    #prompt_head='it is a photo of '\n",
    "    if obj not in valid_ans_cat:\n",
    "        #candidates=other_ans_list\n",
    "        #candidates=bart_dict[ques_id]\n",
    "        valid_items=t5_pred[ques_id]\n",
    "        #print (valid_items)\n",
    "    else:\n",
    "        valid_items=[o for o in valid_ans_cat[obj]]\n",
    "    #print (valid_items[:50])\n",
    "    sents=[o for o in valid_items]\n",
    "    #print (sents)\n",
    "    text = clip.tokenize(sents).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text)\n",
    "        img_prepro=get_cropped(bbox,input_image)\n",
    "        #display.display(img_prepro)\n",
    "        image_features = clip_model.encode_image(preprocess(img_prepro).unsqueeze(0).to(device))\n",
    "        \n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    #print (values,indices)\n",
    "    return valid_items[indices[0][0].item()],values[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e067d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_open_query_candi(bbox,input_image,ques_id,showing=False):\n",
    "    #prompt_head='it is a photo of '\n",
    "    #candis=bart_dict[ques_id]\n",
    "    #candis=load_json(os.path.join(GQA_PATH,'features',ques_id+'.json'))\n",
    "    candis=t5_pred[ques_id]\n",
    "    #print (candis)\n",
    "    valid_items=[o for o in candis]\n",
    "    #print (valid_items[:50])\n",
    "    sents=[o for o in valid_items]\n",
    "    #print (sents)\n",
    "    text = clip.tokenize(sents).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text)\n",
    "        img_prepro=get_cropped(bbox,input_image)\n",
    "        image_features = clip_model.encode_image(preprocess(img_prepro).unsqueeze(0).to(device))\n",
    "        \n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(5)\n",
    "    #print (values,indices)\n",
    "    return valid_items[indices[0][0].item()],values[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ec9aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(layout,img_id,ques_id,showing=False,texts=None):\n",
    "    content=layout[-1]['argument'][0]\n",
    "    if content=='hposition':\n",
    "        pred=ans_query_hposition(layout,img_id,ques_id,showing,texts)\n",
    "    elif content in ['place','color','material']:\n",
    "        candi_dict={'place':scene_list,'color':color_list,'material':material_list}\n",
    "        pred,_=answer_candi(img_id,candi_dict[content],content,ques_id,showing)\n",
    "        if showing:\n",
    "            texts+='\\nPrediction:'+pred\n",
    "            show_img_with_text(os.path.join(GQA_PATH,'images',img_id+'.jpg'),texts)     \n",
    "    elif content in ['size','length','height']:\n",
    "        pred=ans_coord_rela(layout,img_id,ques_id,content,showing,texts)\n",
    "    else:\n",
    "        last_second_op=layout[-2]['operation']\n",
    "        if last_second_op=='relocate':\n",
    "            pred=generate_rela_query(layout,img_id,ques_id,showing,texts)\n",
    "        else:\n",
    "            pred=generate_basic_query(layout,img_id,ques_id,showing,texts)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5ff0592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_basic_query(layout,img_id,ques_id,showing=False,texts=None):\n",
    "    cap,_=cap_generator(layout,len(layout)-2,img_id)\n",
    "    obj=layout[-2]['argument'][0]\n",
    "    if obj=='scene':\n",
    "        bbox=[0.5,0.5,0.99,0.99]\n",
    "        score=0.9\n",
    "    else:\n",
    "        score,bbox=generate_phrase_ground(img_id,cap,obj)\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    pred,_=ans_open_opt_candi(bbox,input_image,ques_id,obj,showing)\n",
    "    if showing:\n",
    "        texts+='\\nCaption:'+cap\n",
    "        texts+='\\nPrediction:'+pred\n",
    "        vis_bbox_only([bbox],[score],input_image,text=texts)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "503ffddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_candi(img_id,candidates,content,ques_id,showing=False):\n",
    "    sents=[p for p in candidates]\n",
    "    text = clip.tokenize(sents).to(device)\n",
    "    img_path=os.path.join(GQA_PATH,'images',img_id+'.jpg')\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text)\n",
    "        if content=='place':\n",
    "            image_features = clip_model.encode_image(preprocess(Image.open(img_path)).unsqueeze(0).to(device))\n",
    "        else:\n",
    "            if len(layout)==2:\n",
    "                score,bbox,_=bbox_generator(layout,0,img_id,ques_id)\n",
    "            else:\n",
    "                cap,_=cap_generator(layout,len(layout)-2,img_id)\n",
    "                obj=layout[0]['argument'][0]\n",
    "                score,bbox=generate_phrase_ground(img_id,cap,obj)\n",
    "            input_image=get_gqa_feat(img_id)\n",
    "            if showing:\n",
    "                vis_bbox_only([bbox],[score],input_image)\n",
    "            img_prepro=get_cropped(bbox,input_image)\n",
    "            image_features = clip_model.encode_image(preprocess(img_prepro).unsqueeze(0).to(device))\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(5)\n",
    "    #print (values,indices)\n",
    "    return candidates[indices[0][0].item()],values[0][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "749e1726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_coord_rela(layout,img_id,ques_id,content,showing=False,texts=None):\n",
    "    if len(layout)==2:\n",
    "        score,bbox,_=bbox_generator(layout,0,img_id,ques_id)\n",
    "    else:\n",
    "        cap,_=cap_generator(layout,len(layout)-2,img_id)\n",
    "        obj=layout[0]['argument'][0]\n",
    "        score,bbox=generate_phrase_ground(img_id,cap,obj)\n",
    "    if content=='size':\n",
    "        if bbox[2]>0.3 or bbox[3]>0.3:\n",
    "            pred='large'\n",
    "        else:\n",
    "            pred='small'\n",
    "    elif content=='length':\n",
    "        if bbox[2]>0.3 or bbox[3]>0.3:\n",
    "            pred='long'\n",
    "        else:\n",
    "            pred='short' \n",
    "    elif content=='height':\n",
    "        if bbox[2]>0.3 or bbox[3]>0.3:\n",
    "            pred='tall'\n",
    "        else:\n",
    "            pred='short' \n",
    "    if showing:\n",
    "        input_image=get_gqa_feat(img_id)\n",
    "        texts+='\\n'+str(bbox[2])+','+str(bbox[3])\n",
    "        texts+='\\nPrediction:'+pred\n",
    "        vis_bbox_only([bbox],[score],input_image,text=texts)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d421e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_coord_rela(layout,img_id,ques_id,content,showing=False,texts=None):\n",
    "    if len(layout)==2:\n",
    "        score,bbox,_=bbox_generator(layout,0,img_id,ques_id)\n",
    "    else:\n",
    "        cap,_=cap_generator(layout,len(layout)-2,img_id)\n",
    "        obj=layout[0]['argument'][0]\n",
    "        score,bbox=generate_phrase_ground(img_id,cap,obj)\n",
    "    if content=='size':\n",
    "        if bbox[2]>0.3 or bbox[3]>0.3:\n",
    "            pred='large'\n",
    "        else:\n",
    "            pred='small'\n",
    "    elif content=='length':\n",
    "        if bbox[2]>0.3 or bbox[3]>0.3:\n",
    "            pred='long'\n",
    "        else:\n",
    "            pred='short' \n",
    "    elif content=='height':\n",
    "        if bbox[2]>0.3 or bbox[3]>0.3:\n",
    "            pred='tall'\n",
    "        else:\n",
    "            pred='short' \n",
    "    if showing:\n",
    "        input_image=get_gqa_feat(img_id)\n",
    "        texts+='\\n'+str(bbox[2])+','+str(bbox[3])\n",
    "        texts+='\\nPrediction:'+pred\n",
    "        vis_bbox_only([bbox],[score],input_image,text=texts)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "022f10d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_query_hposition(layout,img_id,ques_id,showing=False,texts=None):\n",
    "    if len(layout)==2:\n",
    "        score,bbox,_=bbox_generator(layout,0,img_id,ques_id)\n",
    "    else:\n",
    "        cap,_=cap_generator(layout,len(layout)-2,img_id)\n",
    "        obj=layout[0]['argument'][0]\n",
    "        score,bbox=generate_phrase_ground(img_id,cap,obj)\n",
    "    if bbox[0]<0.5:\n",
    "        pred='left'\n",
    "    else:\n",
    "        pred='right'\n",
    "    if showing:\n",
    "        input_image=get_gqa_feat(img_id)\n",
    "        texts+='\\nPrediction:'+pred\n",
    "        vis_bbox_only([bbox],[score],input_image,text=texts)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bae78061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid caption generation for 2324329 firehydrant fire hydrant \n",
      "already finished: 500\n",
      "\tAccuracy: 27.0\n",
      "0\n",
      "Invalid caption generation for 2391784 item man to the left of horse holding item \n",
      "already finished: 1000\n",
      "\tAccuracy: 26.0\n",
      "1\n",
      "already finished: 1500\n",
      "\tAccuracy: 26.93333333333333\n",
      "1\n",
      "already finished: 2000\n",
      "\tAccuracy: 27.1\n",
      "1\n",
      "already finished: 2500\n",
      "\tAccuracy: 26.6\n",
      "1\n",
      "Invalid caption generation for 2324040 furniture books on furniture \n",
      "Invalid caption generation for 2324040 item item on wall \n",
      "already finished: 3000\n",
      "\tAccuracy: 27.46666666666667\n",
      "3\n",
      "Invalid caption generation for 2325221 item man wearing item \n",
      "already finished: 3500\n",
      "\tAccuracy: 26.971428571428575\n",
      "4\n",
      "Invalid caption generation for 2334149 vehicle building behind vehicle \n",
      "already finished: 4000\n",
      "\tAccuracy: 26.35\n",
      "5\n",
      "Invalid caption generation for 2412014 place couch in place \n",
      "already finished: 4500\n",
      "\tAccuracy: 26.422222222222224\n",
      "6\n",
      "Invalid caption generation for 2325221 towel towel \n",
      "already finished: 5000\n",
      "\tAccuracy: 26.340000000000003\n",
      "7\n",
      "already finished: 5500\n",
      "\tAccuracy: 26.50909090909091\n",
      "7\n",
      "already finished: 6000\n",
      "\tAccuracy: 26.450000000000003\n",
      "7\n",
      "already finished: 6500\n",
      "\tAccuracy: 26.43076923076923\n",
      "7\n",
      "already finished: 7000\n",
      "\tAccuracy: 26.5\n",
      "7\n",
      "Invalid caption generation for 2403572 person person wearing shirt \n",
      "already finished: 7500\n",
      "\tAccuracy: 26.653333333333336\n",
      "8\n",
      "Invalid caption generation for 2403572 person person wearing cowboy hat \n",
      "Invalid caption generation for 2412014 item item in front of lamp \n",
      "Invalid caption generation for 2412014 couch office couch \n",
      "already finished: 8000\n",
      "\tAccuracy: 26.5875\n",
      "12\n",
      "Invalid caption generation for 2412014 person painting on person in front of wall \n",
      "already finished: 8500\n",
      "\tAccuracy: 26.741176470588236\n",
      "13\n",
      "Invalid caption generation for 2324040 item item on left bookshelf \n",
      "already finished: 9000\n",
      "\tAccuracy: 26.755555555555553\n",
      "14\n",
      "already finished: 9500\n",
      "\tAccuracy: 26.71578947368421\n",
      "14\n",
      "already finished: 10000\n",
      "\tAccuracy: 26.55\n",
      "14\n",
      "Invalid caption generation for 2412014 item men on couch holding item \n",
      "Invalid caption generation for 2334149 vehicle building behind vehicle \n",
      "already finished: 10500\n",
      "\tAccuracy: 26.561904761904763\n",
      "16\n",
      "already finished: 11000\n",
      "\tAccuracy: 26.563636363636363\n",
      "16\n",
      "Invalid caption generation for 2334149 item item in front of building \n",
      "already finished: 11500\n",
      "\tAccuracy: 26.47826086956522\n",
      "17\n",
      "Invalid caption generation for 882 device device on top of desk \n",
      "Invalid caption generation for 2334149 vehicle vehicle on road \n",
      "Invalid caption generation for 2366977 not small not small vehicle \n",
      "already finished: 12000\n",
      "\tAccuracy: 26.68333333333333\n",
      "20\n",
      "already finished: 12500\n",
      "\tAccuracy: 26.535999999999998\n",
      "20\n",
      "already finished: 13000\n",
      "\tAccuracy: 26.492307692307694\n",
      "20\n",
      "Invalid caption generation for 2391784 animal jumping riding animal man \n",
      "already finished: 13500\n",
      "\tAccuracy: 26.53333333333333\n",
      "21\n",
      "Invalid caption generation for 2403572 animal animal same color cowboy hat \n",
      "already finished: 14000\n",
      "\tAccuracy: 26.535714285714285\n",
      "22\n",
      "already finished: 14500\n",
      "\tAccuracy: 26.634482758620692\n",
      "22\n",
      "Invalid caption generation for 2325221 item person to the right of people wearing item \n",
      "Invalid caption generation for 2403572 barn barn \n",
      "already finished: 15000\n",
      "\tAccuracy: 26.593333333333334\n",
      "24\n",
      "Invalid caption generation for 2325221 not gray not gray clothing \n",
      "already finished: 15500\n",
      "\tAccuracy: 26.651612903225807\n",
      "25\n",
      "already finished: 16000\n",
      "\tAccuracy: 26.73125\n",
      "25\n",
      "already finished: 16500\n",
      "\tAccuracy: 26.721212121212123\n",
      "25\n",
      "already finished: 17000\n",
      "\tAccuracy: 26.823529411764707\n",
      "25\n",
      "Invalid caption generation for 2391784 person person riding horse \n",
      "already finished: 17500\n",
      "\tAccuracy: 26.817142857142855\n",
      "26\n",
      "already finished: 18000\n",
      "\tAccuracy: 26.833333333333332\n",
      "26\n",
      "already finished: 18500\n",
      "\tAccuracy: 26.92972972972973\n",
      "26\n",
      "Invalid caption generation for 2324329 item item full of leaves \n",
      "already finished: 19000\n",
      "\tAccuracy: 26.78421052631579\n",
      "27\n",
      "Invalid caption generation for 2328203 item man riding on item \n",
      "Invalid caption generation for 2328203 person person wearing cap \n",
      "already finished: 19500\n",
      "\tAccuracy: 26.794871794871796\n",
      "29\n",
      "already finished: 20000\n",
      "\tAccuracy: 26.779999999999998\n",
      "29\n",
      "already finished: 20500\n",
      "\tAccuracy: 26.78536585365854\n",
      "29\n",
      "Invalid caption generation for 2403572 animal animal \n",
      "already finished: 21000\n",
      "\tAccuracy: 26.80952380952381\n",
      "30\n",
      "Invalid caption generation for 2325221 gray gray clothing \n",
      "already finished: 21500\n",
      "\tAccuracy: 26.762790697674422\n",
      "31\n",
      "already finished: 22000\n",
      "\tAccuracy: 26.75909090909091\n",
      "31\n",
      "Invalid caption generation for 2391784 man fence to the right of man \n",
      "already finished: 22500\n",
      "\tAccuracy: 26.697777777777777\n",
      "31\n",
      "Invalid caption generation for 2329391 animal animal to the left of man \n",
      "already finished: 23000\n",
      "\tAccuracy: 26.669565217391305\n",
      "32\n",
      "Invalid caption generation for 882 chair left chair \n",
      "already finished: 23500\n",
      "\tAccuracy: 26.65531914893617\n",
      "33\n",
      "Invalid caption generation for 2412014 item item on wall in front of men \n",
      "Invalid caption generation for 2387080 item pillow leaning against item \n",
      "Invalid caption generation for 2412014 person table in front of person \n",
      "already finished: 24000\n",
      "\tAccuracy: 26.6125\n",
      "35\n",
      "already finished: 24500\n",
      "\tAccuracy: 26.571428571428573\n",
      "35\n",
      "already finished: 25000\n",
      "\tAccuracy: 26.588\n",
      "35\n",
      "already finished: 25500\n",
      "\tAccuracy: 26.568627450980394\n",
      "35\n",
      "Invalid caption generation for 2412014 not small not small furniture \n",
      "already finished: 26000\n",
      "\tAccuracy: 26.55769230769231\n",
      "36\n",
      "already finished: 26500\n",
      "\tAccuracy: 26.57358490566038\n",
      "36\n",
      "Invalid caption generation for 2324040 furniture furniture to the right of shelf \n",
      "already finished: 27000\n",
      "\tAccuracy: 26.633333333333333\n",
      "37\n",
      "already finished: 27500\n",
      "\tAccuracy: 26.62909090909091\n",
      "37\n",
      "Invalid caption generation for 2412014 furniture furniture in office \n",
      "already finished: 28000\n",
      "\tAccuracy: 26.646428571428572\n",
      "38\n",
      "Invalid caption generation for 882 tower tower tower \n",
      "already finished: 28500\n",
      "\tAccuracy: 26.610526315789475\n",
      "39\n",
      "already finished: 29000\n",
      "\tAccuracy: 26.665517241379312\n",
      "39\n",
      "already finished: 29500\n",
      "\tAccuracy: 26.640677966101695\n",
      "39\n",
      "already finished: 30000\n",
      "\tAccuracy: 26.640000000000004\n",
      "39\n",
      "already finished: 30500\n",
      "\tAccuracy: 26.65573770491803\n",
      "39\n",
      "already finished: 31000\n",
      "\tAccuracy: 26.683870967741935\n",
      "39\n",
      "Invalid caption generation for 2334149 vehicle vehicle to the left of mirror \n",
      "already finished: 31500\n",
      "\tAccuracy: 26.761904761904766\n",
      "40\n",
      "Invalid caption generation for 2334149 vehicle vehicle in front of building \n",
      "already finished: 32000\n",
      "\tAccuracy: 26.787499999999998\n",
      "41\n",
      "Invalid caption generation for 2328203 item man on item \n",
      "Invalid caption generation for 2412014 person person holding wood racket \n",
      "already finished: 32500\n",
      "\tAccuracy: 26.787692307692307\n",
      "43\n",
      "already finished: 33000\n",
      "\tAccuracy: 26.836363636363636\n",
      "43\n",
      "already finished: 33500\n",
      "\tAccuracy: 26.83283582089552\n",
      "43\n",
      "Invalid caption generation for 2366977 homes homes \n",
      "Invalid caption generation for 882 item item hanging from desk above tower \n",
      "already finished: 34000\n",
      "\tAccuracy: 26.81470588235294\n",
      "44\n",
      "already finished: 34500\n",
      "\tAccuracy: 26.808695652173913\n",
      "44\n",
      "Invalid caption generation for 2412014 item item in front of men behind painting \n",
      "Invalid caption generation for 2325221 gray gray clothing \n",
      "Invalid caption generation for 2403572 standing standing person \n",
      "already finished: 35000\n",
      "\tAccuracy: 26.779999999999998\n",
      "47\n",
      "already finished: 35500\n",
      "\tAccuracy: 26.76901408450704\n",
      "48\n",
      "Invalid caption generation for 2412014 furniture furniture in office \n",
      "already finished: 36000\n",
      "\tAccuracy: 26.761111111111113\n",
      "49\n",
      "already finished: 36500\n",
      "\tAccuracy: 26.821917808219176\n",
      "49\n",
      "already finished: 37000\n",
      "\tAccuracy: 26.802702702702703\n",
      "49\n",
      "Invalid caption generation for 2417045 girl girl \n",
      "Invalid caption generation for 2334149 vehicle vehicle to the left of bed \n",
      "Invalid caption generation for 2334149 vehicle vehicle on road \n",
      "already finished: 37500\n",
      "\tAccuracy: 26.794666666666668\n",
      "51\n",
      "Invalid caption generation for 2403572 barn barn \n",
      "already finished: 38000\n",
      "\tAccuracy: 26.81578947368421\n",
      "52\n",
      "already finished: 38500\n",
      "\tAccuracy: 26.870129870129873\n",
      "52\n",
      "Invalid caption generation for 2412014 not wood not wood furniture \n",
      "already finished: 39000\n",
      "\tAccuracy: 26.879487179487178\n",
      "53\n",
      "already finished: 39500\n",
      "\tAccuracy: 26.89113924050633\n",
      "53\n",
      "already finished: 40000\n",
      "\tAccuracy: 26.900000000000002\n",
      "53\n",
      "Invalid caption generation for 2412014 item item on wall \n",
      "already finished: 40500\n",
      "\tAccuracy: 26.893827160493828\n",
      "54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid caption generation for 2412014 furniture furniture in office \n",
      "already finished: 41000\n",
      "\tAccuracy: 26.856097560975613\n",
      "55\n",
      "already finished: 41500\n",
      "\tAccuracy: 26.877108433734943\n",
      "55\n",
      "Invalid caption generation for 2334149 vehicle vehicle on road \n",
      "Invalid caption generation for 2324040 person person wearing shoes \n",
      "Invalid caption generation for 2324040 person person wearing suit \n",
      "already finished: 42000\n",
      "\tAccuracy: 26.833333333333332\n",
      "58\n",
      "already finished: 42500\n",
      "\tAccuracy: 26.818823529411766\n",
      "58\n",
      "already finished: 43000\n",
      "\tAccuracy: 26.832558139534886\n",
      "58\n",
      "already finished: 43500\n",
      "\tAccuracy: 26.839080459770116\n",
      "58\n",
      "Invalid caption generation for 2329391 chimney chimney \n",
      "Invalid caption generation for 2324040 furniture furniture to the left of painting \n",
      "already finished: 44000\n",
      "\tAccuracy: 26.863636363636363\n",
      "59\n",
      "Invalid caption generation for 2324329 item backpack on item \n",
      "already finished: 44500\n",
      "\tAccuracy: 26.838202247191013\n",
      "60\n",
      "Invalid caption generation for 2412014 person person in front of painting to the left of lamp \n",
      "already finished: 45000\n",
      "\tAccuracy: 26.868888888888886\n",
      "61\n",
      "Invalid caption generation for 2412014 person person holding racket \n",
      "already finished: 45500\n",
      "\tAccuracy: 26.870329670329667\n",
      "62\n",
      "already finished: 46000\n",
      "\tAccuracy: 26.863043478260867\n",
      "62\n",
      "Invalid caption generation for 2324040 place boy in place \n",
      "already finished: 46500\n",
      "\tAccuracy: 26.890322580645158\n",
      "63\n",
      "Invalid caption generation for 2403572 woman horse woman \n",
      "Invalid caption generation for 2412014 furniture wall behind furniture in front of men \n",
      "Invalid caption generation for 882 item item on wall \n",
      "Invalid caption generation for 2412014 place table in place \n",
      "already finished: 47000\n",
      "\tAccuracy: 26.882978723404253\n",
      "67\n",
      "Invalid caption generation for 2334149 vehicle vehicle in front of building \n",
      "Invalid caption generation for 2334149 vehicle vehicle to the left of bed \n",
      "already finished: 47500\n",
      "\tAccuracy: 26.854736842105265\n",
      "69\n",
      "already finished: 48000\n",
      "\tAccuracy: 26.822916666666668\n",
      "69\n",
      "Invalid caption generation for 882 item white cord hanging from item \n",
      "Invalid caption generation for 2324329 backpack backpack \n",
      "already finished: 48500\n",
      "\tAccuracy: 26.81237113402062\n",
      "70\n",
      "Invalid caption generation for 2417045 item man to the right of people wearing item \n",
      "already finished: 49000\n",
      "\tAccuracy: 26.8\n",
      "71\n",
      "already finished: 49500\n",
      "\tAccuracy: 26.802020202020206\n",
      "71\n",
      "already finished: 50000\n",
      "\tAccuracy: 26.810000000000002\n",
      "71\n",
      "Invalid caption generation for 2366977 train small train \n",
      "already finished: 50500\n",
      "\tAccuracy: 26.813861386138615\n",
      "72\n",
      "already finished: 51000\n",
      "\tAccuracy: 26.81176470588235\n",
      "72\n",
      "already finished: 51500\n",
      "\tAccuracy: 26.796116504854368\n",
      "72\n",
      "Invalid caption generation for 2325221 item man in front of item \n",
      "Invalid caption generation for 2329391 person person wearing cap \n",
      "already finished: 52000\n",
      "\tAccuracy: 26.78846153846154\n",
      "74\n",
      "Invalid caption generation for 2391784 horse horse \n",
      "already finished: 52500\n",
      "\tAccuracy: 26.786666666666665\n",
      "75\n",
      "already finished: 53000\n",
      "\tAccuracy: 26.773584905660375\n",
      "75\n",
      "already finished: 53500\n",
      "\tAccuracy: 26.781308411214955\n",
      "75\n",
      "Invalid caption generation for 2324329 item item full of leaves \n",
      "already finished: 54000\n",
      "\tAccuracy: 26.75\n",
      "76\n",
      "already finished: 54500\n",
      "\tAccuracy: 26.743119266055043\n",
      "76\n",
      "already finished: 55000\n",
      "\tAccuracy: 26.743636363636362\n",
      "76\n",
      "already finished: 55500\n",
      "\tAccuracy: 26.754954954954957\n",
      "76\n",
      "Invalid caption generation for 2324040 item item on pole \n",
      "Invalid caption generation for 2324040 item item on left shelf \n",
      "Invalid caption generation for 2338347 place newspaper on place \n",
      "already finished: 56000\n",
      "\tAccuracy: 26.769642857142856\n",
      "79\n",
      "Invalid caption generation for 2334149 vehicle building behind vehicle \n",
      "already finished: 56500\n",
      "\tAccuracy: 26.741592920353984\n",
      "80\n",
      "already finished: 57000\n",
      "\tAccuracy: 26.764912280701754\n",
      "80\n",
      "already finished: 57500\n",
      "\tAccuracy: 26.746086956521744\n",
      "80\n",
      "already finished: 58000\n",
      "\tAccuracy: 26.739655172413794\n",
      "80\n",
      "Invalid caption generation for 2412014 small small furniture \n",
      "already finished: 58500\n",
      "\tAccuracy: 26.740170940170938\n",
      "81\n",
      "already finished: 59000\n",
      "\tAccuracy: 26.73220338983051\n",
      "81\n",
      "already finished: 59500\n",
      "\tAccuracy: 26.74285714285714\n",
      "81\n",
      "already finished: 60000\n",
      "\tAccuracy: 26.743333333333336\n",
      "81\n",
      "Invalid caption generation for 2412014 person person in front of painting \n",
      "Invalid caption generation for 2324040 bookshelf bookshelf \n",
      "already finished: 60500\n",
      "\tAccuracy: 26.73223140495868\n",
      "82\n",
      "already finished: 61000\n",
      "\tAccuracy: 26.729508196721312\n",
      "82\n",
      "Invalid caption generation for 2334149 vehicle building behind vehicle \n",
      "already finished: 61500\n",
      "\tAccuracy: 26.726829268292683\n",
      "83\n",
      "already finished: 62000\n",
      "\tAccuracy: 26.725806451612904\n",
      "83\n",
      "Invalid caption generation for 882 tower middle tower \n",
      "Invalid caption generation for 2403572 person person wearing boots \n",
      "already finished: 62500\n",
      "\tAccuracy: 26.728\n",
      "85\n",
      "Invalid caption generation for 2324040 person person wearing shoes \n",
      "already finished: 63000\n",
      "\tAccuracy: 26.75238095238095\n",
      "86\n",
      "already finished: 63500\n",
      "\tAccuracy: 26.779527559055115\n",
      "86\n",
      "Invalid caption generation for 2412014 person person in front of wall \n",
      "already finished: 64000\n",
      "\tAccuracy: 26.787499999999998\n",
      "87\n",
      "Invalid caption generation for 2324329 item backpack on item \n",
      "Invalid caption generation for 882 device device on top of desk \n",
      "already finished: 64500\n",
      "\tAccuracy: 26.8062015503876\n",
      "89\n",
      "already finished: 65000\n",
      "\tAccuracy: 26.832307692307694\n",
      "89\n",
      "already finished: 65500\n",
      "\tAccuracy: 26.821374045801527\n",
      "89\n",
      "already finished: 66000\n",
      "\tAccuracy: 26.831818181818186\n",
      "89\n",
      "Invalid caption generation for 2403572 white white animal \n",
      "already finished: 66500\n",
      "\tAccuracy: 26.834586466165412\n",
      "90\n",
      "already finished: 67000\n",
      "\tAccuracy: 26.838805970149256\n",
      "90\n",
      "already finished: 67500\n",
      "\tAccuracy: 26.84444444444444\n",
      "90\n",
      "already finished: 68000\n",
      "\tAccuracy: 26.838235294117645\n",
      "90\n",
      "Invalid caption generation for 2328203 item surfing person riding on item \n",
      "68167\n",
      "91\n",
      "26.823829712324144\n"
     ]
    }
   ],
   "source": [
    "#BART Help\n",
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "hold_dict={'hposition': 6502, 'name': 47463, 'color': 7790, \n",
    "           'material': 1387, 'size': 860, 'place': 2166, 'length': 203}\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "showing=False\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    layout=row['semantic']\n",
    "    clean_layout=update_program(layout)\n",
    "    new_prog=mine_clean_all(clean_layout)\n",
    "    if row['types']['structural']!='query':\n",
    "        continue\n",
    "    texts=[' '.join([str(vis),row['question'],row['answer'],row['imageId']])]\n",
    "    for step in new_prog:\n",
    "        texts.append('--'.join([step['operation'],step['argument'][0]]))\n",
    "    texts='\\n'.join(texts)\n",
    "    #pred=answer_query(new_prog,row['imageId'],name,\n",
    "    #                      showing=showing,texts=texts)\n",
    "    #texts.append('\\tPrediction:'+pred)\n",
    "    #texts='\\n'.join(texts)\n",
    "    #print (texts,'\\n')\n",
    "    \n",
    "    try:\n",
    "        pred=answer_query(new_prog,row['imageId'],name,\n",
    "                          showing=showing,texts=texts)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "    #print (texts,'\\nPrediction:',pred,'\\n')\n",
    "    if pred==row['answer']:\n",
    "        acc+=1\n",
    "    vis+=1\n",
    "    \n",
    "    if vis%500==0:\n",
    "        print ('already finished:',vis)\n",
    "        print ('\\tAccuracy:',acc/vis*100.0)\n",
    "        print (len(invalid))\n",
    "print (vis)\n",
    "print (len(invalid))\n",
    "print (acc/vis*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558acc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
