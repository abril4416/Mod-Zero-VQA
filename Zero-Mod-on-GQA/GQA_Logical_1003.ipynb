{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176c5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "from matplotlib import patches,  lines\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05548b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "VQA_PATH='/Data_Storage/Rui_Data_Space/multimodal/VQA'\n",
    "GQA_PATH='/Data_Storage/Rui_Data_Space/multimodal/GQA'\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def load_pkl(path):\n",
    "    data=pkl.load(open(path,'rb'))\n",
    "    return data\n",
    "\n",
    "def load_json(path):\n",
    "    data=json.load(open(path,'r'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b70b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56e7af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "stemmer = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e54a57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132062\n",
      "10696\n"
     ]
    }
   ],
   "source": [
    "gqa_val_q=json.load(\n",
    "    open(os.path.join(GQA_PATH,'original','val_balanced_questions.json')\n",
    "         ,'r')\n",
    ")\n",
    "print (len(gqa_val_q))\n",
    "names=list(gqa_val_q.keys())\n",
    "\n",
    "val_graphs=json.load(\n",
    "    open(os.path.join(GQA_PATH,'val_sceneGraphs.json')\n",
    "         ,'r')\n",
    ")\n",
    "print (len(val_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79b449e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage import io as skimage_io\n",
    "from skimage import transform as skimage_transform\n",
    "\n",
    "def get_tokens(text_queries):\n",
    "    tokenized_queries = np.array([\n",
    "        module.tokenize(q, config.dataset_configs.max_query_length)\n",
    "        for q in text_queries\n",
    "    ])\n",
    "    # Pad tokenized queries to avoid recompilation if number of queries changes:\n",
    "    tokenized_queries = np.pad(\n",
    "        tokenized_queries,\n",
    "        pad_width=((0, 100 - len(text_queries)), (0, 0)),\n",
    "        constant_values=0)\n",
    "    return tokenized_queries\n",
    "\n",
    "def get_gqa_feat(img_id):\n",
    "    # Load example image:\n",
    "    filename = os.path.join(GQA_PATH,'images',img_id+'.jpg')\n",
    "    image_uint8 = skimage_io.imread(filename)\n",
    "    image = image_uint8.astype(np.float32) / 255.0\n",
    "\n",
    "    # Pad to square with gray pixels on bottom and right:\n",
    "    h, w, _ = image.shape\n",
    "    size = max(h, w)\n",
    "    image_padded = np.pad(\n",
    "        image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)\n",
    "\n",
    "    # Resize to model input size:\n",
    "    input_image = skimage.transform.resize(\n",
    "        image_padded,\n",
    "        (840, 840),\n",
    "        anti_aliasing=True)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f53ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ashkamath_mdetr_main\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "CUDA_DEVICE=4\n",
    "torch.cuda.set_device(CUDA_DEVICE)\n",
    "device = torch.device(\"cuda:\"+str(CUDA_DEVICE))\n",
    "#the default ipykernel links to the first conda environment\n",
    "\n",
    "model, postprocessor = torch.hub.load('ashkamath/mdetr:main', 'mdetr_efficientnetB5', pretrained=True, return_postprocessor=True)\n",
    "model = model.to(device)\n",
    "model.eval();\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf19652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference(im, caption, plot=True):\n",
    "    # mean-std normalize the input image (batch-size: 1)\n",
    "    img = transform(im).unsqueeze(0).cuda()\n",
    "\n",
    "    # propagate through the model\n",
    "    memory_cache = model(img, [caption], encode_and_save=True)\n",
    "    outputs = model(img, [caption], encode_and_save=False, memory_cache=memory_cache)\n",
    "    #print (outputs['pred_logits'])\n",
    "\n",
    "    # keep only predictions with 0.7+ confidence\n",
    "    probas = 1 - outputs['pred_logits'].softmax(-1)[0, :, -1].cpu()\n",
    "    keep = (probas > 0.7).cpu()\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'].cpu()[0, keep], im.size)\n",
    "\n",
    "    # Extract the text spans predicted by each box\n",
    "    positive_tokens = (outputs[\"pred_logits\"].cpu()[0, keep].softmax(-1) > 0.1).nonzero().tolist()\n",
    "    predicted_spans = defaultdict(str)\n",
    "    for tok in positive_tokens:\n",
    "        item, pos = tok\n",
    "        if pos < 255:\n",
    "            span = memory_cache[\"tokenized\"].token_to_chars(0, pos)\n",
    "            predicted_spans [item] += \" \" + caption[span.start:span.end]\n",
    "\n",
    "    labels = [predicted_spans [k] for k in sorted(list(predicted_spans .keys()))]\n",
    "    if plot:\n",
    "        plot_results(im, probas[keep], bboxes_scaled, labels)\n",
    "    return probas[keep], bboxes_scaled.tolist(), labels\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c] * 255,\n",
    "                                  image[:, :, c])\n",
    "    return image\n",
    "\n",
    "def plot_results(pil_img, scores, boxes, labels, masks=None):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    np_image = np.array(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    if masks is None:\n",
    "        masks = [None for _ in range(len(scores))]\n",
    "    assert len(scores) == len(boxes) == len(labels) == len(masks)\n",
    "    for s, (xmin, ymin, xmax, ymax), l, mask, c in zip(scores, boxes.tolist(), labels, masks, colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f'{l}: {s:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "        if mask is None:\n",
    "            continue\n",
    "        np_image = apply_mask(np_image, mask, c)\n",
    "\n",
    "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            # Subtract the padding and flip (y, x) to (x, y)\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=c)\n",
    "            ax.add_patch(p)\n",
    "\n",
    "\n",
    "    plt.imshow(np_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_res(results, ax, color='green'):\n",
    "    #for tt in results.values():\n",
    "    if True:\n",
    "        bboxes = results['boxes']\n",
    "        labels = results['labels']\n",
    "        scores = results['scores']\n",
    "        #keep = scores >= 0.0\n",
    "        #bboxes = bboxes[keep].tolist()\n",
    "        #labels = labels[keep].tolist()\n",
    "        #scores = scores[keep].tolist()\n",
    "    #print(torchvision.ops.box_iou(tt['boxes'].cpu().detach(), torch.as_tensor([[xmin, ymin, xmax, ymax]])))\n",
    "    \n",
    "    colors = ['purple', 'yellow', 'red', 'green', 'orange', 'pink']\n",
    "    \n",
    "    for i, (b, ll, ss) in enumerate(zip(bboxes, labels, scores)):\n",
    "        ax.add_patch(plt.Rectangle((b[0], b[1]), b[2] - b[0], b[3] - b[1], fill=False, color=colors[i], linewidth=3))\n",
    "        cls_name = ll if isinstance(ll,str) else CLASSES[ll]\n",
    "        text = f'{cls_name}: {ss:.2f}'\n",
    "        print(text)\n",
    "        ax.text(b[0], b[1], text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89421fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_bbox_only(bbox,scores,input_image,gt_bbox=None,text=None,threshold=0.2,vis_pred=True):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    if vis_pred:\n",
    "        for i,box in enumerate(bbox):\n",
    "            score=scores[i]\n",
    "            if score<threshold:\n",
    "                continue\n",
    "            cx, cy, w, h = box\n",
    "            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
    "            ax.text(\n",
    "                cx - w / 2,\n",
    "                cy + h / 2 + 0.015,\n",
    "                f'{score:1.2f}',\n",
    "                ha='left',\n",
    "                va='top',\n",
    "                color='red',\n",
    "                bbox={\n",
    "                    'facecolor': 'white',\n",
    "                    'edgecolor': 'red',\n",
    "                    'boxstyle': 'square,pad=.3'\n",
    "                })\n",
    "    if text is not None:\n",
    "        ax.set_title(text,  fontsize=12)\n",
    "    if gt_bbox is not None:\n",
    "        for i,box in enumerate(gt_bbox):\n",
    "            cx, cy, w, h = box\n",
    "            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77ac8bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "def size_aware_pred(bb):\n",
    "    if bb[2]>0.3 or bb[3]>0.3:\n",
    "        return 'large'\n",
    "    else:\n",
    "        return 'small'\n",
    "def pos_aware_pred(valid_bbox,candidates):\n",
    "    if 'bottom' in candidates:\n",
    "        if valid_bbox[1]>0.5:\n",
    "            return 'bottom'\n",
    "        else:\n",
    "            return 'top'\n",
    "    elif 'right' in candidates:\n",
    "        if valid_bbox[0]<0.5:\n",
    "            return 'left'\n",
    "        else:\n",
    "            return 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55c8b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coord(bbox,img_id):\n",
    "    width=val_graphs[img_id]['width']\n",
    "    height=val_graphs[img_id]['height']\n",
    "    size = max(height, width)\n",
    "    x=bbox[0]\n",
    "    y=bbox[1]\n",
    "    w=bbox[2]-bbox[0]\n",
    "    h=bbox[3]-bbox[1]\n",
    "    #print (bbox)\n",
    "    return [(x+w/2)/size,(y+h/2)/size,w/size,h/size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b801886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjectives(text):\n",
    "    sent=text.split(',')[0]\n",
    "    words=sent.split(' ')\n",
    "    if 'less' in words:\n",
    "        words=words[-2:]\n",
    "    else:\n",
    "        words=words[-1:]\n",
    "    words=' '.join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97aae31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mdert_result(scores,bboxs,labels,obj,img_id):\n",
    "    sf_s=[]\n",
    "    sf_bbox=[]\n",
    "    labels=[''.join(label.split(' ')) for label in labels]\n",
    "    flag=False\n",
    "    if obj in labels:\n",
    "        flag=True\n",
    "    #print (obj,labels)\n",
    "    for i,label in enumerate(labels):\n",
    "        if flag:\n",
    "            if obj==label:\n",
    "                sf_s.append(scores[i])\n",
    "                sf_bbox.append(bboxs[i])\n",
    "        else:\n",
    "            if obj in label:\n",
    "                sf_s.append(scores[i])\n",
    "                sf_bbox.append(bboxs[i])\n",
    "    #print (len(sf_s))\n",
    "    if len(sf_bbox)==0:\n",
    "        for i,label in enumerate(labels):\n",
    "            sf_s.append(scores[i])\n",
    "            sf_bbox.append(bboxs[i])\n",
    "    max_s=max(sf_s)\n",
    "    max_id=sf_s.index(max_s)\n",
    "    return_bbox=sf_bbox[max_id]\n",
    "    coord=transform_coord(return_bbox,img_id)\n",
    "    return max_s.item(),coord\n",
    "#only consider one bbox now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe5fb4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase_ground(img_id,cap,obj):\n",
    "    cap=cap+' '\n",
    "    im=Image.open(os.path.join(GQA_PATH,'images',img_id+'.jpg'))\n",
    "    try:\n",
    "        scores,bboxs,labels=plot_inference(im, cap, plot=False)\n",
    "    except:\n",
    "        print ('Invalid caption generation for',img_id,obj,cap)\n",
    "        scores=[torch.Tensor([0.99])]\n",
    "        bboxs=[[0.5,0.5,0.99,0.99]]\n",
    "        labels=[obj]\n",
    "    if len(bboxs)==0:\n",
    "        max_s=0.99\n",
    "        valid_bbox=[0.5,0.5,0.99,0.99]\n",
    "    else:\n",
    "        # print (scores,bboxs)\n",
    "        max_s,valid_bbox=preprocess_mdert_result(scores,bboxs,labels,obj,img_id)\n",
    "    return max_s,valid_bbox\n",
    "\n",
    "def bbox_generator(layout,idx,img_id,ques_id):\n",
    "    if layout[idx]['operation']=='select':\n",
    "        #print (idx)\n",
    "        try:\n",
    "            scenic_result=load_pkl(os.path.join(GQA_PATH,\n",
    "                                                'meta_one-all-scenic',\n",
    "                                                ques_id+'.pkl'))\n",
    "            obj=layout[idx]['argument'][0]\n",
    "            scores=[s['score'] for s in scenic_result[obj]]\n",
    "            bboxs=[s['bbox'] for s in scenic_result[obj]]\n",
    "        except:\n",
    "            bboxs=[]\n",
    "            obj=layout[idx]['argument'][0]\n",
    "        if len(bboxs)>0:\n",
    "            max_s=max(scores)\n",
    "            max_idx=scores.index(max_s)\n",
    "            bbox=bboxs[max_idx]\n",
    "            cap=obj\n",
    "        else:\n",
    "            cap=obj\n",
    "            split_obj=''.join(obj.split(' '))\n",
    "            #print (cap,split_obj)\n",
    "            max_s,bbox=generate_phrase_ground(img_id,cap,split_obj)\n",
    "            #print (max_s,bbox)\n",
    "    else:\n",
    "        cap,obj=cap_generator(layout,idx,img_id)\n",
    "        max_s,bbox=generate_phrase_ground(img_id,cap,obj)\n",
    "    return max_s,bbox,cap        \n",
    "            \n",
    "def cap_generator(layout,idx,img_id):\n",
    "    words=[]\n",
    "    dep=layout[idx]['dependencies']\n",
    "    word=layout[idx]['argument'][0]\n",
    "    words.append(word)\n",
    "    obj=''.join(word.split(' '))\n",
    "    while len(dep)>0:\n",
    "        cur_step=layout[dep[0]]\n",
    "        if cur_step['operation']=='relocate':\n",
    "            relo_symbol=cur_step['argument'][1]\n",
    "            cur_phrase=' '.join(words)\n",
    "            words=[]\n",
    "            words.append(cur_phrase)\n",
    "            #print(cur_step)\n",
    "            words.append(cur_step['argument'][0])\n",
    "            dep=cur_step['dependencies']\n",
    "            cur_step=layout[dep[0]]\n",
    "            dep=cur_step['dependencies']\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            if relo_symbol=='o':\n",
    "                words=list(reversed(words))\n",
    "        else:\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            dep=cur_step['dependencies']\n",
    "    cap=' '.join(words)\n",
    "    return cap,obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7731d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_bbox_generator_exist(layout,idx,img_id):\n",
    "    cap=cap_generator_exist(layout,idx,img_id)\n",
    "    max_s,bbox=generate_phrase_ground(img_id,cap,cap)\n",
    "    return bbox,cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc20992f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped(bb,input_image,showing=False):\n",
    "    width=bb[2]*840\n",
    "    height=bb[3]*840\n",
    "    xy=(bb[0]*840-width/2,bb[1]*840-height/2)\n",
    "    h0=int(max(0,xy[1]))\n",
    "    w0=int(max(0,xy[0]))\n",
    "    h1=int(max(0,840-(height+xy[1])))\n",
    "    w1=int(max(0,840-(width+xy[0])))\n",
    "    cropped=skimage.util.crop(input_image,((h0,h1),(w0,w1),(0,0)), copy=False)\n",
    "    trans_crop=Image.fromarray(np.uint8(cropped*255.0))\n",
    "    if showing:\n",
    "        display.display(trans_crop)\n",
    "    return trans_crop\n",
    "\n",
    "def clip_aware_pred(img_feat,valid_bbox,candidates,showing=False):\n",
    "    #tokens=clip.tokenize(candidates)\n",
    "    trans_crop=get_cropped(valid_bbox,img_feat)\n",
    "    if showing:\n",
    "        display.display(trans_crop)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "        text_features = clip_model.encode_text(clip.tokenize(candidates).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    #print (indices[0][0].item())\n",
    "    ans=candidates[indices[0][0].item()]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "534f7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_holder=['left','right','bottom','top','front']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0da2555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d29b75bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(ques):\n",
    "    nouns=[]\n",
    "    tokens_tag = pos_tag(ques.replace('?','').split(' '))\n",
    "    for w,pos in tokens_tag:\n",
    "        if pos in ['NN','NNS'] and w not in position_holder:\n",
    "            nouns.append(lemmatizer.lemmatize(w))\n",
    "    return nouns\n",
    "\n",
    "def verify_noun(nouns,obj):\n",
    "    if obj in nouns or \\\n",
    "    lemmatizer.lemmatize(obj) in nouns or\\\n",
    "    stemmer.singular_noun(obj) in nouns:\n",
    "        return True\n",
    "    elif len(obj.split(' '))>1:\n",
    "        words=obj.split(' ')\n",
    "        for w in words:\n",
    "            #print (w)\n",
    "            if lemmatizer.lemmatize(w) in nouns or\\\n",
    "            stemmer.singular_noun(w) in nouns:\n",
    "                return True\n",
    "    elif obj in ['he','she','they']:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0c26abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relo(layout):\n",
    "    ops=[step['operation'] for step in layout]\n",
    "    length=len(layout)-2\n",
    "    relo_id=length\n",
    "    counter=0\n",
    "    rela_symbol=[]\n",
    "    for i in range(len(layout)-2):\n",
    "        cur_op=layout[length-i]['operation']\n",
    "        if cur_op=='relocate':\n",
    "            relo_id=length-i\n",
    "            rela_symbol=layout[length-i]['argument']\n",
    "            break\n",
    "        elif cur_op=='filter':\n",
    "            counter+=1#find some attributes finally, if true\n",
    "    return rela_symbol,relo_id, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fc4a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_generator_exist(layout,idx,img_id):\n",
    "    words=[]\n",
    "    dep=layout[idx]['dependencies']\n",
    "    while len(dep)>0:\n",
    "        cur_step=layout[dep[0]]\n",
    "        if cur_step['operation']=='relocate':\n",
    "            relo_symbol=cur_step['argument'][1]\n",
    "            cur_phrase=' '.join(words)\n",
    "            words=[]\n",
    "            words.append(cur_phrase)\n",
    "            #print(cur_step)\n",
    "            words.append(cur_step['argument'][0])\n",
    "            dep=cur_step['dependencies']\n",
    "            new_sent=[]\n",
    "            while len(dep)>0:\n",
    "                cur_step=layout[dep[0]]\n",
    "                dep=cur_step['dependencies']\n",
    "                arg=cur_step['argument'][0]\n",
    "                new_sent.append(arg)\n",
    "            words.append(' '.join(new_sent))\n",
    "            if relo_symbol=='o':\n",
    "                words=list(reversed(words))\n",
    "        else:\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            dep=cur_step['dependencies']\n",
    "    cap=' '.join(words)\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61d6469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_involved_ans(layout,img_id,ques_id,thresh=0.2):\n",
    "    nouns=get_nouns(gqa_val_q[ques_id]['question'][:-1])#remove the punctuation\n",
    "    ops=[step['operation'] for step in layout]\n",
    "    #must have select operation\n",
    "    for m,step in enumerate(layout[:-1]):\n",
    "        op=step['operation']\n",
    "        if op not in ['filter','select']:\n",
    "            continue\n",
    "        arg=step['argument'][0]\n",
    "        if arg in hold_list:\n",
    "            continue\n",
    "        if op=='filter':\n",
    "            flag=verify_noun(nouns,arg)\n",
    "            if flag==False:#not a noun\n",
    "                continue\n",
    "        if op=='select':\n",
    "            #infos=select_info[arg]\n",
    "            s,_,_=bbox_generator(layout,m,img_id,ques_id)\n",
    "            if s<thresh:\n",
    "                return 'no'\n",
    "        else:\n",
    "            try:\n",
    "                filter_info=load_pkl(os.path.join(GQA_PATH,\n",
    "                                              'meta_filter-related',\n",
    "                                              ques_id+'.pkl'))\n",
    "                infos=filter_info[arg]\n",
    "                s=[info['score'] for info in infos if info['score']>=thresh]\n",
    "            except:\n",
    "                print('No filter result',arg,ques_id)\n",
    "                continue\n",
    "            \n",
    "            if len(s)==0:\n",
    "                return 'no'\n",
    "    rela_symbol,rela_id,counter=get_relo(new_prog)    \n",
    "    if counter==1:\n",
    "        pred=verify_relation(layout,img_id,ques_id,rela_symbol,rela_id)\n",
    "    else:\n",
    "        pred=verify_attr_last_step(layout,img_id,ques_id)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51c2d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "heur_rela=['to the left of','to the right of',\n",
    "           'above','under','on top of','below',\n",
    "           'beneath','underneath']\n",
    "#if color on the last argument, use phrase grounding and binary color function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d42b2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heur_rules(bb_a,bb_b,rela):\n",
    "    #whether there is bb_a rela bb_b\n",
    "    if rela=='to the left of':\n",
    "        if bb_a[0]<bb_b[0]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif rela=='to the right of':\n",
    "        if bb_a[0]>bb_b[0]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif rela in ['above','on top of']:\n",
    "        if bb_a[1]>bb_b[1]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif rela in [ 'under','below','beneath','underneath']:\n",
    "        if bb_a[1]<bb_b[1]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e276faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_relation(layout,img_id,ques_id,rela_symbol,rela_id,showing=False):\n",
    "    relation=rela_symbol[0]\n",
    "    sub_obj=rela_symbol[1]\n",
    "    obj=layout[-2]['argument'][0]\n",
    "    cap=cap_generator_exist(layout,len(layout)-1,img_id)\n",
    "    cap=cap.replace(obj,'').replace(relation,'')\n",
    "    if relation in heur_rela:\n",
    "        try:\n",
    "            scenic_info=load_pkl(os.path.join(GQA_PATH,\n",
    "                                              'meta_filter-related',\n",
    "                                              ques_id+'.pkl'))\n",
    "        #print (scenic_info.keys())\n",
    "            info=scenic_info[obj]\n",
    "            scores=[i['score'] for i in info]\n",
    "            bboxs=[i['bbox'] for i in info]\n",
    "            if len(scores)==0:\n",
    "                return 'no'\n",
    "            bb_1=bboxs[scores.index(max(scores))]\n",
    "        except:\n",
    "            s,b,_=bbox_generator(layout,len(layout)-2,img_id,ques_id)\n",
    "            bb_1=b\n",
    "        \n",
    "        if rela_id==1:\n",
    "            _,bb_2,_=bbox_generator(layout,0,img_id,ques_id)\n",
    "        else:\n",
    "            obj=layout[rela_id-1]['argument'][0]\n",
    "            _,bb_2=generate_phrase_ground(img_id,cap,obj)\n",
    "        if sub_obj=='o':\n",
    "            bb_a=bb_2\n",
    "            bb_b=bb_1\n",
    "        else:\n",
    "            bb_a=bb_1\n",
    "            bb_b=bb_2\n",
    "        pred=heur_rules(bb_a,bb_b,relation)\n",
    "        if showing:\n",
    "            img_feat=get_gqa_feat(img_id)\n",
    "            #print (bb_a,bb_b)\n",
    "            vis_bbox_only([bb_a],[0.5],img_feat,[bb_b],img_id)\n",
    "    else:\n",
    "        if sub_obj=='s':\n",
    "            pos=' '.join([obj,'is',relation,cap])\n",
    "            neg=' '.join([obj,'is not',relation,cap])\n",
    "        else:\n",
    "            pos=' '.join([cap,'is',relation,obj])\n",
    "            neg=' '.join([cap,'is not',relation,obj])\n",
    "        #print (pos,'\\n',neg,'\\n',gqa_val_q[ques_id]['question'])\n",
    "        pred=binary_matching(img_id,[pos,neg])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b52834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_matching(img_id,sents):\n",
    "    tokens=clip.tokenize(sents)\n",
    "    im=Image.open(os.path.join(GQA_PATH,'images',img_id+'.jpg'))\n",
    "    with torch.no_grad():\n",
    "        text_features=clip_model.encode_text(tokens.to(device))\n",
    "        image_features = clip_model.encode_image(preprocess(im).unsqueeze(0).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    indices=torch.max(similarity,1)[1]\n",
    "    #print (similarity,indices)\n",
    "    similarity=similarity.squeeze()\n",
    "    scores=[str(similarity[0].item()),str(similarity[1].item())]\n",
    "    if indices.squeeze().item()==0:\n",
    "        pred='yes'\n",
    "    else:\n",
    "        pred='no'\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9c56c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_attr_last_step(layout,img_id,ques_id,showing=False):\n",
    "    cap=cap_generator_exist(layout,len(layout)-1,img_id)\n",
    "    attr=layout[-2]['argument'][0]\n",
    "    phrase=cap.replace(attr,'')\n",
    "    obj=layout[-3]['argument'][0]\n",
    "    _,bbox=generate_phrase_ground(img_id,phrase,obj)\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    pred=filter_binary_color(bbox,input_image,attr,showing)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3f71eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_binary_color(bb,input_image,color,showing=False):\n",
    "    width=bb[2]*840\n",
    "    height=bb[3]*840\n",
    "    xy=(bb[0]*840-width/2,bb[1]*840-height/2)\n",
    "    h0=int(max(0,xy[1]))\n",
    "    w0=int(max(0,xy[0]))\n",
    "    h1=int(max(0,840-(height+xy[1])))\n",
    "    w1=int(max(0,840-(width+xy[0])))\n",
    "    cropped=skimage.util.crop(input_image,((h0,h1),(w0,w1),(0,0)), copy=False)\n",
    "    trans_crop=Image.fromarray(np.uint8(cropped*255.0))\n",
    "    flag=True #color is a positive statement\n",
    "    if 'not' in color:\n",
    "        flag=False#reverse the pred\n",
    "        words=color.split(' ')[1:]\n",
    "        color=' '.join(words)\n",
    "    pos_sent=color\n",
    "    neg_sent='Not '+color\n",
    "    tokens=clip.tokenize([neg_sent,pos_sent])\n",
    "    with torch.no_grad():\n",
    "        text_features=clip_model.encode_text(tokens.to(device))\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    indices=torch.max(similarity,1)[1]\n",
    "    #print (similarity,indices)\n",
    "    similarity=similarity.squeeze()\n",
    "    scores=[str(similarity[0].item()),str(similarity[1].item())]\n",
    "    if showing:\n",
    "        vis_no_score([bb],input_image,'not '+color+'-'+'-'.join(scores))\n",
    "    if indices.squeeze().item()==1:\n",
    "        pred='yes'\n",
    "    else:\n",
    "        pred='no'\n",
    "    if flag:\n",
    "        return pred\n",
    "    else:\n",
    "        if pred=='yes':\n",
    "            return 'no'\n",
    "        else:\n",
    "            return 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2903ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_basic_ans(layout,img_id,ques_id,thresh=0.2):\n",
    "    arg=layout[0]['argument'][0]\n",
    "    scenic_result=load_pkl(os.path.join(GQA_PATH,'meta_one-all-scenic',ques_id+'.pkl'))\n",
    "    info=scenic_result[arg]\n",
    "    scores=[]\n",
    "    scores=[i['score'] for i in info if i['score']>=thresh]\n",
    "    if len(scores)==0:\n",
    "        return 'no'\n",
    "    if max(scores)>=thresh:\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47e79b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_position(bb,pos,input_image,showing=False):\n",
    "    if showing:\n",
    "        vis_no_score([bb],input_image,pos)\n",
    "    if pos=='left':\n",
    "        if bb[0]<0.4:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif pos=='right':\n",
    "        if bb[0]>0.6:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif pos=='top':\n",
    "        if bb[1]<0.4:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif pos=='bottom':\n",
    "        if bb[1]>0.6:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "80de85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_no_score(bbox,input_image,text=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    for i,box in enumerate(bbox):\n",
    "        cx, cy, w, h = box\n",
    "        ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
    "    if text is not None:\n",
    "        ax.set_title(text,  fontsize=12)\n",
    "            \n",
    "def verify_attr_ans(layout,img_id,ques_id,showing=False):\n",
    "    attr=layout[-2]['argument'][0]#the last filter module\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    if len(layout)==3:\n",
    "        _,bbox,_=bbox_generator(layout,0,img_id,ques_id)\n",
    "    else:\n",
    "        bbox,cap=phrase_bbox_generator_exist(layout,len(layout)-2,img_id)\n",
    "    if attr in ['left','right','top','bottom']:\n",
    "        pred=filter_position(bbox,attr,input_image,showing)\n",
    "    else:\n",
    "        pred=filter_binary_color(bbox,input_image,attr,showing)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5d17f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scene_ans(layout,img_id,ques_id,showing=False):\n",
    "    attr=layout[-2]['argument'][0]#the last filter module\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    bbox=[0.5,0.5,0.99,0.99]#the whole image\n",
    "    pred=filter_binary_color(bbox,input_image,attr,showing)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "747298b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def existence_for_all(layout,img_id,ques_id):\n",
    "    ops=[step['operation'] for step in layout]\n",
    "    if 'filter' not in ops:\n",
    "        pred=generate_basic_ans(layout,img_id,ques_id)\n",
    "    elif 'relocate' not in ops:\n",
    "        if layout[0]['argument'][0]=='scene':\n",
    "            pred=generate_scene_ans(layout,img_id,ques_id)\n",
    "        else:\n",
    "            pred=verify_attr_ans(layout,img_id,ques_id)\n",
    "    else:\n",
    "        pred=relocate_involved_ans(layout,img_id,ques_id)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeabffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "77a9e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_prog=load_pkl('../meta_generated_layout/dep_all_meta_layout.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2395c738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68167\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "colors=defaultdict(int)\n",
    "materials=defaultdict(int)\n",
    "shapes=defaultdict(int)\n",
    "for k,name in enumerate(names):\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "    if new_prog[-1]['operation']!='query':\n",
    "        continue\n",
    "    if len(new_prog[-1]['argument'])==0:\n",
    "        continue\n",
    "    query_t=new_prog[-1]['argument'][0]\n",
    "    if query_t=='color':\n",
    "        colors[row['answer']]+=1\n",
    "    elif query_t=='material':\n",
    "        materials[row['answer']]+=1\n",
    "    elif query_t=='shape':\n",
    "        shapes[row['answer']]+=1\n",
    "    vis+=1\n",
    "print (vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a45f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_list=[]\n",
    "hold_list.extend([c for c in colors])\n",
    "hold_list.extend([c for c in materials])\n",
    "hold_list.extend([c for c in shapes])\n",
    "hold_list=set(hold_list)\n",
    "#print (hold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4dade6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_prog(layout):\n",
    "    first_prog=[]\n",
    "    second_prog=[]\n",
    "    flag=True#whether meet the first exist module\n",
    "    flag_idx=0\n",
    "    inserts=0\n",
    "    for i ,step in enumerate(layout[:-1]):\n",
    "        if flag:\n",
    "            first_prog.append(step)\n",
    "            if step['operation']=='exist':\n",
    "                flag=False\n",
    "                flag_idx=i+1\n",
    "            continue\n",
    "        dep=step['dependencies']\n",
    "        second_prog.append(step)\n",
    "        while len(dep)>0 and dep[0]<flag_idx-1:\n",
    "            idx=dep[0]\n",
    "            insertion=layout[idx]\n",
    "            second_prog.insert(0,insertion)\n",
    "            inserts+=1\n",
    "            dep=insertion['dependencies']\n",
    "    for step in second_prog:\n",
    "        dep=step['dependencies']\n",
    "        if len(dep)>0:\n",
    "            idx=dep[0]\n",
    "            if idx>=flag_idx:\n",
    "                dep[0]-=flag_idx\n",
    "                dep[0]+=inserts\n",
    "    return first_prog,second_prog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "114948bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prog(layout):\n",
    "    for i,step in enumerate(layout):\n",
    "        dep=[str(d) for d in step['dependencies']]\n",
    "        dep=','.join(dep)\n",
    "        dep='  '+dep\n",
    "        print('\\t','--'.join([str(i),step['operation'],step['argument'][0],dep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c80c3f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ans_logical(layout,img_id,ques_id):\n",
    "    first_prog,second_prog=separate_prog(layout)\n",
    "    first_ans=existence_for_all(first_prog,img_id,ques_id)\n",
    "    second_ans=existence_for_all(second_prog,img_id,ques_id)\n",
    "    logic=layout[-1]['operation']\n",
    "    if logic=='and':\n",
    "        if first_ans=='yes' and second_ans=='yes':\n",
    "            return 'yes',first_ans,second_ans\n",
    "        else:\n",
    "            return 'no',first_ans,second_ans\n",
    "    elif logic=='or':\n",
    "        if first_ans=='no' and second_ans=='no':\n",
    "            return 'no',first_ans,second_ans\n",
    "        else:\n",
    "            return 'yes',first_ans,second_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3314cf88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/torch/hub/ashkamath_mdetr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already finished 500\n",
      "\tAccuracy 68.8\n",
      "\t 4\n",
      "Already finished 1000\n",
      "\tAccuracy 69.2\n",
      "\t 5\n",
      "Already finished 1500\n",
      "\tAccuracy 68.93333333333334\n",
      "\t 7\n",
      "Already finished 2000\n",
      "\tAccuracy 68.55\n",
      "\t 10\n",
      "Invalid caption generation for 2321443 pillow  pillow to the right of pillow \n",
      "Invalid caption generation for 2321443 pillow  pillow to the right of pillow \n",
      "Already finished 2500\n",
      "\tAccuracy 69.24\n",
      "\t 12\n",
      "Already finished 3000\n",
      "\tAccuracy 69.03333333333333\n",
      "\t 16\n",
      "Already finished 3500\n",
      "\tAccuracy 68.62857142857143\n",
      "\t 17\n",
      "Already finished 4000\n",
      "\tAccuracy 68.875\n",
      "\t 19\n",
      "Already finished 4500\n",
      "\tAccuracy 68.71111111111111\n",
      "\t 21\n",
      "Already finished 5000\n",
      "\tAccuracy 68.46\n",
      "\t 26\n",
      "Already finished 5500\n",
      "\tAccuracy 68.52727272727273\n",
      "\t 32\n",
      "Already finished 6000\n",
      "\tAccuracy 68.45\n",
      "\t 33\n",
      "Already finished 6500\n",
      "\tAccuracy 68.24615384615385\n",
      "\t 36\n",
      "Already finished 7000\n",
      "\tAccuracy 68.47142857142858\n",
      "\t 39\n",
      "Already finished 7500\n",
      "\tAccuracy 68.2\n",
      "\t 43\n",
      "Already finished 8000\n",
      "\tAccuracy 68.2375\n",
      "\t 44\n",
      "Already finished 8500\n",
      "\tAccuracy 68.36470588235294\n",
      "\t 47\n",
      "Already finished 9000\n",
      "\tAccuracy 68.2\n",
      "\t 50\n",
      "Already finished 9500\n",
      "\tAccuracy 68.18947368421053\n",
      "\t 56\n",
      "Already finished 10000\n",
      "\tAccuracy 68.29\n",
      "\t 60\n",
      "Already finished 10500\n",
      "\tAccuracy 68.31428571428572\n",
      "\t 61\n",
      "Invalid caption generation for 2394519 pillow  pillow to the right of pillow \n",
      "Invalid caption generation for 2394519 pillow  pillow to the right of pillow \n",
      "Already finished 11000\n",
      "\tAccuracy 68.43636363636364\n",
      "\t 66\n",
      "Already finished 11500\n",
      "\tAccuracy 68.4\n",
      "\t 67\n",
      "Already finished 12000\n",
      "\tAccuracy 68.45833333333333\n",
      "\t 73\n",
      "Already finished 12500\n",
      "\tAccuracy 68.552\n",
      "\t 75\n",
      "Already finished 13000\n",
      "\tAccuracy 68.49230769230769\n",
      "\t 79\n",
      "Already finished 13500\n",
      "\tAccuracy 68.5037037037037\n",
      "\t 80\n",
      "Already finished 14000\n",
      "\tAccuracy 68.46428571428571\n",
      "\t 85\n",
      "Already finished 14500\n",
      "\tAccuracy 68.48965517241379\n",
      "\t 87\n",
      "Already finished 15000\n",
      "\tAccuracy 68.56\n",
      "\t 89\n",
      "Already finished 15500\n",
      "\tAccuracy 68.6\n",
      "\t 90\n",
      "Already finished 16000\n",
      "\tAccuracy 68.6125\n",
      "\t 91\n",
      "Invalid caption generation for 2324202 pillow  pillow to the right of pillow \n",
      "Invalid caption generation for 2324202 pillow  pillow to the right of pillow \n",
      "16096\n",
      "93\n",
      "68.62574552683897\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "hold_dict={'hposition': 6502, 'name': 47463, 'color': 7790, \n",
    "           'material': 1387, 'size': 860, 'place': 2166, 'length': 203}\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "invalid=[]\n",
    "acc=0.0\n",
    "total={}\n",
    "for k,name in enumerate(names):\n",
    "    row=gqa_val_q[name]\n",
    "    if row['types']['structural']!='logical':\n",
    "        continue\n",
    "    new_prog=meta_prog[name]\n",
    "    texts=[' '.join([str(vis),row['question'],row['imageId'],name])]\n",
    "    for i, step in enumerate(new_prog[:-1]):\n",
    "        dep=[str(d) for d in step['dependencies']]\n",
    "        dep=','.join(dep)\n",
    "        dep='  '+dep\n",
    "        texts.append('--'.join([str(i),step['operation'],step['argument'][0],dep]))\n",
    "    texts.append(new_prog[-1]['operation'])\n",
    "    texts='\\n'.join(texts)\n",
    "    \"\"\"fs_prog,sd_prog=separate_prog(new_prog)\n",
    "    print_prog(fs_prog)\n",
    "    print ('\\n')\n",
    "    print_prog(sd_prog)\n",
    "    print ('\\n')\"\"\"\n",
    "    #pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    try:\n",
    "        pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "    vis+=1\n",
    "    total[name]=pred\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('Already finished',vis)\n",
    "        print ('\\tAccuracy',acc*100.0/vis)\n",
    "        print ('\\t',len(invalid))\n",
    "        \n",
    "print (vis)\n",
    "print (len(invalid))\n",
    "print (acc*100.0/vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3de536c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(total,open('logical.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8440a2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0--select--blanket--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--blanket--  \n",
      "\t 1--select--bed--  \n",
      "\t 2--exist--?--  0\n",
      "Are there both blankets and beds in the picture? yes\n",
      "2319473 00355900\n",
      "\t 0 select ['blanket']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['bed']\n",
      "\t 3 exist ['?']\n",
      "\t 4 and []\n",
      "yes yes Prediction: yes \n",
      "\n",
      "\t 0--select--window--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--window--  \n",
      "\t 1--select--door--  \n",
      "\t 2--exist--?--  0\n",
      "Are there both a door and a window in the picture? yes\n",
      "2397563 17135398\n",
      "\t 0 select ['window']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['door']\n",
      "\t 3 exist ['?']\n",
      "\t 4 and []\n",
      "yes yes Prediction: yes \n",
      "\n",
      "\t 0--select--coach--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--coach--  \n",
      "\t 1--select--employee--  \n",
      "\t 2--exist--?--  0\n",
      "Are there any coaches or employees? no\n",
      "2340117 07806226\n",
      "\t 0 select ['coach']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['employee']\n",
      "\t 3 exist ['?']\n",
      "\t 4 or []\n",
      "yes yes Prediction: yes \n",
      "\n",
      "\t 0--select--plate--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--plate--  \n",
      "\t 1--select--sandwich--  \n",
      "\t 2--exist--?--  0\n",
      "Do you see either plates or sandwiches there? yes\n",
      "2364527 01389577\n",
      "\t 0 select ['plate']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['sandwich']\n",
      "\t 3 exist ['?']\n",
      "\t 4 or []\n",
      "yes yes Prediction: yes \n",
      "\n",
      "\t 0--select--cabinet--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--cabinet--  \n",
      "\t 1--select--faucet--  \n",
      "\t 2--exist--?--  0\n",
      "Are there either any faucets or cabinets in the picture? yes\n",
      "2375294 13495051\n",
      "\t 0 select ['cabinet']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['faucet']\n",
      "\t 3 exist ['?']\n",
      "\t 4 or []\n",
      "yes yes Prediction: yes \n",
      "\n",
      "\t 0--select--pillow--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--pillow--  \n",
      "\t 1--select--object--  \n",
      "\t 2--exist--?--  0\n",
      "Are there pillows or objects? yes\n",
      "2336032 19106571\n",
      "\t 0 select ['pillow']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['object']\n",
      "\t 3 exist ['?']\n",
      "\t 4 or []\n",
      "no no Prediction: no \n",
      "\n",
      "\t 0--select--shelf--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--shelf--  \n",
      "\t 1--select--chair--  \n",
      "\t 2--exist--?--  0\n",
      "Are there any chairs or shelves? no\n",
      "2370435 0693352\n",
      "\t 0 select ['shelf']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['chair']\n",
      "\t 3 exist ['?']\n",
      "\t 4 or []\n",
      "yes yes Prediction: yes \n",
      "\n",
      "\t 0--select--pillow--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--pillow--  \n",
      "\t 1--select--bed--  \n",
      "\t 2--exist--?--  0\n",
      "Are there either any pillows or beds in the scene? yes\n",
      "2369300 15569714\n",
      "\t 0 select ['pillow']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['bed']\n",
      "\t 3 exist ['?']\n",
      "\t 4 or []\n",
      "yes yes Prediction: yes \n",
      "\n",
      "\t 0--select--train--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--train--  \n",
      "\t 1--select--car--  \n",
      "\t 2--exist--?--  0\n",
      "Are there trains or cars? yes\n",
      "2324168 02490096\n",
      "\t 0 select ['train']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['car']\n",
      "\t 3 exist ['?']\n",
      "\t 4 or []\n",
      "yes yes Prediction: yes \n",
      "\n",
      "\t 0--select--table--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--table--  \n",
      "\t 1--select--chair--  \n",
      "\t 2--exist--?--  0\n",
      "Are there both a chair and a table in the picture? yes\n",
      "2402457 1664719\n",
      "\t 0 select ['table']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['chair']\n",
      "\t 3 exist ['?']\n",
      "\t 4 and []\n",
      "yes yes Prediction: yes \n",
      "\n",
      "\t 0--select--computer--  \n",
      "\t 1--exist--?--  0\n",
      "\t 0--select--computer--  \n",
      "\t 1--select--papers--  \n",
      "\t 2--exist--?--  0\n",
      "Are there papers or computers? yes\n",
      "150449 13654527\n",
      "\t 0 select ['computer']\n",
      "\t 1 exist ['?']\n",
      "\t 2 select ['papers']\n",
      "\t 3 exist ['?']\n",
      "\t 4 or []\n",
      "yes yes Prediction: yes \n",
      "\n",
      "11\n",
      "0\n",
      "72.72727272727273\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "hold_dict={'hposition': 6502, 'name': 47463, 'color': 7790, \n",
    "           'material': 1387, 'size': 860, 'place': 2166, 'length': 203}\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "invalid=[]\n",
    "acc=0.0\n",
    "for k,name in enumerate(names):\n",
    "    if vis>10:\n",
    "        break\n",
    "    row=gqa_val_q[name]\n",
    "    if row['types']['structural']!='logical':\n",
    "        continue\n",
    "    new_prog=meta_prog[name]\n",
    "    if len(new_prog)!=5:\n",
    "        continue\n",
    "    texts=[' '.join([str(vis),row['question'],row['imageId'],name])]\n",
    "    for i, step in enumerate(new_prog[:-1]):\n",
    "        dep=[str(d) for d in step['dependencies']]\n",
    "        dep=','.join(dep)\n",
    "        dep='  '+dep\n",
    "        texts.append('--'.join([str(i),step['operation'],step['argument'][0],dep]))\n",
    "    texts.append(new_prog[-1]['operation'])\n",
    "    texts='\\n'.join(texts)\n",
    "    fs_prog,sd_prog=separate_prog(new_prog)\n",
    "    print_prog(fs_prog)\n",
    "    print_prog(sd_prog)\n",
    "    #pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    try:\n",
    "        pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "    print (row['question'],row['answer'])\n",
    "    print (row['imageId'],name)\n",
    "    for i,step in enumerate(new_prog):\n",
    "        print ('\\t',i,step['operation'],step['argument'])\n",
    "    print (fs_ans,sd_ans,'Prediction:',pred,'\\n')\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('Already finished',vis)\n",
    "        print ('\\tAccuracy',acc*100.0/vis)\n",
    "        print ('\\t',len(invalid))\n",
    "        \n",
    "print (vis)\n",
    "print (len(invalid))\n",
    "print (acc*100.0/vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49f83352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already finished 500\n",
      "\tAccuracy 78.8\n",
      "\t 7\n",
      "Already finished 1000\n",
      "\tAccuracy 79.5\n",
      "\t 12\n",
      "Already finished 1500\n",
      "\tAccuracy 79.4\n",
      "\t 19\n",
      "Already finished 2000\n",
      "\tAccuracy 79.8\n",
      "\t 21\n",
      "Already finished 2500\n",
      "\tAccuracy 79.52\n",
      "\t 27\n",
      "Already finished 3000\n",
      "\tAccuracy 79.8\n",
      "\t 33\n",
      "Already finished 3500\n",
      "\tAccuracy 79.77142857142857\n",
      "\t 39\n",
      "Already finished 4000\n",
      "\tAccuracy 79.8\n",
      "\t 45\n",
      "Already finished 4500\n",
      "\tAccuracy 79.71111111111111\n",
      "\t 52\n",
      "Already finished 5000\n",
      "\tAccuracy 79.72\n",
      "\t 56\n",
      "Already finished 5500\n",
      "\tAccuracy 79.6\n",
      "\t 61\n",
      "Already finished 6000\n",
      "\tAccuracy 79.4\n",
      "\t 65\n",
      "Already finished 6500\n",
      "\tAccuracy 79.36923076923077\n",
      "\t 67\n",
      "Already finished 7000\n",
      "\tAccuracy 79.32857142857142\n",
      "\t 68\n",
      "Already finished 7500\n",
      "\tAccuracy 79.22666666666667\n",
      "\t 72\n",
      "Already finished 8000\n",
      "\tAccuracy 79.3125\n",
      "\t 77\n",
      "Already finished 8500\n",
      "\tAccuracy 79.37647058823529\n",
      "\t 80\n",
      "Already finished 9000\n",
      "\tAccuracy 79.53333333333333\n",
      "\t 87\n",
      "9083\n",
      "89\n",
      "79.53319387867445\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "hold_dict={'hposition': 6502, 'name': 47463, 'color': 7790, \n",
    "           'material': 1387, 'size': 860, 'place': 2166, 'length': 203}\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "invalid=[]\n",
    "acc=0.0\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    if row['types']['structural']!='logical':\n",
    "        continue\n",
    "    new_prog=meta_prog[name]\n",
    "    if len(new_prog)!=5:\n",
    "        continue\n",
    "    texts=[' '.join([str(vis),row['question'],row['imageId'],name])]\n",
    "    for i, step in enumerate(new_prog[:-1]):\n",
    "        dep=[str(d) for d in step['dependencies']]\n",
    "        dep=','.join(dep)\n",
    "        dep='  '+dep\n",
    "        texts.append('--'.join([str(i),step['operation'],step['argument'][0],dep]))\n",
    "    texts.append(new_prog[-1]['operation'])\n",
    "    texts='\\n'.join(texts)\n",
    "    \"\"\"fs_prog,sd_prog=separate_prog(new_prog)\n",
    "    print_prog(fs_prog)\n",
    "    print ('\\n')\n",
    "    print_prog(sd_prog)\n",
    "    print ('\\n')\"\"\"\n",
    "    #pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    try:\n",
    "        pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('Already finished',vis)\n",
    "        print ('\\tAccuracy',acc*100.0/vis)\n",
    "        print ('\\t',len(invalid))\n",
    "        \n",
    "print (vis)\n",
    "print (len(invalid))\n",
    "print (acc*100.0/vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c24e277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/torch/hub/ashkamath_mdetr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already finished 500\n",
      "\tAccuracy 54.6\n",
      "\t 0\n",
      "Already finished 1000\n",
      "\tAccuracy 55.2\n",
      "\t 0\n",
      "Already finished 1500\n",
      "\tAccuracy 55.2\n",
      "\t 1\n",
      "1913\n",
      "2\n",
      "54.260324098274964\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "hold_dict={'hposition': 6502, 'name': 47463, 'color': 7790, \n",
    "           'material': 1387, 'size': 860, 'place': 2166, 'length': 203}\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "invalid=[]\n",
    "acc=0.0\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    if row['types']['structural']!='logical':\n",
    "        continue\n",
    "    new_prog=meta_prog[name]\n",
    "    if len(new_prog)!=6:\n",
    "        continue\n",
    "    texts=[' '.join([str(vis),row['question'],row['imageId'],name])]\n",
    "    for i, step in enumerate(new_prog[:-1]):\n",
    "        dep=[str(d) for d in step['dependencies']]\n",
    "        dep=','.join(dep)\n",
    "        dep='  '+dep\n",
    "        texts.append('--'.join([str(i),step['operation'],step['argument'][0],dep]))\n",
    "    texts.append(new_prog[-1]['operation'])\n",
    "    texts='\\n'.join(texts)\n",
    "    \"\"\"fs_prog,sd_prog=separate_prog(new_prog)\n",
    "    print_prog(fs_prog)\n",
    "    print ('\\n')\n",
    "    print_prog(sd_prog)\n",
    "    print ('\\n')\"\"\"\n",
    "    #pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    try:\n",
    "        pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('Already finished',vis)\n",
    "        print ('\\tAccuracy',acc*100.0/vis)\n",
    "        print ('\\t',len(invalid))\n",
    "        \n",
    "print (vis)\n",
    "print (len(invalid))\n",
    "print (acc*100.0/vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98ee5de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already finished 500\n",
      "\tAccuracy 55.4\n",
      "\t 0\n",
      "Already finished 1000\n",
      "\tAccuracy 58.3\n",
      "\t 1\n",
      "Already finished 1500\n",
      "\tAccuracy 57.13333333333333\n",
      "\t 1\n",
      "Already finished 2000\n",
      "\tAccuracy 57.15\n",
      "\t 1\n",
      "Already finished 2500\n",
      "\tAccuracy 56.52\n",
      "\t 1\n",
      "Already finished 3000\n",
      "\tAccuracy 56.2\n",
      "\t 2\n",
      "Already finished 3500\n",
      "\tAccuracy 56.05714285714286\n",
      "\t 2\n",
      "3766\n",
      "2\n",
      "55.86829527349973\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "hold_dict={'hposition': 6502, 'name': 47463, 'color': 7790, \n",
    "           'material': 1387, 'size': 860, 'place': 2166, 'length': 203}\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "invalid=[]\n",
    "acc=0.0\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    if row['types']['structural']!='logical':\n",
    "        continue\n",
    "    new_prog=meta_prog[name]\n",
    "    if len(new_prog)!=7:\n",
    "        continue\n",
    "    texts=[' '.join([str(vis),row['question'],row['imageId'],name])]\n",
    "    for i, step in enumerate(new_prog[:-1]):\n",
    "        dep=[str(d) for d in step['dependencies']]\n",
    "        dep=','.join(dep)\n",
    "        dep='  '+dep\n",
    "        texts.append('--'.join([str(i),step['operation'],step['argument'][0],dep]))\n",
    "    texts.append(new_prog[-1]['operation'])\n",
    "    texts='\\n'.join(texts)\n",
    "    \"\"\"fs_prog,sd_prog=separate_prog(new_prog)\n",
    "    print_prog(fs_prog)\n",
    "    print ('\\n')\n",
    "    print_prog(sd_prog)\n",
    "    print ('\\n')\"\"\"\n",
    "    #pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    try:\n",
    "        pred,fs_ans,sd_ans=ans_logical(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('Already finished',vis)\n",
    "        print ('\\tAccuracy',acc*100.0/vis)\n",
    "        print ('\\t',len(invalid))\n",
    "        \n",
    "print (vis)\n",
    "print (len(invalid))\n",
    "print (acc*100.0/vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403fbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
