{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12036f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from skimage.measure import find_contours\n",
    "\n",
    "from matplotlib import patches,  lines\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "torch.set_grad_enabled(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69298bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "VQA_PATH='/Data_Storage/Rui_Data_Space/multimodal/VQA'\n",
    "GQA_PATH='/Data_Storage/Rui_Data_Space/multimodal/GQA'\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def load_pkl(path):\n",
    "    data=pkl.load(open(path,'rb'))\n",
    "    return data\n",
    "\n",
    "def load_json(path):\n",
    "    data=json.load(open(path,'r'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412c5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5745957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "stemmer = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2513c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132062\n",
      "10696\n"
     ]
    }
   ],
   "source": [
    "gqa_val_q=json.load(\n",
    "    open(os.path.join(GQA_PATH,'original','val_balanced_questions.json')\n",
    "         ,'r')\n",
    ")\n",
    "print (len(gqa_val_q))\n",
    "names=list(gqa_val_q.keys())\n",
    "\n",
    "val_graphs=json.load(\n",
    "    open(os.path.join(GQA_PATH,'val_sceneGraphs.json')\n",
    "         ,'r')\n",
    ")\n",
    "print (len(val_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e2da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage import io as skimage_io\n",
    "from skimage import transform as skimage_transform\n",
    "\n",
    "def get_tokens(text_queries):\n",
    "    tokenized_queries = np.array([\n",
    "        module.tokenize(q, config.dataset_configs.max_query_length)\n",
    "        for q in text_queries\n",
    "    ])\n",
    "    # Pad tokenized queries to avoid recompilation if number of queries changes:\n",
    "    tokenized_queries = np.pad(\n",
    "        tokenized_queries,\n",
    "        pad_width=((0, 100 - len(text_queries)), (0, 0)),\n",
    "        constant_values=0)\n",
    "    return tokenized_queries\n",
    "\n",
    "def get_gqa_feat(img_id):\n",
    "    # Load example image:\n",
    "    filename = os.path.join(GQA_PATH,'images',img_id+'.jpg')\n",
    "    image_uint8 = skimage_io.imread(filename)\n",
    "    image = image_uint8.astype(np.float32) / 255.0\n",
    "\n",
    "    # Pad to square with gray pixels on bottom and right:\n",
    "    h, w, _ = image.shape\n",
    "    size = max(h, w)\n",
    "    image_padded = np.pad(\n",
    "        image, ((0, size - h), (0, size - w), (0, 0)), constant_values=0.5)\n",
    "\n",
    "    # Resize to model input size:\n",
    "    input_image = skimage.transform.resize(\n",
    "        image_padded,\n",
    "        (840, 840),\n",
    "        anti_aliasing=True)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df748d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ashkamath_mdetr_main\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "CUDA_DEVICE=12\n",
    "torch.cuda.set_device(CUDA_DEVICE)\n",
    "device = torch.device(\"cuda:\"+str(CUDA_DEVICE))\n",
    "#the default ipykernel links to the first conda environment\n",
    "\n",
    "model, postprocessor = torch.hub.load('ashkamath/mdetr:main', 'mdetr_efficientnetB5', pretrained=True, return_postprocessor=True)\n",
    "model = model.to(device)\n",
    "model.eval();\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "416f3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inference(im, caption, plot=True):\n",
    "    # mean-std normalize the input image (batch-size: 1)\n",
    "    img = transform(im).unsqueeze(0).cuda()\n",
    "\n",
    "    # propagate through the model\n",
    "    memory_cache = model(img, [caption], encode_and_save=True)\n",
    "    outputs = model(img, [caption], encode_and_save=False, memory_cache=memory_cache)\n",
    "    #print (outputs['pred_logits'])\n",
    "\n",
    "    # keep only predictions with 0.7+ confidence\n",
    "    probas = 1 - outputs['pred_logits'].softmax(-1)[0, :, -1].cpu()\n",
    "    keep = (probas > 0.7).cpu()\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'].cpu()[0, keep], im.size)\n",
    "\n",
    "    # Extract the text spans predicted by each box\n",
    "    positive_tokens = (outputs[\"pred_logits\"].cpu()[0, keep].softmax(-1) > 0.1).nonzero().tolist()\n",
    "    predicted_spans = defaultdict(str)\n",
    "    for tok in positive_tokens:\n",
    "        item, pos = tok\n",
    "        if pos < 255:\n",
    "            span = memory_cache[\"tokenized\"].token_to_chars(0, pos)\n",
    "            predicted_spans [item] += \" \" + caption[span.start:span.end]\n",
    "\n",
    "    labels = [predicted_spans [k] for k in sorted(list(predicted_spans .keys()))]\n",
    "    if plot:\n",
    "        plot_results(im, probas[keep], bboxes_scaled, labels)\n",
    "    return probas[keep], bboxes_scaled.tolist(), labels\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def apply_mask(image, mask, color, alpha=0.5):\n",
    "    \"\"\"Apply the given mask to the image.\n",
    "    \"\"\"\n",
    "    for c in range(3):\n",
    "        image[:, :, c] = np.where(mask == 1,\n",
    "                                  image[:, :, c] *\n",
    "                                  (1 - alpha) + alpha * color[c] * 255,\n",
    "                                  image[:, :, c])\n",
    "    return image\n",
    "\n",
    "def plot_results(pil_img, scores, boxes, labels, masks=None):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    np_image = np.array(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    if masks is None:\n",
    "        masks = [None for _ in range(len(scores))]\n",
    "    assert len(scores) == len(boxes) == len(labels) == len(masks)\n",
    "    for s, (xmin, ymin, xmax, ymax), l, mask, c in zip(scores, boxes.tolist(), labels, masks, colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f'{l}: {s:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "        if mask is None:\n",
    "            continue\n",
    "        np_image = apply_mask(np_image, mask, c)\n",
    "\n",
    "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
    "        padded_mask[1:-1, 1:-1] = mask\n",
    "        contours = find_contours(padded_mask, 0.5)\n",
    "        for verts in contours:\n",
    "            # Subtract the padding and flip (y, x) to (x, y)\n",
    "            verts = np.fliplr(verts) - 1\n",
    "            p = Polygon(verts, facecolor=\"none\", edgecolor=c)\n",
    "            ax.add_patch(p)\n",
    "\n",
    "\n",
    "    plt.imshow(np_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def add_res(results, ax, color='green'):\n",
    "    #for tt in results.values():\n",
    "    if True:\n",
    "        bboxes = results['boxes']\n",
    "        labels = results['labels']\n",
    "        scores = results['scores']\n",
    "        #keep = scores >= 0.0\n",
    "        #bboxes = bboxes[keep].tolist()\n",
    "        #labels = labels[keep].tolist()\n",
    "        #scores = scores[keep].tolist()\n",
    "    #print(torchvision.ops.box_iou(tt['boxes'].cpu().detach(), torch.as_tensor([[xmin, ymin, xmax, ymax]])))\n",
    "    \n",
    "    colors = ['purple', 'yellow', 'red', 'green', 'orange', 'pink']\n",
    "    \n",
    "    for i, (b, ll, ss) in enumerate(zip(bboxes, labels, scores)):\n",
    "        ax.add_patch(plt.Rectangle((b[0], b[1]), b[2] - b[0], b[3] - b[1], fill=False, color=colors[i], linewidth=3))\n",
    "        cls_name = ll if isinstance(ll,str) else CLASSES[ll]\n",
    "        text = f'{cls_name}: {ss:.2f}'\n",
    "        print(text)\n",
    "        ax.text(b[0], b[1], text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "578804e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_bbox_only(bbox,scores,input_image,gt_bbox=None,text=None,threshold=0.2,vis_pred=True):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    if vis_pred:\n",
    "        for i,box in enumerate(bbox):\n",
    "            score=scores[i]\n",
    "            if score<threshold:\n",
    "                continue\n",
    "            cx, cy, w, h = box\n",
    "            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
    "            ax.text(\n",
    "                cx - w / 2,\n",
    "                cy + h / 2 + 0.015,\n",
    "                f'{score:1.2f}',\n",
    "                ha='left',\n",
    "                va='top',\n",
    "                color='red',\n",
    "                bbox={\n",
    "                    'facecolor': 'white',\n",
    "                    'edgecolor': 'red',\n",
    "                    'boxstyle': 'square,pad=.3'\n",
    "                })\n",
    "    if text is not None:\n",
    "        ax.set_title(text,  fontsize=12)\n",
    "    if gt_bbox is not None:\n",
    "        for i,box in enumerate(gt_bbox):\n",
    "            cx, cy, w, h = box\n",
    "            ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                    [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7988503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_coord(bbox,img_id):\n",
    "    width=val_graphs[img_id]['width']\n",
    "    height=val_graphs[img_id]['height']\n",
    "    size = max(height, width)\n",
    "    x=bbox[0]\n",
    "    y=bbox[1]\n",
    "    w=bbox[2]-bbox[0]\n",
    "    h=bbox[3]-bbox[1]\n",
    "    #print (bbox)\n",
    "    return [(x+w/2)/size,(y+h/2)/size,w/size,h/size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ece93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mdert_result(scores,bboxs,labels,obj,img_id):\n",
    "    sf_s=[]\n",
    "    sf_bbox=[]\n",
    "    labels=[''.join(label.split(' ')) for label in labels]\n",
    "    flag=False\n",
    "    if obj in labels:\n",
    "        flag=True\n",
    "    #print (obj,labels)\n",
    "    for i,label in enumerate(labels):\n",
    "        if flag:\n",
    "            if obj==label:\n",
    "                sf_s.append(scores[i])\n",
    "                sf_bbox.append(bboxs[i])\n",
    "        else:\n",
    "            if obj in label:\n",
    "                sf_s.append(scores[i])\n",
    "                sf_bbox.append(bboxs[i])\n",
    "    #print (len(sf_s))\n",
    "    if len(sf_bbox)==0:\n",
    "        for i,label in enumerate(labels):\n",
    "            sf_s.append(scores[i])\n",
    "            sf_bbox.append(bboxs[i])\n",
    "    max_s=max(sf_s)\n",
    "    max_id=sf_s.index(max_s)\n",
    "    return_bbox=sf_bbox[max_id]\n",
    "    coord=transform_coord(return_bbox,img_id)\n",
    "    return max_s.item(),coord\n",
    "#only consider one bbox now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "968ce628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrase_ground(img_id,cap,obj):\n",
    "    cap=cap+' '\n",
    "    im=Image.open(os.path.join(GQA_PATH,'images',img_id+'.jpg'))\n",
    "    try:\n",
    "        scores,bboxs,labels=plot_inference(im, cap, plot=False)\n",
    "    except:\n",
    "        print ('Invalid caption generation for',img_id,obj,cap)\n",
    "        scores=[torch.Tensor([0.99])]\n",
    "        bboxs=[[0.5,0.5,0.99,0.99]]\n",
    "        labels=[obj]\n",
    "    if len(bboxs)==0:\n",
    "        max_s=0.99\n",
    "        valid_bbox=[0.5,0.5,0.99,0.99]\n",
    "    else:\n",
    "        # print (scores,bboxs)\n",
    "        max_s,valid_bbox=preprocess_mdert_result(scores,bboxs,labels,obj,img_id)\n",
    "    return max_s,valid_bbox\n",
    "\n",
    "def bbox_generator(layout,idx,img_id,ques_id):\n",
    "    if layout[idx]['operation']=='select':\n",
    "        #print (idx)\n",
    "        try:\n",
    "            scenic_result=load_pkl(os.path.join(GQA_PATH,\n",
    "                                                'meta_one-all-scenic',\n",
    "                                                ques_id+'.pkl'))\n",
    "            obj=layout[idx]['argument'][0]\n",
    "            scores=[s['score'] for s in scenic_result[obj]]\n",
    "            bboxs=[s['bbox'] for s in scenic_result[obj]]\n",
    "        except:\n",
    "            bboxs=[]\n",
    "            obj=layout[idx]['argument'][0]\n",
    "        if len(bboxs)>0:\n",
    "            max_s=max(scores)\n",
    "            max_idx=scores.index(max_s)\n",
    "            bbox=bboxs[max_idx]\n",
    "            cap=obj\n",
    "        else:\n",
    "            cap=obj\n",
    "            split_obj=''.join(obj.split(' '))\n",
    "            #print (cap,split_obj)\n",
    "            max_s,bbox=generate_phrase_ground(img_id,cap,split_obj)\n",
    "            #print (max_s,bbox)\n",
    "    else:\n",
    "        cap,obj=cap_generator(layout,idx,img_id)\n",
    "        max_s,bbox=generate_phrase_ground(img_id,cap,obj)\n",
    "    return max_s,bbox,cap        \n",
    "            \n",
    "def cap_generator(layout,idx,img_id):\n",
    "    words=[]\n",
    "    dep=layout[idx]['dependencies']\n",
    "    word=layout[idx]['argument'][0]\n",
    "    words.append(word)\n",
    "    obj=''.join(word.split(' '))\n",
    "    while len(dep)>0:\n",
    "        cur_step=layout[dep[0]]\n",
    "        if cur_step['operation']=='relocate':\n",
    "            relo_symbol=cur_step['argument'][1]\n",
    "            cur_phrase=' '.join(words)\n",
    "            words=[]\n",
    "            words.append(cur_phrase)\n",
    "            #print(cur_step)\n",
    "            words.append(cur_step['argument'][0])\n",
    "            dep=cur_step['dependencies']\n",
    "            cur_step=layout[dep[0]]\n",
    "            dep=cur_step['dependencies']\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            if relo_symbol=='o':\n",
    "                words=list(reversed(words))\n",
    "        else:\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            dep=cur_step['dependencies']\n",
    "    cap=' '.join(words)\n",
    "    return cap,obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f37fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_bbox_generator_exist(layout,idx,img_id):\n",
    "    cap=cap_generator_exist(layout,idx,img_id)\n",
    "    max_s,bbox=generate_phrase_ground(img_id,cap,cap)\n",
    "    return bbox,cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce0e2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped(bb,input_image,showing=False):\n",
    "    width=bb[2]*840\n",
    "    height=bb[3]*840\n",
    "    xy=(bb[0]*840-width/2,bb[1]*840-height/2)\n",
    "    h0=int(max(0,xy[1]))\n",
    "    w0=int(max(0,xy[0]))\n",
    "    h1=int(max(0,840-(height+xy[1])))\n",
    "    w1=int(max(0,840-(width+xy[0])))\n",
    "    cropped=skimage.util.crop(input_image,((h0,h1),(w0,w1),(0,0)), copy=False)\n",
    "    trans_crop=Image.fromarray(np.uint8(cropped*255.0))\n",
    "    if showing:\n",
    "        display.display(trans_crop)\n",
    "    return trans_crop\n",
    "\n",
    "def clip_aware_pred(img_feat,valid_bbox,candidates,showing=False):\n",
    "    #tokens=clip.tokenize(candidates)\n",
    "    trans_crop=get_cropped(valid_bbox,img_feat)\n",
    "    if showing:\n",
    "        display.display(trans_crop)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "        text_features = clip_model.encode_text(clip.tokenize(candidates).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    values, indices = similarity.topk(2)\n",
    "    #print (indices[0][0].item())\n",
    "    ans=candidates[indices[0][0].item()]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99c4fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_holder=['left','right','bottom','top','front']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18aa2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b63f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(ques):\n",
    "    nouns=[]\n",
    "    tokens_tag = pos_tag(ques.replace('?','').split(' '))\n",
    "    for w,pos in tokens_tag:\n",
    "        if pos in ['NN','NNS'] and w not in position_holder:\n",
    "            nouns.append(lemmatizer.lemmatize(w))\n",
    "    return nouns\n",
    "\n",
    "def verify_noun(nouns,obj):\n",
    "    if obj in nouns or \\\n",
    "    lemmatizer.lemmatize(obj) in nouns or\\\n",
    "    stemmer.singular_noun(obj) in nouns:\n",
    "        return True\n",
    "    elif len(obj.split(' '))>1:\n",
    "        words=obj.split(' ')\n",
    "        for w in words:\n",
    "            #print (w)\n",
    "            if lemmatizer.lemmatize(w) in nouns or\\\n",
    "            stemmer.singular_noun(w) in nouns:\n",
    "                return True\n",
    "    elif obj in ['he','she','they']:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "790647e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relo(layout):\n",
    "    ops=[step['operation'] for step in layout]\n",
    "    length=len(layout)-2\n",
    "    relo_id=length\n",
    "    counter=0\n",
    "    rela_symbol=[]\n",
    "    for i in range(len(layout)-2):\n",
    "        cur_op=layout[length-i]['operation']\n",
    "        if cur_op=='relocate':\n",
    "            relo_id=length-i\n",
    "            rela_symbol=layout[length-i]['argument']\n",
    "            break\n",
    "        elif cur_op=='filter':\n",
    "            counter+=1#find some attributes finally, if true\n",
    "    return rela_symbol,relo_id, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4b6578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_generator_exist(layout,idx,img_id):\n",
    "    words=[]\n",
    "    dep=layout[idx]['dependencies']\n",
    "    while len(dep)>0:\n",
    "        cur_step=layout[dep[0]]\n",
    "        if cur_step['operation']=='relocate':\n",
    "            relo_symbol=cur_step['argument'][1]\n",
    "            cur_phrase=' '.join(words)\n",
    "            words=[]\n",
    "            words.append(cur_phrase)\n",
    "            #print(cur_step)\n",
    "            words.append(cur_step['argument'][0])\n",
    "            dep=cur_step['dependencies']\n",
    "            new_sent=[]\n",
    "            while len(dep)>0:\n",
    "                cur_step=layout[dep[0]]\n",
    "                dep=cur_step['dependencies']\n",
    "                arg=cur_step['argument'][0]\n",
    "                new_sent.append(arg)\n",
    "            words.append(' '.join(new_sent))\n",
    "            if relo_symbol=='o':\n",
    "                words=list(reversed(words))\n",
    "        else:\n",
    "            arg=cur_step['argument'][0]\n",
    "            words.append(arg)\n",
    "            dep=cur_step['dependencies']\n",
    "    cap=' '.join(words)\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9924d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relocate_involved_ans(layout,img_id,ques_id,thresh=0.2):\n",
    "    nouns=get_nouns(gqa_val_q[ques_id]['question'][:-1])#remove the punctuation\n",
    "    ops=[step['operation'] for step in layout]\n",
    "    #must have select operation\n",
    "    for m,step in enumerate(layout[:-1]):\n",
    "        op=step['operation']\n",
    "        if op not in ['filter','select']:\n",
    "            continue\n",
    "        arg=step['argument'][0]\n",
    "        if arg in hold_list:\n",
    "            continue\n",
    "        if op=='filter':\n",
    "            flag=verify_noun(nouns,arg)\n",
    "            if flag==False:#not a noun\n",
    "                continue\n",
    "        if op=='select':\n",
    "            #infos=select_info[arg]\n",
    "            s,_,_=bbox_generator(layout,m,img_id,ques_id)\n",
    "            if s<thresh:\n",
    "                return 'no'\n",
    "        else:\n",
    "            try:\n",
    "                filter_info=load_pkl(os.path.join(GQA_PATH,\n",
    "                                              'meta_filter-related',\n",
    "                                              ques_id+'.pkl'))\n",
    "                infos=filter_info[arg]\n",
    "                s=[info['score'] for info in infos if info['score']>=thresh]\n",
    "            except:\n",
    "                print('No filter result',arg,ques_id)\n",
    "                continue\n",
    "            \n",
    "            if len(s)==0:\n",
    "                return 'no'\n",
    "    rela_symbol,rela_id,counter=get_relo(new_prog)    \n",
    "    if counter==1:\n",
    "        pred=verify_relation(layout,img_id,ques_id,rela_symbol,rela_id)\n",
    "    else:\n",
    "        pred=verify_attr_last_step(layout,img_id,ques_id)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da97a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "heur_rela=['to the left of','to the right of',\n",
    "           'above','under','on top of','below',\n",
    "           'beneath','underneath']\n",
    "#if color on the last argument, use phrase grounding and binary color function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d73dcb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heur_rules(bb_a,bb_b,rela):\n",
    "    #whether there is bb_a rela bb_b\n",
    "    if rela=='to the left of':\n",
    "        if bb_a[0]<bb_b[0]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif rela=='to the right of':\n",
    "        if bb_a[0]>bb_b[0]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif rela in ['above','on top of']:\n",
    "        if bb_a[1]>bb_b[1]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif rela in [ 'under','below','beneath','underneath']:\n",
    "        if bb_a[1]<bb_b[1]:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f56e067b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_relation(layout,img_id,ques_id,rela_symbol,rela_id,showing=False):\n",
    "    relation=rela_symbol[0]\n",
    "    sub_obj=rela_symbol[1]\n",
    "    obj=layout[-2]['argument'][0]\n",
    "    cap=cap_generator_exist(layout,len(layout)-1,img_id)\n",
    "    cap=cap.replace(obj,'').replace(relation,'')\n",
    "    if relation in heur_rela:\n",
    "        try:\n",
    "            scenic_info=load_pkl(os.path.join(GQA_PATH,\n",
    "                                              'meta_filter-related',\n",
    "                                              ques_id+'.pkl'))\n",
    "        #print (scenic_info.keys())\n",
    "            info=scenic_info[obj]\n",
    "            scores=[i['score'] for i in info]\n",
    "            bboxs=[i['bbox'] for i in info]\n",
    "            if len(scores)==0:\n",
    "                return 'no'\n",
    "            bb_1=bboxs[scores.index(max(scores))]\n",
    "        except:\n",
    "            s,b,_=bbox_generator(layout,len(layout)-2,img_id,ques_id)\n",
    "            bb_1=b\n",
    "        \n",
    "        if rela_id==1:\n",
    "            _,bb_2,_=bbox_generator(layout,0,img_id,ques_id)\n",
    "        else:\n",
    "            obj=layout[rela_id-1]['argument'][0]\n",
    "            _,bb_2=generate_phrase_ground(img_id,cap,obj)\n",
    "        if sub_obj=='o':\n",
    "            bb_a=bb_2\n",
    "            bb_b=bb_1\n",
    "        else:\n",
    "            bb_a=bb_1\n",
    "            bb_b=bb_2\n",
    "        pred=heur_rules(bb_a,bb_b,relation)\n",
    "        if showing:\n",
    "            img_feat=get_gqa_feat(img_id)\n",
    "            #print (bb_a,bb_b)\n",
    "            vis_bbox_only([bb_a],[0.5],img_feat,[bb_b],img_id)\n",
    "    else:\n",
    "        if sub_obj=='s':\n",
    "            pos=' '.join([obj,'is',relation,cap])\n",
    "            neg=' '.join([obj,'is not',relation,cap])\n",
    "        else:\n",
    "            pos=' '.join([cap,'is',relation,obj])\n",
    "            neg=' '.join([cap,'is not',relation,obj])\n",
    "        #print (pos,'\\n',neg,'\\n',gqa_val_q[ques_id]['question'])\n",
    "        pred=binary_matching(img_id,[pos,neg])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0600a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_matching(img_id,sents):\n",
    "    tokens=clip.tokenize(sents)\n",
    "    im=Image.open(os.path.join(GQA_PATH,'images',img_id+'.jpg'))\n",
    "    with torch.no_grad():\n",
    "        text_features=clip_model.encode_text(tokens.to(device))\n",
    "        image_features = clip_model.encode_image(preprocess(im).unsqueeze(0).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    indices=torch.max(similarity,1)[1]\n",
    "    #print (similarity,indices)\n",
    "    similarity=similarity.squeeze()\n",
    "    scores=[str(similarity[0].item()),str(similarity[1].item())]\n",
    "    if indices.squeeze().item()==0:\n",
    "        pred='yes'\n",
    "    else:\n",
    "        pred='no'\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0893ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_attr_last_step(layout,img_id,ques_id,showing=False):\n",
    "    cap=cap_generator_exist(layout,len(layout)-1,img_id)\n",
    "    attr=layout[-2]['argument'][0]\n",
    "    phrase=cap.replace(attr,'')\n",
    "    obj=layout[-3]['argument'][0]\n",
    "    _,bbox=generate_phrase_ground(img_id,phrase,obj)\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    pred=filter_binary_color(bbox,input_image,attr,showing)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da445bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_binary_color(bb,input_image,color,showing=False):\n",
    "    width=bb[2]*840\n",
    "    height=bb[3]*840\n",
    "    xy=(bb[0]*840-width/2,bb[1]*840-height/2)\n",
    "    h0=int(max(0,xy[1]))\n",
    "    w0=int(max(0,xy[0]))\n",
    "    h1=int(max(0,840-(height+xy[1])))\n",
    "    w1=int(max(0,840-(width+xy[0])))\n",
    "    cropped=skimage.util.crop(input_image,((h0,h1),(w0,w1),(0,0)), copy=False)\n",
    "    trans_crop=Image.fromarray(np.uint8(cropped*255.0))\n",
    "    flag=True #color is a positive statement\n",
    "    if 'not' in color:\n",
    "        flag=False#reverse the pred\n",
    "        words=color.split(' ')[1:]\n",
    "        color=' '.join(words)\n",
    "    pos_sent=color\n",
    "    neg_sent='Not '+color\n",
    "    tokens=clip.tokenize([neg_sent,pos_sent])\n",
    "    with torch.no_grad():\n",
    "        text_features=clip_model.encode_text(tokens.to(device))\n",
    "        image_features = clip_model.encode_image(preprocess(trans_crop).unsqueeze(0).to(device))\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "    indices=torch.max(similarity,1)[1]\n",
    "    #print (similarity,indices)\n",
    "    similarity=similarity.squeeze()\n",
    "    scores=[str(similarity[0].item()),str(similarity[1].item())]\n",
    "    if showing:\n",
    "        vis_no_score([bb],input_image,'not '+color+'-'+'-'.join(scores))\n",
    "    if indices.squeeze().item()==1:\n",
    "        pred='yes'\n",
    "    else:\n",
    "        pred='no'\n",
    "    if flag:\n",
    "        return pred\n",
    "    else:\n",
    "        if pred=='yes':\n",
    "            return 'no'\n",
    "        else:\n",
    "            return 'yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3771f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_basic_ans(layout,img_id,ques_id,thresh=0.2):\n",
    "    arg=layout[0]['argument'][0]\n",
    "    scenic_result=load_pkl(os.path.join(GQA_PATH,'meta_one-all-scenic',ques_id+'.pkl'))\n",
    "    info=scenic_result[arg]\n",
    "    scores=[]\n",
    "    scores=[i['score'] for i in info if i['score']>=thresh]\n",
    "    if len(scores)==0:\n",
    "        return 'no'\n",
    "    if max(scores)>=thresh:\n",
    "        return 'yes'\n",
    "    else:\n",
    "        return 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37a4010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_position(bb,pos,input_image,showing=False):\n",
    "    if showing:\n",
    "        vis_no_score([bb],input_image,pos)\n",
    "    if pos=='left':\n",
    "        if bb[0]<0.4:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif pos=='right':\n",
    "        if bb[0]>0.6:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif pos=='top':\n",
    "        if bb[1]<0.4:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'\n",
    "    elif pos=='bottom':\n",
    "        if bb[1]>0.6:\n",
    "            return 'yes'\n",
    "        else:\n",
    "            return 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00af94c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_no_score(bbox,input_image,text=None):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    ax.imshow(input_image, extent=(0, 1, 1, 0))\n",
    "    ax.set_axis_off()\n",
    "    for i,box in enumerate(bbox):\n",
    "        cx, cy, w, h = box\n",
    "        ax.plot([cx - w / 2, cx + w / 2, cx + w / 2, cx - w / 2, cx - w / 2],\n",
    "                [cy - h / 2, cy - h / 2, cy + h / 2, cy + h / 2, cy - h / 2], 'r')\n",
    "    if text is not None:\n",
    "        ax.set_title(text,  fontsize=12)\n",
    "            \n",
    "def verify_attr_ans(layout,img_id,ques_id,showing=False):\n",
    "    attr=layout[-2]['argument'][0]#the last filter module\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    if len(layout)==3:\n",
    "        _,bbox,_=bbox_generator(layout,0,img_id,ques_id)\n",
    "    else:\n",
    "        bbox,cap=phrase_bbox_generator_exist(layout,len(layout)-2,img_id)\n",
    "    if attr in ['left','right','top','bottom']:\n",
    "        pred=filter_position(bbox,attr,input_image,showing)\n",
    "    else:\n",
    "        pred=filter_binary_color(bbox,input_image,attr,showing)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14ac985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scene_ans(layout,img_id,ques_id,showing=False):\n",
    "    attr=layout[-2]['argument'][0]#the last filter module\n",
    "    input_image=get_gqa_feat(img_id)\n",
    "    bbox=[0.5,0.5,0.99,0.99]#the whole image\n",
    "    pred=filter_binary_color(bbox,input_image,attr,showing)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "179a829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def existence_for_all(layout,img_id,ques_id):\n",
    "    ops=[step['operation'] for step in layout]\n",
    "    if 'filter' not in ops:\n",
    "        pred=generate_basic_ans(layout,img_id,ques_id)\n",
    "    elif 'relocate' not in ops:\n",
    "        if layout[0]['argument'][0]=='scene':\n",
    "            pred=generate_scene_ans(layout,img_id,ques_id)\n",
    "        else:\n",
    "            pred=verify_attr_ans(layout,img_id,ques_id)\n",
    "    else:\n",
    "        pred=relocate_involved_ans(layout,img_id,ques_id)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3eab2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_prog=load_pkl('../meta_generated_layout/dep_all_meta_layout.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ed428c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68167\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "colors=defaultdict(int)\n",
    "materials=defaultdict(int)\n",
    "shapes=defaultdict(int)\n",
    "for k,name in enumerate(names):\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if new_prog[-1]['operation']!='query':\n",
    "        continue\n",
    "    if len(new_prog[-1]['argument'])==0:\n",
    "        continue\n",
    "    query_t=new_prog[-1]['argument'][0]\n",
    "    if query_t=='color':\n",
    "        colors[row['answer']]+=1\n",
    "    elif query_t=='material':\n",
    "        materials[row['answer']]+=1\n",
    "    elif query_t=='shape':\n",
    "        shapes[row['answer']]+=1\n",
    "    vis+=1\n",
    "print (vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c99677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_list=[]\n",
    "hold_list.extend([c for c in colors])\n",
    "hold_list.extend([c for c in materials])\n",
    "hold_list.extend([c for c in shapes])\n",
    "hold_list=set(hold_list)\n",
    "#print (hold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bde9d69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/torch/hub/ashkamath_mdetr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid caption generation for 2325480 waiting   waiting person \n",
      "current accuracy: 69.0\n",
      "\tInvalid number 0\n",
      "current accuracy: 68.5\n",
      "\tInvalid number 0\n",
      "current accuracy: 67.66666666666666\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2361213 bag   bag holding man \n",
      "current accuracy: 68.5\n",
      "\tInvalid number 0\n",
      "current accuracy: 70.19999999999999\n",
      "\tInvalid number 0\n",
      "current accuracy: 70.0\n",
      "\tInvalid number 0\n",
      "current accuracy: 69.85714285714286\n",
      "\tInvalid number 0\n",
      "current accuracy: 69.125\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2335787 book   book on top of desk \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result bike 06652937\n",
      "Invalid caption generation for 2324329 bike fire to the right of bike \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "current accuracy: 69.11111111111111\n",
      "\tInvalid number 0\n",
      "current accuracy: 69.1\n",
      "\tInvalid number 0\n",
      "1001\n",
      "69.13086913086913\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    if vis>1000:\n",
    "        break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%100==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8669a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea0607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae804a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3889068d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d84054b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/torch/hub/ashkamath_mdetr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid caption generation for 2333858 boys   boys on the side of water \n",
      "Invalid caption generation for 2319436 tray   tray above paper \n",
      "Invalid caption generation for 2412014 couch couch \n",
      "No filter result men 10767993\n",
      "Check: 882 00759257\n",
      "current accuracy: 69.39999999999999\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2326319 bowl   bowl in vegetables \n",
      "current accuracy: 69.19999999999999\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result fence 06652843\n",
      "Invalid caption generation for 2324329 fence fence to the left of fire \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "Invalid caption generation for 2374121 grass   grass near road \n",
      "Invalid caption generation for 2391143 curtains   curtains in front of windows \n",
      "Invalid caption generation for 2328203 surfboard surfboard \n",
      "No filter result man 14988840\n",
      "No filter result shorts 14988840\n",
      "Invalid caption generation for 2381592 food   food inside bread \n",
      "current accuracy: 69.73333333333333\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2363853 bicycle   bicycle on the front of bus \n",
      "Invalid caption generation for 2364911 counter   counter on candle \n",
      "Check: 2391784 04214417\n",
      "Check: 882 00759248\n",
      "current accuracy: 69.65\n",
      "\tInvalid number 3\n",
      "current accuracy: 69.16\n",
      "\tInvalid number 3\n",
      "Check: 2403572 17455849\n",
      "Check: 2417045 1596056\n",
      "Invalid caption generation for 2374714 car   car near people \n",
      "current accuracy: 69.39999999999999\n",
      "\tInvalid number 5\n",
      "Invalid caption generation for 882 desk desk \n",
      "No filter result tower 00759318\n",
      "Invalid caption generation for 882 tower desk above tower \n",
      "Invalid caption generation for 882 desk desk \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result fence 06652929\n",
      "Invalid caption generation for 2324329 fence fire to the right of fence \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "current accuracy: 69.05714285714286\n",
      "\tInvalid number 5\n",
      "Invalid caption generation for 2333753 animal   animal holding chain \n",
      "Invalid caption generation for 2363853 bicycle   bicycle on the front of bus \n",
      "Invalid caption generation for 2383595 girl   girl to the right of box \n",
      "current accuracy: 69.475\n",
      "\tInvalid number 5\n",
      "Invalid caption generation for 2353038 food   food to the right of woman \n",
      "Invalid caption generation for 2324040 shelf shelf \n",
      "No filter result painting 12228760\n",
      "Invalid caption generation for 2324040 painting shelf to the right of painting \n",
      "Invalid caption generation for 2324040 shelf shelf \n",
      "Invalid caption generation for 2405190 bag   bag to the right of shower \n",
      "Invalid caption generation for 2363853 bicycle   bicycle on the front of bus \n",
      "current accuracy: 69.55555555555556\n",
      "\tInvalid number 5\n",
      "Check: 2412014 10767860\n",
      "current accuracy: 69.54\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2357279 mirrors   mirrors in bathroom \n",
      "current accuracy: 69.63636363636364\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2405250 waiting   waiting person \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result person 06652884\n",
      "No filter result ski 06652884\n",
      "No filter result notebook 18460693\n",
      "current accuracy: 69.58333333333333\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2324329 backpack backpack \n",
      "Invalid caption generation for 2324329 man man to the right of backpack \n",
      "Invalid caption generation for 2324329 backpack backpack \n",
      "Invalid caption generation for 2386495 curtain   curtain behind floor \n",
      "Invalid caption generation for 2368963 curtain   curtain to the right of drawer \n",
      "current accuracy: 69.75384615384615\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2360429 girl   girl to the right of butterfly \n",
      "current accuracy: 69.89999999999999\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2412014 lamp lamp \n",
      "No filter result pillow 10767905\n",
      "Invalid caption generation for 2412014 girl girl to the left of pillow to the left of lamp \n",
      "Invalid caption generation for 2412014 pillow   pillow  lamp \n",
      "current accuracy: 69.82666666666667\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2412014 painting painting \n",
      "No filter result men 10768138\n",
      "No filter result notebook 07638896\n",
      "current accuracy: 69.83749999999999\n",
      "\tInvalid number 6\n",
      "current accuracy: 69.76470588235294\n",
      "\tInvalid number 6\n",
      "No filter result notebook 11573897\n",
      "Invalid caption generation for 2417045 pants pants \n",
      "No filter result man 1596118\n",
      "No filter result woman 1596118\n",
      "Invalid caption generation for 2417045 woman woman to the left of man wearing pants \n",
      "Invalid caption generation for 2417045 man   man wearing pants \n",
      "current accuracy: 69.69999999999999\n",
      "\tInvalid number 6\n",
      "No filter result notebook 13127352\n",
      "Invalid caption generation for 2406655 fork   fork near plate \n",
      "current accuracy: 69.8\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2325221 towel towel \n",
      "No filter result people 14116057\n",
      "No filter result umbrella 14116057\n",
      "Invalid caption generation for 2325221 umbrella umbrella above people to the left of towel \n",
      "Invalid caption generation for 2325221 people   people to the left of towel \n",
      "current accuracy: 69.69\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2412014 lamp lamp \n",
      "No filter result painting 10768105\n",
      "Invalid caption generation for 2412014 painting lamp to the left of painting \n",
      "Invalid caption generation for 2412014 lamp lamp \n",
      "current accuracy: 69.80952380952381\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2410918 pillow  pillow to the left of pillow \n",
      "Invalid caption generation for 2359142 cow   cow eating grass \n",
      "Invalid caption generation for 2331158 pillow  pillow to the left of pillow \n",
      "Invalid caption generation for 2321676 bag   bag holding man \n",
      "current accuracy: 69.7909090909091\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2324040 lamp lamp \n",
      "No filter result man 12228812\n",
      "No filter result motorcycle 12228812\n",
      "Invalid caption generation for 2324040 living living \n",
      "No filter result person 12228864\n",
      "Invalid caption generation for 2324040 man man to the right of person in living \n",
      "Invalid caption generation for 2324040 person   person in living \n",
      "Invalid caption generation for 2412014 office office \n",
      "No filter result cabinet 10768072\n",
      "No filter result notebook 17897769\n",
      "current accuracy: 69.75652173913043\n",
      "\tInvalid number 6\n",
      "current accuracy: 69.74166666666667\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2324040 lamp lamp \n",
      "No filter result man 12228626\n",
      "Invalid caption generation for 2324040 man lamp to the right of man \n",
      "Invalid caption generation for 2324040 lamp lamp \n",
      "current accuracy: 69.864\n",
      "\tInvalid number 6\n",
      "current accuracy: 69.82307692307693\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 882 tower tower \n",
      "No filter result desk 00759286\n",
      "No filter result monitor 00759286\n",
      "Invalid caption generation for 882 monitor monitor on top of desk above tower \n",
      "Invalid caption generation for 882 desk   desk above tower \n",
      "Invalid caption generation for 2334149 bed bed \n",
      "No filter result taxi 17861366\n",
      "No filter result truck 17861366\n",
      "Invalid caption generation for 2334149 truck truck to the left of taxi to the left of bed \n",
      "Invalid caption generation for 2334149 taxi   taxi  bed \n",
      "current accuracy: 69.82962962962964\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2364335 refrigerator   refrigerator in kitchen \n",
      "Invalid caption generation for 2367641 bags   bags by woman \n",
      "current accuracy: 69.77142857142857\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2401498 curtains   curtains in kitchen \n",
      "Invalid caption generation for 2325221 wall wall \n",
      "No filter result toilet 14115917\n",
      "Invalid caption generation for 2325221 hat hat \n",
      "No filter result man 14116060\n",
      "No filter result kite 14116060\n",
      "Invalid caption generation for 2325221 kite kite above man wearing hat \n",
      "Invalid caption generation for 2325221 man   man wearing hat \n",
      "Invalid caption generation for 2328203 person person \n",
      "No filter result surfboard 14988835\n",
      "current accuracy: 69.77241379310345\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2324040 painting painting \n",
      "No filter result shelf 12228843\n",
      "Invalid caption generation for 2324040 shelf painting to the right of shelf \n",
      "Invalid caption generation for 2324040 painting painting \n",
      "Invalid caption generation for 2361213 bag   bag holding man \n",
      "Invalid caption generation for 2389268 bun   bun with onions \n",
      "Invalid caption generation for 2321676 bag   bag holding man \n",
      "Invalid caption generation for 2340966 woman   woman to the left of motorcycle \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy: 69.79333333333332\n",
      "\tInvalid number 6\n",
      "current accuracy: 69.72903225806452\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2334149 building building \n",
      "No filter result truck 17861382\n",
      "current accuracy: 69.75\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2373294 banana   banana in front of cucumber \n",
      "Invalid caption generation for 2348606 appliance   appliance on magnets \n",
      "current accuracy: 69.72727272727272\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2367894 curtains   curtains on window \n",
      "Invalid caption generation for 2324329 vehicle vehicle \n",
      "No filter result middle 06652817\n",
      "No filter result bag 06652817\n",
      "Invalid caption generation for 2324329 bag bag to the left of middle vehicle \n",
      "Invalid caption generation for 2324329 middle   middle vehicle \n",
      "Invalid caption generation for 2363947 sign   sign in street \n",
      "current accuracy: 69.70588235294117\n",
      "\tInvalid number 6\n",
      "Invalid caption generation for 2329006 pillow  pillow to the left of pillow \n",
      "Check: 2412014 10768021\n",
      "current accuracy: 69.76\n",
      "\tInvalid number 7\n",
      "Invalid caption generation for 4488 woman   wo wearing skirt \n",
      "Invalid caption generation for 2325480 waiting   waiting person \n",
      "Invalid caption generation for 2334149 building building \n",
      "No filter result truck 17861346\n",
      "current accuracy: 69.79444444444445\n",
      "\tInvalid number 7\n",
      "Invalid caption generation for 2377682 vehicle   vehicle near grass \n",
      "Invalid caption generation for 2401635 chair   chair in front of floor \n",
      "Invalid caption generation for 2342223 device   device on bed \n",
      "Invalid caption generation for 2366130 spoon   spoon on table \n",
      "current accuracy: 69.76756756756757\n",
      "\tInvalid number 7\n",
      "Invalid caption generation for 2350772 bottle   bottle holding man \n",
      "current accuracy: 69.82105263157895\n",
      "\tInvalid number 7\n",
      "Check: 2412014 10767940\n",
      "Check: 2324040 12228673\n",
      "current accuracy: 69.74871794871795\n",
      "\tInvalid number 9\n",
      "Invalid caption generation for 2324329 truck truck \n",
      "No filter result bag 06652709\n",
      "Invalid caption generation for 2324329 bag truck to the left of bag \n",
      "Invalid caption generation for 2324329 truck truck \n",
      "Invalid caption generation for 2410230 cow   cow above bucket \n",
      "Invalid caption generation for 2390108 pizza   pizza to the left of chair \n",
      "Invalid caption generation for 2386495 curtains   curtains to the right of chair \n",
      "Invalid caption generation for 2383884 men   men to the left of glass \n",
      "current accuracy: 69.795\n",
      "\tInvalid number 9\n",
      "Check: 2328203 14988802\n",
      "Check: 2324040 12228733\n",
      "current accuracy: 69.6780487804878\n",
      "\tInvalid number 11\n",
      "Invalid caption generation for 2335787 book   book on top of desk \n",
      "current accuracy: 69.76190476190476\n",
      "\tInvalid number 11\n",
      "Check: 2334149 17861435\n",
      "Invalid caption generation for 2334149 building building \n",
      "No filter result bus 17861326\n",
      "current accuracy: 69.66511627906978\n",
      "\tInvalid number 12\n",
      "Invalid caption generation for 2349040 device   device to the right of man \n",
      "Invalid caption generation for 2355234 sofa   sofa in front of lamp \n",
      "current accuracy: 69.72727272727272\n",
      "\tInvalid number 12\n",
      "Invalid caption generation for 2409346 children   children behind table \n",
      "Invalid caption generation for 2324329 truck truck \n",
      "No filter result middle 06652735\n",
      "No filter result house 06652735\n",
      "Invalid caption generation for 2324329 house house to the left of middle truck \n",
      "Invalid caption generation for 2324329 middle   middle truck \n",
      "current accuracy: 69.72\n",
      "\tInvalid number 12\n",
      "Invalid caption generation for 2383983 toilet   toilet in front of flowers \n",
      "current accuracy: 69.6391304347826\n",
      "\tInvalid number 12\n",
      "No filter result notebook 10780303\n",
      "Invalid caption generation for 2325221 man man \n",
      "No filter result umbrella 14115985\n",
      "Invalid caption generation for 2325221 umbrella man below umbrella \n",
      "Invalid caption generation for 2325221 man man \n",
      "Invalid caption generation for 2340495 papers   papers on top of desk \n",
      "current accuracy: 69.57872340425531\n",
      "\tInvalid number 12\n",
      "current accuracy: 69.57083333333334\n",
      "\tInvalid number 12\n",
      "Invalid caption generation for 2386029 curtain   curtain behind lamp \n",
      "Check: 2324040 12228833\n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result bike 06652937\n",
      "Invalid caption generation for 2324329 bike fire to the right of bike \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "Invalid caption generation for 2342623 strawberries   strawberries with oatmeal \n",
      "current accuracy: 69.56326530612245\n",
      "\tInvalid number 13\n",
      "Invalid caption generation for 2350792 bike   bike on plate \n",
      "Invalid caption generation for 2401039 plates   plates to the right of cake \n",
      "current accuracy: 69.58800000000001\n",
      "\tInvalid number 13\n",
      "Invalid caption generation for 2412014 office office \n",
      "No filter result men 10768002\n",
      "Check: 2403572 17455891\n",
      "Invalid caption generation for 2324329 backpack backpack \n",
      "No filter result bus 06652791\n",
      "Invalid caption generation for 2324329 bus backpack to the left of bus \n",
      "Invalid caption generation for 2324329 backpack backpack \n",
      "current accuracy: 69.58823529411765\n",
      "\tInvalid number 14\n",
      "current accuracy: 69.58846153846154\n",
      "\tInvalid number 14\n",
      "Invalid caption generation for 2342223 device   device on bed \n",
      "Invalid caption generation for 2323821 vehicle   vehicle on the side of street \n",
      "current accuracy: 69.58867924528302\n",
      "\tInvalid number 14\n",
      "Invalid caption generation for 2407659 bicycle   bicycle to the right of backpack \n",
      "Invalid caption generation for 2403401 counter   counter in front of cutting \n",
      "Invalid caption generation for 2322902 bread   bread in meat \n",
      "Invalid caption generation for 2318908 box   box in grapes \n",
      "Invalid caption generation for 2366184 device   device on table \n",
      "Invalid caption generation for 2331158 pillow  pillow to the right of pillow \n",
      "current accuracy: 69.61851851851853\n",
      "\tInvalid number 14\n",
      "Invalid caption generation for 2384329 chair   chair in front of floor \n",
      "27413\n",
      "69.63119687739395\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "total={}\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    total[name]=pred\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87da3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(total,open('verify.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0e9f5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27413\n",
      "5.7709845693649 3.2065078612337214 29.073797103564 61.94871046583738\n"
     ]
    }
   ],
   "source": [
    "counts={0:0,1:0,2:0,3:0}\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    if 'filter' not in ops:\n",
    "        counts[0]+=1\n",
    "    elif 'relocate' not in ops:\n",
    "        if new_prog[0]['argument'][0]=='scene':\n",
    "            counts[1]+=1\n",
    "        else:\n",
    "            counts[2]+=1\n",
    "    else:\n",
    "        counts[3]+=1\n",
    "total=sum(counts.values())\n",
    "print (total)\n",
    "print (counts[0]*100.0/total,\n",
    "       counts[1]*100.0/total,\n",
    "       counts[2]*100.0/total,\n",
    "       counts[3]*100.0/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0e0480c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/torch/hub/ashkamath_mdetr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: 2412014 10768021\n",
      "current accuracy: 68.0\n",
      "\tInvalid number 1\n",
      "Check: 2403572 17455891\n",
      "current accuracy: 67.0\n",
      "\tInvalid number 2\n",
      "Check: 2324040 12228833\n",
      "current accuracy: 68.60000000000001\n",
      "\tInvalid number 3\n",
      "current accuracy: 69.39999999999999\n",
      "\tInvalid number 3\n",
      "current accuracy: 69.08\n",
      "\tInvalid number 3\n",
      "Check: 2412014 10767860\n",
      "current accuracy: 68.89999999999999\n",
      "\tInvalid number 4\n",
      "current accuracy: 69.22857142857143\n",
      "\tInvalid number 4\n",
      "Check: 882 00759248\n",
      "current accuracy: 69.5\n",
      "\tInvalid number 5\n",
      "current accuracy: 69.44444444444444\n",
      "\tInvalid number 5\n",
      "current accuracy: 69.16\n",
      "\tInvalid number 5\n",
      "current accuracy: 69.12727272727273\n",
      "\tInvalid number 5\n",
      "Check: 882 00759257\n",
      "current accuracy: 69.13333333333334\n",
      "\tInvalid number 6\n",
      "Check: 2328203 14988802\n",
      "Check: 2324040 12228733\n",
      "current accuracy: 69.12307692307692\n",
      "\tInvalid number 8\n",
      "Check: 2412014 10767940\n",
      "Check: 2391784 04214417\n",
      "current accuracy: 69.01428571428572\n",
      "\tInvalid number 10\n",
      "Check: 2403572 17455849\n",
      "current accuracy: 68.88\n",
      "\tInvalid number 11\n",
      "current accuracy: 69.0375\n",
      "\tInvalid number 11\n",
      "current accuracy: 68.95294117647059\n",
      "\tInvalid number 11\n",
      "8849\n",
      "69.1038535427732\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    if 'filter' not in ops:\n",
    "        continue\n",
    "    if 'relocate' in ops:\n",
    "        continue    \n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff68c83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: 2324040 12228733\n",
      "Check: 882 00759248\n",
      "current accuracy: 85.6\n",
      "\tInvalid number 2\n",
      "current accuracy: 85.2\n",
      "\tInvalid number 2\n",
      "current accuracy: 84.33333333333334\n",
      "\tInvalid number 2\n",
      "current accuracy: 84.7\n",
      "\tInvalid number 2\n",
      "current accuracy: 84.56\n",
      "\tInvalid number 2\n",
      "Check: 2403572 17455891\n",
      "Check: 2324040 12228833\n",
      "2858\n",
      "84.88453463960812\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    if 'filter' not in ops:\n",
    "        continue\n",
    "    if 'relocate' in ops:\n",
    "        continue  \n",
    "    attr=new_prog[-2]['argument'][0]\n",
    "    if attr not in ['left','right','top','bottom']:\n",
    "        continue\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dcfa02a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy: 64.2\n",
      "\tInvalid number 0\n",
      "Check: 2412014 10768021\n",
      "current accuracy: 62.9\n",
      "\tInvalid number 1\n",
      "Check: 882 00759257\n",
      "Check: 2403572 17455849\n",
      "Check: 2328203 14988802\n",
      "Check: 2391784 04214417\n",
      "current accuracy: 62.0\n",
      "\tInvalid number 5\n",
      "Check: 2412014 10767940\n",
      "current accuracy: 61.75000000000001\n",
      "\tInvalid number 6\n",
      "Check: 2412014 10767860\n",
      "current accuracy: 61.480000000000004\n",
      "\tInvalid number 7\n",
      "current accuracy: 61.46666666666667\n",
      "\tInvalid number 7\n",
      "current accuracy: 61.6\n",
      "\tInvalid number 7\n",
      "current accuracy: 62.025\n",
      "\tInvalid number 7\n",
      "current accuracy: 61.84444444444445\n",
      "\tInvalid number 7\n",
      "current accuracy: 61.68\n",
      "\tInvalid number 7\n",
      "current accuracy: 61.81818181818181\n",
      "\tInvalid number 7\n",
      "5991\n",
      "61.60908028709732\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    if 'filter' not in ops:\n",
    "        continue\n",
    "    if 'relocate' in ops:\n",
    "        continue  \n",
    "    attr=new_prog[-2]['argument'][0]\n",
    "    if attr in ['left','right','top','bottom']:\n",
    "        continue\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efe7e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy: 62.0\n",
      "\tInvalid number 0\n",
      "Check: 882 00759257\n",
      "Check: 2391784 04214417\n",
      "Check: 2412014 10767940\n",
      "current accuracy: 60.699999999999996\n",
      "\tInvalid number 3\n",
      "current accuracy: 62.06666666666667\n",
      "\tInvalid number 3\n",
      "Check: 2328203 14988802\n",
      "current accuracy: 61.650000000000006\n",
      "\tInvalid number 4\n",
      "current accuracy: 60.96\n",
      "\tInvalid number 4\n",
      "Check: 2412014 10767860\n",
      "current accuracy: 61.133333333333326\n",
      "\tInvalid number 5\n",
      "current accuracy: 61.771428571428565\n",
      "\tInvalid number 5\n",
      "Check: 2403572 17455849\n",
      "current accuracy: 61.724999999999994\n",
      "\tInvalid number 6\n",
      "Check: 2412014 10768021\n",
      "current accuracy: 61.46666666666667\n",
      "\tInvalid number 7\n",
      "current accuracy: 61.7\n",
      "\tInvalid number 7\n",
      "5444\n",
      "62.16017634092579\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    if 'filter' not in ops:\n",
    "        continue\n",
    "    if 'relocate' in ops:\n",
    "        continue  \n",
    "    if len(new_prog)!=3:\n",
    "        continue\n",
    "    attr=new_prog[-2]['argument'][0]\n",
    "    if attr in ['left','right','top','bottom']:\n",
    "        continue\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f1f4382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy: 56.99999999999999\n",
      "\tInvalid number 0\n",
      "547\n",
      "56.672760511883006\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    if 'filter' not in ops:\n",
    "        continue\n",
    "    if 'relocate' in ops:\n",
    "        continue  \n",
    "    if len(new_prog)==3:\n",
    "        continue\n",
    "    attr=new_prog[-2]['argument'][0]\n",
    "    if attr in ['left','right','top','bottom']:\n",
    "        continue\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5f79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1995f2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid caption generation for 2412014 office office \n",
      "No filter result cabinet 10768072\n",
      "Invalid caption generation for 2324040 lamp lamp \n",
      "No filter result man 12228812\n",
      "No filter result motorcycle 12228812\n",
      "Invalid caption generation for 2325221 wall wall \n",
      "No filter result toilet 14115917\n",
      "current accuracy: 67.80000000000001\n",
      "\tInvalid number 0\n",
      "current accuracy: 68.2\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2348606 appliance   appliance on magnets \n",
      "Invalid caption generation for 882 desk desk \n",
      "No filter result tower 00759318\n",
      "Invalid caption generation for 882 tower desk above tower \n",
      "Invalid caption generation for 882 desk desk \n",
      "Invalid caption generation for 2412014 couch couch \n",
      "No filter result men 10767993\n",
      "current accuracy: 68.26666666666667\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2328203 person person \n",
      "No filter result surfboard 14988835\n",
      "No filter result notebook 10780303\n",
      "Invalid caption generation for 882 tower tower \n",
      "No filter result desk 00759286\n",
      "No filter result monitor 00759286\n",
      "Invalid caption generation for 882 monitor monitor on top of desk above tower \n",
      "Invalid caption generation for 882 desk   desk above tower \n",
      "current accuracy: 68.30000000000001\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2331158 pillow  pillow to the left of pillow \n",
      "Invalid caption generation for 2324040 living living \n",
      "No filter result person 12228864\n",
      "Invalid caption generation for 2324040 man man to the right of person in living \n",
      "Invalid caption generation for 2324040 person   person in living \n",
      "No filter result notebook 18460693\n",
      "current accuracy: 68.60000000000001\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2321676 bag   bag holding man \n",
      "Invalid caption generation for 2350792 bike   bike on plate \n",
      "Invalid caption generation for 2401498 curtains   curtains in kitchen \n",
      "No filter result notebook 07638896\n",
      "current accuracy: 68.06666666666666\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2386495 curtain   curtain behind floor \n",
      "Invalid caption generation for 4488 woman   wo wearing skirt \n",
      "current accuracy: 68.05714285714286\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result fence 06652843\n",
      "Invalid caption generation for 2324329 fence fence to the left of fire \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "Invalid caption generation for 2412014 lamp lamp \n",
      "No filter result painting 10768105\n",
      "Invalid caption generation for 2412014 painting lamp to the left of painting \n",
      "Invalid caption generation for 2412014 lamp lamp \n",
      "current accuracy: 68.27499999999999\n",
      "\tInvalid number 0\n",
      "No filter result notebook 17897769\n",
      "Invalid caption generation for 2363853 bicycle   bicycle on the front of bus \n",
      "Invalid caption generation for 2407659 bicycle   bicycle to the right of backpack \n",
      "Invalid caption generation for 2335787 book   book on top of desk \n",
      "Invalid caption generation for 2328203 surfboard surfboard \n",
      "No filter result man 14988840\n",
      "No filter result shorts 14988840\n",
      "Invalid caption generation for 2401635 chair   chair in front of floor \n",
      "current accuracy: 68.46666666666667\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2373294 banana   banana in front of cucumber \n",
      "Invalid caption generation for 2367894 curtains   curtains on window \n",
      "current accuracy: 68.36\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2403401 counter   counter in front of cutting \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result fence 06652929\n",
      "Invalid caption generation for 2324329 fence fire to the right of fence \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "Invalid caption generation for 2342223 device   device on bed \n",
      "current accuracy: 68.47272727272727\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2364911 counter   counter on candle \n",
      "Invalid caption generation for 2319436 tray   tray above paper \n",
      "Invalid caption generation for 2405250 waiting   waiting person \n",
      "Invalid caption generation for 2383595 girl   girl to the right of box \n",
      "Invalid caption generation for 2383884 men   men to the left of glass \n",
      "current accuracy: 68.33333333333333\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2410918 pillow  pillow to the left of pillow \n",
      "Invalid caption generation for 2325221 hat hat \n",
      "No filter result man 14116060\n",
      "No filter result kite 14116060\n",
      "Invalid caption generation for 2325221 kite kite above man wearing hat \n",
      "Invalid caption generation for 2325221 man   man wearing hat \n",
      "current accuracy: 68.53846153846153\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2333858 boys   boys on the side of water \n",
      "Invalid caption generation for 2389268 bun   bun with onions \n",
      "Invalid caption generation for 2324329 backpack backpack \n",
      "Invalid caption generation for 2324329 man man to the right of backpack \n",
      "Invalid caption generation for 2324329 backpack backpack \n",
      "current accuracy: 68.52857142857142\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2367641 bags   bags by woman \n",
      "Invalid caption generation for 2353038 food   food to the right of woman \n",
      "current accuracy: 68.37333333333333\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2361213 bag   bag holding man \n",
      "Invalid caption generation for 2331158 pillow  pillow to the right of pillow \n",
      "No filter result notebook 13127352\n",
      "current accuracy: 68.5\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2412014 painting painting \n",
      "No filter result men 10768138\n",
      "Invalid caption generation for 2409346 children   children behind table \n",
      "current accuracy: 68.4\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2363947 sign   sign in street \n",
      "Invalid caption generation for 2333753 animal   animal holding chain \n",
      "current accuracy: 68.42222222222222\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2391143 curtains   curtains in front of windows \n",
      "Invalid caption generation for 2323821 vehicle   vehicle on the side of street \n",
      "Invalid caption generation for 2322902 bread   bread in meat \n",
      "Invalid caption generation for 2329006 pillow  pillow to the left of pillow \n",
      "current accuracy: 68.33684210526316\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2359142 cow   cow eating grass \n",
      "Invalid caption generation for 2390108 pizza   pizza to the left of chair \n",
      "Invalid caption generation for 2377682 vehicle   vehicle near grass \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result bike 06652937\n",
      "Invalid caption generation for 2324329 bike fire to the right of bike \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "Invalid caption generation for 2417045 pants pants \n",
      "No filter result man 1596118\n",
      "No filter result woman 1596118\n",
      "Invalid caption generation for 2417045 woman woman to the left of man wearing pants \n",
      "Invalid caption generation for 2417045 man   man wearing pants \n",
      "current accuracy: 68.22\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result person 06652884\n",
      "No filter result ski 06652884\n",
      "Invalid caption generation for 2325221 man man \n",
      "No filter result umbrella 14115985\n",
      "Invalid caption generation for 2325221 umbrella man below umbrella \n",
      "Invalid caption generation for 2325221 man man \n",
      "current accuracy: 68.25714285714287\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2386029 curtain   curtain behind lamp \n",
      "Invalid caption generation for 2334149 bed bed \n",
      "No filter result taxi 17861366\n",
      "No filter result truck 17861366\n",
      "Invalid caption generation for 2334149 truck truck to the left of taxi to the left of bed \n",
      "Invalid caption generation for 2334149 taxi   taxi  bed \n",
      "current accuracy: 68.30909090909091\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2384329 chair   chair in front of floor \n",
      "Invalid caption generation for 2321676 bag   bag holding man \n",
      "Invalid caption generation for 2406655 fork   fork near plate \n",
      "current accuracy: 68.50434782608696\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2342623 strawberries   strawberries with oatmeal \n",
      "Invalid caption generation for 2334149 building building \n",
      "No filter result truck 17861346\n",
      "Invalid caption generation for 2357279 mirrors   mirrors in bathroom \n",
      "Invalid caption generation for 2364335 refrigerator   refrigerator in kitchen \n",
      "current accuracy: 68.425\n",
      "\tInvalid number 0\n",
      "No filter result notebook 11573897\n",
      "current accuracy: 68.464\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2350772 bottle   bottle holding man \n",
      "Invalid caption generation for 2325480 waiting   waiting person \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid caption generation for 2383983 toilet   toilet in front of flowers \n",
      "Invalid caption generation for 2334149 building building \n",
      "No filter result truck 17861382\n",
      "Invalid caption generation for 2349040 device   device to the right of man \n",
      "Invalid caption generation for 2325221 towel towel \n",
      "No filter result people 14116057\n",
      "No filter result umbrella 14116057\n",
      "Invalid caption generation for 2325221 umbrella umbrella above people to the left of towel \n",
      "Invalid caption generation for 2325221 people   people to the left of towel \n",
      "current accuracy: 68.37692307692308\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2324040 painting painting \n",
      "No filter result shelf 12228843\n",
      "Invalid caption generation for 2324040 shelf painting to the right of shelf \n",
      "Invalid caption generation for 2324040 painting painting \n",
      "Invalid caption generation for 2324329 vehicle vehicle \n",
      "No filter result middle 06652817\n",
      "No filter result bag 06652817\n",
      "Invalid caption generation for 2324329 bag bag to the left of middle vehicle \n",
      "Invalid caption generation for 2324329 middle   middle vehicle \n",
      "Invalid caption generation for 2401039 plates   plates to the right of cake \n",
      "Invalid caption generation for 2334149 building building \n",
      "No filter result bus 17861326\n",
      "Invalid caption generation for 2412014 office office \n",
      "No filter result men 10768002\n",
      "Invalid caption generation for 2368963 curtain   curtain to the right of drawer \n",
      "current accuracy: 68.42962962962963\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2412014 lamp lamp \n",
      "No filter result pillow 10767905\n",
      "Invalid caption generation for 2412014 girl girl to the left of pillow to the left of lamp \n",
      "Invalid caption generation for 2412014 pillow   pillow  lamp \n",
      "Invalid caption generation for 2340966 woman   woman to the left of motorcycle \n",
      "Invalid caption generation for 2381592 food   food inside bread \n",
      "Invalid caption generation for 2324329 truck truck \n",
      "No filter result bag 06652709\n",
      "Invalid caption generation for 2324329 bag truck to the left of bag \n",
      "Invalid caption generation for 2324329 truck truck \n",
      "current accuracy: 68.48571428571428\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2324329 truck truck \n",
      "No filter result middle 06652735\n",
      "No filter result house 06652735\n",
      "Invalid caption generation for 2324329 house house to the left of middle truck \n",
      "Invalid caption generation for 2324329 middle   middle truck \n",
      "Invalid caption generation for 2386495 curtains   curtains to the right of chair \n",
      "Invalid caption generation for 2374121 grass   grass near road \n",
      "current accuracy: 68.46896551724137\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2355234 sofa   sofa in front of lamp \n",
      "current accuracy: 68.51333333333334\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2405190 bag   bag to the right of shower \n",
      "Invalid caption generation for 2366184 device   device on table \n",
      "Invalid caption generation for 2363853 bicycle   bicycle on the front of bus \n",
      "Invalid caption generation for 2318908 box   box in grapes \n",
      "Invalid caption generation for 2410230 cow   cow above bucket \n",
      "current accuracy: 68.49032258064516\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2360429 girl   girl to the right of butterfly \n",
      "Invalid caption generation for 2324329 backpack backpack \n",
      "No filter result bus 06652791\n",
      "Invalid caption generation for 2324329 bus backpack to the left of bus \n",
      "Invalid caption generation for 2324329 backpack backpack \n",
      "Invalid caption generation for 2374714 car   car near people \n",
      "Invalid caption generation for 2342223 device   device on bed \n",
      "current accuracy: 68.525\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2363853 bicycle   bicycle on the front of bus \n",
      "Invalid caption generation for 2324040 shelf shelf \n",
      "No filter result painting 12228760\n",
      "Invalid caption generation for 2324040 painting shelf to the right of painting \n",
      "Invalid caption generation for 2324040 shelf shelf \n",
      "Invalid caption generation for 2340495 papers   papers on top of desk \n",
      "current accuracy: 68.44242424242424\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2326319 bowl   bowl in vegetables \n",
      "Invalid caption generation for 2324040 lamp lamp \n",
      "No filter result man 12228626\n",
      "Invalid caption generation for 2324040 man lamp to the right of man \n",
      "Invalid caption generation for 2324040 lamp lamp \n",
      "Invalid caption generation for 2366130 spoon   spoon on table \n",
      "16982\n",
      "68.46072311859615\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    if 'filter' not in ops:\n",
    "        continue\n",
    "    if 'relocate' not in ops:\n",
    "        continue    \n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b220eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57eeb9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27413\n"
     ]
    }
   ],
   "source": [
    "vis=0\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    vis+=1\n",
    "print (vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02a32a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "75.75757575757575\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    if vis>32:\n",
    "        break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    if 'relocate' in ops:\n",
    "        continue\n",
    "    if 'filter' not in ops:\n",
    "        continue\n",
    "    if new_prog[0]['argument'][0]!='scene':\n",
    "        continue\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67b944d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/torch/hub/ashkamath_mdetr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No filter result notebook 18460693\n",
      "Invalid caption generation for 2367894 curtains   curtains on window \n",
      "Invalid caption generation for 2410918 pillow  pillow to the left of pillow \n",
      "current accuracy: 69.19999999999999\n",
      "\tInvalid number 0\n",
      "No filter result notebook 11573897\n",
      "Invalid caption generation for 2377682 vehicle   vehicle near grass \n",
      "Invalid caption generation for 2368963 curtain   curtain to the right of drawer \n",
      "current accuracy: 69.19999999999999\n",
      "\tInvalid number 0\n",
      "1001\n",
      "69.13086913086913\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    if vis>1000:\n",
    "        break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    if 'relocate' not in ops:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19ce1b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current accuracy: 85.39999999999999\n",
      "\tInvalid number 0\n",
      "Check: 2417045 1596056\n",
      "Check: 2324040 12228673\n",
      "current accuracy: 85.3\n",
      "\tInvalid number 2\n",
      "Check: 2334149 17861435\n",
      "current accuracy: 85.26666666666667\n",
      "\tInvalid number 3\n",
      "1582\n",
      "84.95575221238938\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    #if vis>10:\n",
    "    #    break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    if 'filter' in ops:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17916ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fe1b4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: 2324040 12228833\n",
      "Invalid caption generation for 2417045 pants pants \n",
      "No filter result man 1596118\n",
      "No filter result woman 1596118\n",
      "Invalid caption generation for 2417045 woman woman to the left of man wearing pants \n",
      "Invalid caption generation for 2417045 man   man wearing pants \n",
      "current accuracy: 66.60000000000001\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2321676 bag   bag holding man \n",
      "Invalid caption generation for 2383983 toilet   toilet in front of flowers \n",
      "current accuracy: 65.9\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2412014 couch couch \n",
      "No filter result men 10767993\n",
      "Invalid caption generation for 2329006 pillow  pillow to the left of pillow \n",
      "Invalid caption generation for 2319436 tray   tray above paper \n",
      "Check: 2403572 17455849\n",
      "current accuracy: 67.80000000000001\n",
      "\tInvalid number 2\n",
      "Check: 2334149 17861435\n",
      "current accuracy: 69.15\n",
      "\tInvalid number 3\n",
      "2201\n",
      "68.8323489323035\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    if vis>2200:\n",
    "        break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c83a2c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid caption generation for 2325221 towel towel \n",
      "No filter result people 14116057\n",
      "No filter result umbrella 14116057\n",
      "Invalid caption generation for 2325221 umbrella umbrella above people to the left of towel \n",
      "Invalid caption generation for 2325221 people   people to the left of towel \n",
      "Invalid caption generation for 2373294 banana   banana in front of cucumber \n",
      "Invalid caption generation for 2324329 vehicle vehicle \n",
      "No filter result middle 06652817\n",
      "No filter result bag 06652817\n",
      "Invalid caption generation for 2324329 bag bag to the left of middle vehicle \n",
      "Invalid caption generation for 2324329 middle   middle vehicle \n",
      "current accuracy: 68.4\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2367894 curtains   curtains on window \n",
      "Invalid caption generation for 2383595 girl   girl to the right of box \n",
      "Invalid caption generation for 2386495 curtains   curtains to the right of chair \n",
      "Check: 2324040 12228733\n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result fence 06652843\n",
      "Invalid caption generation for 2324329 fence fence to the left of fire \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "current accuracy: 68.7\n",
      "\tInvalid number 1\n",
      "current accuracy: 70.19999999999999\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2350772 bottle   bottle holding man \n",
      "Invalid caption generation for 2326319 bowl   bowl in vegetables \n",
      "Invalid caption generation for 2363853 bicycle   bicycle on the front of bus \n",
      "current accuracy: 69.95\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2386495 curtain   curtain behind floor \n",
      "No filter result notebook 13127352\n",
      "2201\n",
      "70.19536574284416\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    if vis>2200:\n",
    "        break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c8672ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: 2324040 12228673\n",
      "Invalid caption generation for 2361213 bag   bag holding man \n",
      "Invalid caption generation for 2381592 food   food inside bread \n",
      "current accuracy: 73.6\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2334149 building building \n",
      "No filter result bus 17861326\n",
      "Invalid caption generation for 2406655 fork   fork near plate \n",
      "Invalid caption generation for 2412014 couch couch \n",
      "No filter result men 10767993\n",
      "Invalid caption generation for 2412014 painting painting \n",
      "No filter result men 10768138\n",
      "current accuracy: 71.39999999999999\n",
      "\tInvalid number 1\n",
      "Check: 2412014 10767860\n",
      "Invalid caption generation for 2363947 sign   sign in street \n",
      "current accuracy: 69.6\n",
      "\tInvalid number 2\n",
      "Invalid caption generation for 2350792 bike   bike on plate \n",
      "Invalid caption generation for 2374714 car   car near people \n",
      "current accuracy: 69.55\n",
      "\tInvalid number 2\n",
      "Invalid caption generation for 2357279 mirrors   mirrors in bathroom \n",
      "Invalid caption generation for 2331158 pillow  pillow to the left of pillow \n",
      "2201\n",
      "70.0590640617901\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    if vis>2200:\n",
    "        break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ae51fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid caption generation for 2324329 truck truck \n",
      "No filter result bag 06652709\n",
      "Invalid caption generation for 2324329 bag truck to the left of bag \n",
      "Invalid caption generation for 2324329 truck truck \n",
      "current accuracy: 70.6\n",
      "\tInvalid number 0\n",
      "Invalid caption generation for 2325221 wall wall \n",
      "No filter result toilet 14115917\n",
      "Invalid caption generation for 2383595 girl   girl to the right of box \n",
      "Check: 2324040 12228733\n",
      "current accuracy: 69.6\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2355234 sofa   sofa in front of lamp \n",
      "current accuracy: 69.73333333333333\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2321676 bag   bag holding man \n",
      "Invalid caption generation for 2403401 counter   counter in front of cutting \n",
      "current accuracy: 70.55\n",
      "\tInvalid number 1\n",
      "2201\n",
      "70.01363016810541\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    if vis>2200:\n",
    "        break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63c4a2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: 2391784 04214417\n",
      "current accuracy: 70.6\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2331158 pillow  pillow to the right of pillow \n",
      "Invalid caption generation for 2361213 bag   bag holding man \n",
      "current accuracy: 69.39999999999999\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2403401 counter   counter in front of cutting \n",
      "Invalid caption generation for 2391143 curtains   curtains in front of windows \n",
      "Invalid caption generation for 2374121 grass   grass near road \n",
      "current accuracy: 69.19999999999999\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2333858 boys   boys on the side of water \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "No filter result fence 06652929\n",
      "Invalid caption generation for 2324329 fence fire to the right of fence \n",
      "Invalid caption generation for 2324329 fire fire \n",
      "current accuracy: 70.35\n",
      "\tInvalid number 1\n",
      "Invalid caption generation for 2412014 couch couch \n",
      "No filter result men 10767993\n",
      "2201\n",
      "69.92276238073603\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    if vis>2200:\n",
    "        break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    ops=[step['operation'] for step in new_prog]\n",
    "    try:\n",
    "        pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    except:\n",
    "        invalid.append(name)\n",
    "        print ('Check:',row['imageId'],name)\n",
    "    #pred=existence_for_all(new_prog,row['imageId'],name)\n",
    "    vis+=1\n",
    "    if pred==row['answer'].strip():\n",
    "        acc+=1\n",
    "    if vis%500==0:\n",
    "        print ('current accuracy:',acc/vis*100.0)\n",
    "        print ('\\tInvalid number',len(invalid))\n",
    "print (vis)\n",
    "print (acc/vis*100.0)\n",
    "print (len(invalid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "679427a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yes': 13569, 'no': 13844}\n"
     ]
    }
   ],
   "source": [
    "#focus on exist now\n",
    "#without the scene related questions\n",
    "\n",
    "thresh=0.2\n",
    "#random.shuffle(names)\n",
    "vis=0\n",
    "acc=0.0\n",
    "counts={'yes':0,'no':0}\n",
    "invalid=[]\n",
    "for k,name in enumerate(names):\n",
    "    if vis>10:\n",
    "        break\n",
    "    row=gqa_val_q[name]\n",
    "    new_prog=meta_prog[name]\n",
    "\n",
    "    if row['types']['structural']!='verify':\n",
    "        continue\n",
    "    counts[row['answer']]+=1\n",
    "print (counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8710f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
